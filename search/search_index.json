{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"cloud/intro/","text":"AWS Documentation AWS is currently the leading cloud provider. Azure Documentation Azure is currently the leading cloud provider. GCP Documentation GCP is currently the leading cloud provider.","title":"Introduction"},{"location":"cloud/aws/aws-ddns-ref/","text":"[[ TOC ]] VPC DNS \u00b6 DNS provides a friendly name to refer to an EC2's IP Address. AWS already provides DNS support for their VPC . However, they have some limitations: AWS-provided private DNS hostnames are static domain names that take the form ip-private-ipv4-address.ec2.internal for the us-east-1 Region and , and ip-private-ipv4-address.region.compute.internal for other Regions. These private DNS hostnames are only accessible between instances in the same VPC. In a multi-VPC environment, instances can't resolve the DNS hostname outside the VPC that they are in. Route53 \u00b6 Route53 can be used to create custom private hosted zones. A private hosted zone is a container for one or more VPC Domain records. By creating a private hosted zone for a domain (such as gtz4all.com), and then creating records, instances can query Route 53 to find out how traffic should be routed for that domain within their VPC and among others. Private Hosted Zone Requirements \u00b6 For a private hosted zone to be accessible, there are multiple requirements. for additional details, please refer to these links [Creating a private hosted zone] * A VPC must be associated with the Private Hosted Zone * The following VPC settings must be set to true * enableDnsHostnames * enableDnsSupport * DHCP options sets * Valid domain name`` (such as gtz4all.com) * domain-name-servers value: AmazonProvidedDNS``` for additional details, please refer to these links * Creating a private hosted zone * DHCP options sets for your VPC Private Hosted Zone Records \u00b6 * * A(Address)record A record maps a hostname and a domain name to IP addresses. The value inside an A record can only be an IP Address Name Domain A Record Value ec2lab gtz4all.com ec2lab.gtz4all.com 10.168.0.10 Reverse Lookup Zone Records \u00b6 A reverse lookup zone is used primarily to resolve IP addresses to Address records. Reverse lookup Zones use a special root domain called in-and. An IP address CIDR block reverse octet order is used to create a subdomain within the in-and.ARPA domain. VPC CIDR Block Reverse Lookup Domain 10.168.0.0/16 168.10.in-addr.arpa. A DNS pointer record (PTR) provides the mapping of an IP address to a domain name. A DNS PTR record is the opposite of the 'A' record, which provides the IP address associated with a domain name. IP Address Reverse Lookup Zone PTR Record Value 10.168.0.10 168.10.in-addr.arpa. 10.0.168.10.in-addr.arpa. ec2lab.gtz4all.com EC2 DDNS Lambda \u00b6 This solution uses tagging as a way to interact with the users. Overview \u00b6 graph TD A[ddns_lambda<br>core_ddns_lambda] -->|assume role| B(sts_ddns_lambda) B --> C{EC2} C -->|Get Tag Value| D[DNSName] C -->|Get VPC| E[SharedVPC] C -->|Route53| F[A/PTR Records] C -->|Update Tag| G[DDNS] the users create a tag key DNSName with the value of their requested DNS name. DNSName: ec2lab VPC, Private IP, State Lambda will pull resource DNSName Value to validate DNS Naming Standards valid DNS name Characters Lambda will update the tag DDNS to notify users if they provided invalid characters Lambda will get the VPC attributes where the resource resides to validate if it is properly configured to support Route53. enableDnsHostnames , enableDnsSupport set to true if it isn't already domain-name and AmazonProvidedDNS `domain name is later used to create Private Hosted Zone if it doesn't already exist VPC CIDR Block VPC_CIDR_Block is used to generate Reverse Zone Lookup and create if required. Lambda will then go into multiple phases to find out if ` DNSName value already exists or if it's unique Lambda will update tag DDNS to notify users if the provided DNSName value already exist if the provided DNSName value is unique, A Record is created in the VPC's domain name PTR Record based on the resource private ip is created in the VPC CIDR Block Reverse Lookup Zone DynamoDB DDNS used to keep track of instance attributes and DNSName Changes used to retrieve instance interface attributes when the instance has been terminated. EC2 DDNS Lambda Flow \u00b6 Infrastructure as code \u00b6 Infrastructure as code ( IaC ) solves several problems with manual and static infrastructure management. IaC provides the ability to create and destroy an environment multiple times. This provides a way to version control and create an environment lifecycle where environmental changes can transition from development, testing, and production. Terraform \u00b6 Terraform is an IaC (Infrastructure as Code) tool used to build and modify infrastructure safely and efficiently. It keeps track of its environment state so rerunning the tool against a code with no changes will not generate any changes. Terraform Modules \u00b6 Terraform modules can be used to create reusable code using terraform loop-like features such as count . Terraform Module Structure \u00b6 ![Terraform Design](/img/tf_structure.jpg) VPC Module \u00b6 The VPC Module is used to build multiple Three Tier VPCs graph TD subgraph A[Three Tier VPC1] subgraph B[Availability Zone 1] 1[Public Subnet] 2[Private Subnet] 3[Local Subnet] end subgraph C[Availability Zone 2] 4[Local Subnet] 5[Private Subnet] 6[Public Subnet] end end style 1 fill:#f5d5d5 style 2 fill:#e0ebff style 3 fill:#d8e6d8 style 6 fill:#f5d5d5 style 5 fill:#e0ebff style 4 fill:#d8e6d8 style A fill:#f5f5f5 module \"vpc_build\" { count = var . vpc_count source = \"./modules/vpc\" vpc_cidr = var . vpc_cidrs [ count . index ] vpc_name = var . vpc_names [ count . index ] vpc_domain = \"$ {var.vpc_names[count.index]} .gtz4all.com\" } TGW Module \u00b6 This module builds the Transit Gateway and One route table graph TD subgraph D[Transit Gateway] subgraph E[Multi-VPC Route Table] 7[Attachments] end end style 7 fill:#e0ebff style D fill:#f5f5f5 module \"tgw\" { source = \"./modules/tgw\" tgw_name = \"multi_vpc\" } Networking Module \u00b6 This module is used to create VPC to TGW Connectivity which results in all VPCs being able to communicate with each other. graph TD subgraph A[Three Tier VPC1] 1[Availability Zone 1] 2[Availability Zone 2] end subgraph B[Three Tier VPC2] 3[Availability Zone 1] 4[Availability Zone 2] end subgraph C[Three Tier VPC3] 5[Availability Zone 1] 6[Availability Zone 2] end subgraph D[Transit Gateway] subgraph E[Multi-VPC Route Table] 7[VPC1 Attachment] --> 1 7[VPC1 Attachment] --> 2 8[VPC2 Attachment] --> 3 8[VPC2 Attachment] --> 4 9[VPC3 Attachment] --> 5 9[VPC4 Attachment] --> 6 end end style A fill:#f5f5f5 style B fill:#f5f5f5 style C fill:#f5f5f5 style D fill:#f5f5f5 style 1 fill:#d8e6d8 style 2 fill:#d8e6d8 style 3 fill:#d8e6d8 style 4 fill:#d8e6d8 style 5 fill:#d8e6d8 style 6 fill:#d8e6d8 * variables are being pass from the vpc and tgw to the root module which in turn passes those values to the networking module module \"vpc_networking\" { count = var . vpc_count source = \"./modules/networking\" dest_cidr_block = var . vpc_to_tgw_dest_cidr tgw_id = module . tgw . tgw_id tgw_rt_id = module . tgw . tgw_rt_id vpc_id = module . vpc_build [ count . index ] . vpc_id vpc_name = var . vpc_names [ count . index ] vpc_subnet0_id = module . vpc_build [ count . index ] . local_subnet0_id vpc_subnet1_id = module . vpc_build [ count . index ] . local_subnet1_id public_subnet_rt_id = module . vpc_build [ count . index ] . public_rt_id private_subnet_rt_id = module . vpc_build [ count . index ] . private_rt_id local_subnet_rt_id = module . vpc_build [ count . index ] . local_rt_id } IAM Module \u00b6 This module first creates the core_ddns_lambda role and policy. It then uses the core_ddns_lambda ARN to generate the sts_ddns_lambda policy which is associated with its corresponding role it also creates a ec2_tag_restriction policy to maintain a healthy Route53 environment and be able to track Route53 Records correctly. Once the DNSName is created, it must remain for the lifetime of its resource. If the tag gets deleted, Lambda would not be able to perform route 53 clean up like deleted A and PTR records. DDNS tag is used by lambda to update users on their DNS Name request. So users shouldn't be allowed to create or modify this tag. { \"Statement\" : [ { \"Action\" : [ \"ec2:DeleteTags\" ], \"Condition\" : { \"ForAllValues:StringEquals\" : { \"aws:TagKeys\" : [ \"DNSName\" ] } }, \"Effect\" : \"Deny\" , \"Resource\" : \"*\" , \"Sid\" : \"DenyDNSNameTagDeletion\" }, { \"Action\" : [ \"ec2:DeleteTags\" , \"ec2:CreateTags\" ], \"Condition\" : { \"ForAllValues:StringEquals\" : { \"aws:TagKeys\" : [ \"DDNS\" ] } }, \"Effect\" : \"Deny\" , \"Resource\" : \"*\" , \"Sid\" : \"DenyDDNSTagChanges\" } ], \"Version\" : \"2012-10-17\" } module \"iam_roles\" { source = \"./modules/iam\" } Lambda Module \u00b6 This module builds a zip file which later is used to create the lambda function and its triggers. Events are used to trigger the creation/deletion of Route53 resources aws.ec2 trigger is used to notify lambda about the creation or termination of an EC2. aws.tag trigger is used to notify lambda about the creation or modification of the \"DNSName\" tag. module \"ddns_lambda\" { source = \"./modules/lambda\" ec2_tag_ddns_role_arn = module . iam_roles . core_ddns_role_arn } EC2 Module \u00b6 This module is just to validate ec2_tag_ddns lambda environment module \"ec2_ddns_lab\" { count = var . vpc_count source = \"./modules/ec2\" vpc_id = module . vpc_build [ count . index ] . vpc_id public_subnet_id = module . vpc_build [ count . index ] . public_subnet0_id ec2_name = \"$ {var.vpc_names[count.index]} -ddns-lab\" } EC2 Tag DDNS Test Drive \u00b6 Requirements \u00b6 Install Terraform Verify Terraform Installation terraform -help Install AWS CLI Configuring the AWS CLI Verify AWS CLI Access aws ec2 describe-vpcs Building Infrastructure \u00b6 terraform init terraform plan terraform apply Do you want to perform these actions ? Terraform will perform the actions described above . Only 'yes' will be accepted to approve . Enter a value : yes Review the following Services VPC EC2 Route53 DynamoDB Lambda IAM Update EC2 DNSName Value review DDNS Tag Value Terminate Instance Review DDNS Table and Route53 Private Hosted Zone and Reverse Zone Lookup Clean up \u00b6 terraform destroy Do you really want to destroy all resources ? Terraform will destroy all your managed infrastructure , as shown above . There is no undo . Only 'yes' will be accepted to confirm . Enter a value : yes Resources built by EC2_TAG_DDNS_Lambda are not destroyed, they will need to be deleted manually DynamoDB - DDNS Private Hosted Zones Reverse Lookup Zones Cross-Account EC2 Dynamic DNS \u00b6 This design can be used across multiple accounts using the follow AWS Features: Shared VPCs EventBridge Bus AssumeRole EC2 DDNS Lambda Flow \u00b6 \u00b6 NOTE Currently working on a Module to take care of the steps below. Share Subnets to a child account in your organization using [Resource Access Manager] ( https://aws.amazon.com/blogs/networking-and-content-delivery/vpc-sharing-a-new-approach-to-multiple-accounts-and-vpc-management/ ) Create AssumeRole (sts)... see IAM Module STS Role Enable EventBus on Network Account Create aws.ec2 and aws.tag EventBridge Triggers towards Network Account EventBus on Child Account Create Instance on child account with DNSName Tag","title":"AWS Tag Based Dynamic DNS"},{"location":"cloud/aws/aws-ddns-ref/#vpc-dns","text":"DNS provides a friendly name to refer to an EC2's IP Address. AWS already provides DNS support for their VPC . However, they have some limitations: AWS-provided private DNS hostnames are static domain names that take the form ip-private-ipv4-address.ec2.internal for the us-east-1 Region and , and ip-private-ipv4-address.region.compute.internal for other Regions. These private DNS hostnames are only accessible between instances in the same VPC. In a multi-VPC environment, instances can't resolve the DNS hostname outside the VPC that they are in.","title":"VPC DNS"},{"location":"cloud/aws/aws-ddns-ref/#route53","text":"Route53 can be used to create custom private hosted zones. A private hosted zone is a container for one or more VPC Domain records. By creating a private hosted zone for a domain (such as gtz4all.com), and then creating records, instances can query Route 53 to find out how traffic should be routed for that domain within their VPC and among others.","title":"Route53"},{"location":"cloud/aws/aws-ddns-ref/#private-hosted-zone-requirements","text":"For a private hosted zone to be accessible, there are multiple requirements. for additional details, please refer to these links [Creating a private hosted zone] * A VPC must be associated with the Private Hosted Zone * The following VPC settings must be set to true * enableDnsHostnames * enableDnsSupport * DHCP options sets * Valid domain name`` (such as gtz4all.com) * domain-name-servers value: AmazonProvidedDNS``` for additional details, please refer to these links * Creating a private hosted zone * DHCP options sets for your VPC","title":"Private Hosted Zone Requirements"},{"location":"cloud/aws/aws-ddns-ref/#private-hosted-zone-records","text":"* * A(Address)record A record maps a hostname and a domain name to IP addresses. The value inside an A record can only be an IP Address Name Domain A Record Value ec2lab gtz4all.com ec2lab.gtz4all.com 10.168.0.10","title":"Private Hosted Zone Records"},{"location":"cloud/aws/aws-ddns-ref/#reverse-lookup-zone-records","text":"A reverse lookup zone is used primarily to resolve IP addresses to Address records. Reverse lookup Zones use a special root domain called in-and. An IP address CIDR block reverse octet order is used to create a subdomain within the in-and.ARPA domain. VPC CIDR Block Reverse Lookup Domain 10.168.0.0/16 168.10.in-addr.arpa. A DNS pointer record (PTR) provides the mapping of an IP address to a domain name. A DNS PTR record is the opposite of the 'A' record, which provides the IP address associated with a domain name. IP Address Reverse Lookup Zone PTR Record Value 10.168.0.10 168.10.in-addr.arpa. 10.0.168.10.in-addr.arpa. ec2lab.gtz4all.com","title":"Reverse Lookup Zone Records"},{"location":"cloud/aws/aws-ddns-ref/#ec2-ddns-lambda","text":"This solution uses tagging as a way to interact with the users.","title":"EC2 DDNS Lambda"},{"location":"cloud/aws/aws-ddns-ref/#overview","text":"graph TD A[ddns_lambda<br>core_ddns_lambda] -->|assume role| B(sts_ddns_lambda) B --> C{EC2} C -->|Get Tag Value| D[DNSName] C -->|Get VPC| E[SharedVPC] C -->|Route53| F[A/PTR Records] C -->|Update Tag| G[DDNS] the users create a tag key DNSName with the value of their requested DNS name. DNSName: ec2lab VPC, Private IP, State Lambda will pull resource DNSName Value to validate DNS Naming Standards valid DNS name Characters Lambda will update the tag DDNS to notify users if they provided invalid characters Lambda will get the VPC attributes where the resource resides to validate if it is properly configured to support Route53. enableDnsHostnames , enableDnsSupport set to true if it isn't already domain-name and AmazonProvidedDNS `domain name is later used to create Private Hosted Zone if it doesn't already exist VPC CIDR Block VPC_CIDR_Block is used to generate Reverse Zone Lookup and create if required. Lambda will then go into multiple phases to find out if ` DNSName value already exists or if it's unique Lambda will update tag DDNS to notify users if the provided DNSName value already exist if the provided DNSName value is unique, A Record is created in the VPC's domain name PTR Record based on the resource private ip is created in the VPC CIDR Block Reverse Lookup Zone DynamoDB DDNS used to keep track of instance attributes and DNSName Changes used to retrieve instance interface attributes when the instance has been terminated.","title":"Overview"},{"location":"cloud/aws/aws-ddns-ref/#ec2-ddns-lambda-flow","text":"","title":"EC2 DDNS Lambda Flow"},{"location":"cloud/aws/aws-ddns-ref/#infrastructure-as-code","text":"Infrastructure as code ( IaC ) solves several problems with manual and static infrastructure management. IaC provides the ability to create and destroy an environment multiple times. This provides a way to version control and create an environment lifecycle where environmental changes can transition from development, testing, and production.","title":"Infrastructure as code"},{"location":"cloud/aws/aws-ddns-ref/#terraform","text":"Terraform is an IaC (Infrastructure as Code) tool used to build and modify infrastructure safely and efficiently. It keeps track of its environment state so rerunning the tool against a code with no changes will not generate any changes.","title":"Terraform"},{"location":"cloud/aws/aws-ddns-ref/#terraform-modules","text":"Terraform modules can be used to create reusable code using terraform loop-like features such as count .","title":"Terraform Modules"},{"location":"cloud/aws/aws-ddns-ref/#terraform-module-structure","text":"![Terraform Design](/img/tf_structure.jpg)","title":"Terraform Module Structure"},{"location":"cloud/aws/aws-ddns-ref/#vpc-module","text":"The VPC Module is used to build multiple Three Tier VPCs graph TD subgraph A[Three Tier VPC1] subgraph B[Availability Zone 1] 1[Public Subnet] 2[Private Subnet] 3[Local Subnet] end subgraph C[Availability Zone 2] 4[Local Subnet] 5[Private Subnet] 6[Public Subnet] end end style 1 fill:#f5d5d5 style 2 fill:#e0ebff style 3 fill:#d8e6d8 style 6 fill:#f5d5d5 style 5 fill:#e0ebff style 4 fill:#d8e6d8 style A fill:#f5f5f5 module \"vpc_build\" { count = var . vpc_count source = \"./modules/vpc\" vpc_cidr = var . vpc_cidrs [ count . index ] vpc_name = var . vpc_names [ count . index ] vpc_domain = \"$ {var.vpc_names[count.index]} .gtz4all.com\" }","title":"VPC Module"},{"location":"cloud/aws/aws-ddns-ref/#tgw-module","text":"This module builds the Transit Gateway and One route table graph TD subgraph D[Transit Gateway] subgraph E[Multi-VPC Route Table] 7[Attachments] end end style 7 fill:#e0ebff style D fill:#f5f5f5 module \"tgw\" { source = \"./modules/tgw\" tgw_name = \"multi_vpc\" }","title":"TGW Module"},{"location":"cloud/aws/aws-ddns-ref/#networking-module","text":"This module is used to create VPC to TGW Connectivity which results in all VPCs being able to communicate with each other. graph TD subgraph A[Three Tier VPC1] 1[Availability Zone 1] 2[Availability Zone 2] end subgraph B[Three Tier VPC2] 3[Availability Zone 1] 4[Availability Zone 2] end subgraph C[Three Tier VPC3] 5[Availability Zone 1] 6[Availability Zone 2] end subgraph D[Transit Gateway] subgraph E[Multi-VPC Route Table] 7[VPC1 Attachment] --> 1 7[VPC1 Attachment] --> 2 8[VPC2 Attachment] --> 3 8[VPC2 Attachment] --> 4 9[VPC3 Attachment] --> 5 9[VPC4 Attachment] --> 6 end end style A fill:#f5f5f5 style B fill:#f5f5f5 style C fill:#f5f5f5 style D fill:#f5f5f5 style 1 fill:#d8e6d8 style 2 fill:#d8e6d8 style 3 fill:#d8e6d8 style 4 fill:#d8e6d8 style 5 fill:#d8e6d8 style 6 fill:#d8e6d8 * variables are being pass from the vpc and tgw to the root module which in turn passes those values to the networking module module \"vpc_networking\" { count = var . vpc_count source = \"./modules/networking\" dest_cidr_block = var . vpc_to_tgw_dest_cidr tgw_id = module . tgw . tgw_id tgw_rt_id = module . tgw . tgw_rt_id vpc_id = module . vpc_build [ count . index ] . vpc_id vpc_name = var . vpc_names [ count . index ] vpc_subnet0_id = module . vpc_build [ count . index ] . local_subnet0_id vpc_subnet1_id = module . vpc_build [ count . index ] . local_subnet1_id public_subnet_rt_id = module . vpc_build [ count . index ] . public_rt_id private_subnet_rt_id = module . vpc_build [ count . index ] . private_rt_id local_subnet_rt_id = module . vpc_build [ count . index ] . local_rt_id }","title":"Networking Module"},{"location":"cloud/aws/aws-ddns-ref/#iam-module","text":"This module first creates the core_ddns_lambda role and policy. It then uses the core_ddns_lambda ARN to generate the sts_ddns_lambda policy which is associated with its corresponding role it also creates a ec2_tag_restriction policy to maintain a healthy Route53 environment and be able to track Route53 Records correctly. Once the DNSName is created, it must remain for the lifetime of its resource. If the tag gets deleted, Lambda would not be able to perform route 53 clean up like deleted A and PTR records. DDNS tag is used by lambda to update users on their DNS Name request. So users shouldn't be allowed to create or modify this tag. { \"Statement\" : [ { \"Action\" : [ \"ec2:DeleteTags\" ], \"Condition\" : { \"ForAllValues:StringEquals\" : { \"aws:TagKeys\" : [ \"DNSName\" ] } }, \"Effect\" : \"Deny\" , \"Resource\" : \"*\" , \"Sid\" : \"DenyDNSNameTagDeletion\" }, { \"Action\" : [ \"ec2:DeleteTags\" , \"ec2:CreateTags\" ], \"Condition\" : { \"ForAllValues:StringEquals\" : { \"aws:TagKeys\" : [ \"DDNS\" ] } }, \"Effect\" : \"Deny\" , \"Resource\" : \"*\" , \"Sid\" : \"DenyDDNSTagChanges\" } ], \"Version\" : \"2012-10-17\" } module \"iam_roles\" { source = \"./modules/iam\" }","title":"IAM Module"},{"location":"cloud/aws/aws-ddns-ref/#lambda-module","text":"This module builds a zip file which later is used to create the lambda function and its triggers. Events are used to trigger the creation/deletion of Route53 resources aws.ec2 trigger is used to notify lambda about the creation or termination of an EC2. aws.tag trigger is used to notify lambda about the creation or modification of the \"DNSName\" tag. module \"ddns_lambda\" { source = \"./modules/lambda\" ec2_tag_ddns_role_arn = module . iam_roles . core_ddns_role_arn }","title":"Lambda Module"},{"location":"cloud/aws/aws-ddns-ref/#ec2-module","text":"This module is just to validate ec2_tag_ddns lambda environment module \"ec2_ddns_lab\" { count = var . vpc_count source = \"./modules/ec2\" vpc_id = module . vpc_build [ count . index ] . vpc_id public_subnet_id = module . vpc_build [ count . index ] . public_subnet0_id ec2_name = \"$ {var.vpc_names[count.index]} -ddns-lab\" }","title":"EC2 Module"},{"location":"cloud/aws/aws-ddns-ref/#ec2-tag-ddns-test-drive","text":"","title":"EC2 Tag DDNS Test Drive"},{"location":"cloud/aws/aws-ddns-ref/#requirements","text":"Install Terraform Verify Terraform Installation terraform -help Install AWS CLI Configuring the AWS CLI Verify AWS CLI Access aws ec2 describe-vpcs","title":"Requirements"},{"location":"cloud/aws/aws-ddns-ref/#building-infrastructure","text":"terraform init terraform plan terraform apply Do you want to perform these actions ? Terraform will perform the actions described above . Only 'yes' will be accepted to approve . Enter a value : yes Review the following Services VPC EC2 Route53 DynamoDB Lambda IAM Update EC2 DNSName Value review DDNS Tag Value Terminate Instance Review DDNS Table and Route53 Private Hosted Zone and Reverse Zone Lookup","title":"Building Infrastructure"},{"location":"cloud/aws/aws-ddns-ref/#clean-up","text":"terraform destroy Do you really want to destroy all resources ? Terraform will destroy all your managed infrastructure , as shown above . There is no undo . Only 'yes' will be accepted to confirm . Enter a value : yes Resources built by EC2_TAG_DDNS_Lambda are not destroyed, they will need to be deleted manually DynamoDB - DDNS Private Hosted Zones Reverse Lookup Zones","title":"Clean up"},{"location":"cloud/aws/aws-ddns-ref/#cross-account-ec2-dynamic-dns","text":"This design can be used across multiple accounts using the follow AWS Features: Shared VPCs EventBridge Bus AssumeRole","title":"Cross-Account EC2 Dynamic DNS"},{"location":"cloud/aws/aws-ddns-ref/#ec2-ddns-lambda-flow_1","text":"","title":"EC2 DDNS Lambda Flow"},{"location":"cloud/aws/aws-ddns-ref/#_1","text":"NOTE Currently working on a Module to take care of the steps below. Share Subnets to a child account in your organization using [Resource Access Manager] ( https://aws.amazon.com/blogs/networking-and-content-delivery/vpc-sharing-a-new-approach-to-multiple-accounts-and-vpc-management/ ) Create AssumeRole (sts)... see IAM Module STS Role Enable EventBus on Network Account Create aws.ec2 and aws.tag EventBridge Triggers towards Network Account EventBus on Child Account Create Instance on child account with DNSName Tag","title":""},{"location":"cloud/aws/default-vpc-cleanup/","text":"Delete DefaultVPC Lambda \u00b6 This lambda is used to delete all DefaultVPCs and their resources in every region * Log all work * push to BD for historical reference IAM Roles \u00b6 In order to provide this solution we need to create IAM roles in both core and child account. Core Network Lambda Role \u00b6 Access DynamoDB Table 'DefaultVPC_Cleanup' used for inventory purposes. Allows lambda to assume role 'core-network-delete-defaultvpc-lambda-role' on other accounts Logging core-network-delete-defaultvpc-lambda-policy \u00b6 { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"DefaultVPCCleanupDB\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"dynamodb:DeleteItem\" , \"dynamodb:PutItem\" , \"dynamodb:UpdateItem\" ], \"Resource\" : \"arn:aws:dynamodb:us-east-1:860014166701:table/DefaultVPC_Cleanup\" }, { \"Sid\" : \"SwitchRole\" , \"Effect\" : \"Allow\" , \"Action\" : \"sts:AssumeRole\" , \"Resource\" : [ \"arn:aws:iam::*:role/core-network-delete-defaultvpc-lambda-role\" , \"arn:aws:iam::*:role/core-network-restricted-access-lambda-role\" ] }, { \"Sid\" : \"Logging\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"logs:CreateLogGroup\" , \"logs:PutLogEvents\" ], \"Resource\" : \"*\" } ] } Trust Relationship \u00b6 { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"Service\" : \"lambda.amazonaws.com\" }, \"Action\" : \"sts:AssumeRole\" } ] } Lambda \u00b6 import sys import boto3 import botocore import json import requests from datetime import datetime from botocore.exceptions import ClientError # a way to print messages - VERBOSE = 0 will not print messages VERBOSE = 1 DeleteDefaultVpcDB = 'DefaultVPC_Cleanup' print ( 'Loading function ' + datetime . now () . time () . isoformat ()) def lambda_handler ( event , context ): print ( event ) # API Gateway Response response = { \"statusCode\" : 200 , \"headers\" : { \"Access-Control-Allow-Origin\" : \"*\" , \"Content-Type\" : \"application/json;\" , }, \"isBase64Encoded\" : False } if \"FormAwsAcct\" in event [ 'queryStringParameters' ]: #get Org Accounts getOrgAccounts = requests . get ( 'https://7bzbqo4224.execute-api.us-east-1.amazonaws.com/Production/listaccounts' ) OrgAccountsDB = getOrgAccounts . json () #print(OrgAccountsDB) for account in OrgAccountsDB : print ( 'Validating Account: ' + event [ 'queryStringParameters' ][ 'FormAwsAcct' ]) #print(account['Id']) #print(event['queryStringParameters']['FormAwsAcct']) if account [ 'Id' ] == event [ 'queryStringParameters' ][ 'FormAwsAcct' ]: account_id = event [ 'queryStringParameters' ][ 'FormAwsAcct' ] role_id = event [ 'queryStringParameters' ][ 'FormSwitchRole' ] region_id = 'us-east-1' type_id = 'ec2' # get account regions ec2 , client = switch_role ( account_id , role_id , region_id , type_id ) regions = get_regions ( client ) account_list = {} account_list [ 'AccountCleanup' ] = {} account_list [ 'AccountId' ] = account_id account_list [ 'DateTime' ] = datetime . now () . strftime ( \" %d -%m-%Y %H:%M:%S\" ) for region in regions : #if region == 'us-east-1': print ( region ) account_list [ region ] = {} try : ec2 , client = switch_role ( account_id , role_id , region , type_id ) vpcs = get_default_vpcs ( client ) print ( vpcs ) except ClientError as e : #print(e.response['Error']['Message'] + 'this is my msg test' + \"\\n\") ## for any errors add a dynamodb tab with the error msg per region account_list [ 'AccountCleanup' ][ region ] = ( 'GetDefaultVpc: ' + ( e . response [ 'Error' ][ 'Message' ])) continue #else: if ( vpcs ): #print(vpcs) for vpc in vpcs : #print(vpc) print ( \"REGION:\" + region + \" - \" + \"DefaultVPC Id: \" + vpc ) account_list [ region ][ 'DefaultVpcId' ] = ( vpc ) igw = del_igw ( ec2 , vpc ) account_list [ region ][ 'DefaultVpcIgw' ] = ( igw ) subnets = del_sub ( ec2 , vpc ) account_list [ region ][ 'DefaultVpcSubnets' ] = ( subnets ) rtb = del_rtb ( ec2 , vpc ) account_list [ region ][ 'DefaultVpcRouteTable' ] = ( rtb ) rtb_acls = del_acl ( ec2 , vpc ) account_list [ region ][ 'DefaultVpcRouteTableAcls' ] = ( rtb_acls ) secgroups = del_sgp ( ec2 , vpc ) account_list [ region ][ 'DefaultVpcSecurityGroups' ] = ( secgroups ) cleanup = del_vpc ( ec2 , vpc ) account_list [ region ][ 'DefaultVpcCleanup' ] = ( cleanup ) if ( 'Deleted Successfully' in account_list [ region ][ 'DefaultVpcCleanup' ]): account_list [ 'AccountCleanup' ][ region ] = 'Complete' else : account_list [ 'AccountCleanup' ][ region ] = 'Incomplete' else : print ( \"REGION:\" + region + \" - \" + \"DefaultVPC Id: \" + 'None' + \" \\n \" ) account_list [ region ] = { 'DefaultVpcId' : 'None' } account_list [ 'AccountCleanup' ][ region ] = 'Complete' # update DynamoDB Tables AccountList_dump = json . dumps ( account_list ) AccountList = json . loads ( AccountList_dump ) table = boto3 . resource ( 'dynamodb' ) . Table ( DeleteDefaultVpcDB ) table . put_item ( Item = AccountList ) response [ 'body' ] = ( AccountList_dump ) print ( response ) return ( response ) break else : response [ 'body' ] = ( '<p style=\"text-align: center;\">You have provided an invalid BCH Managed AWS Account. Please resubmit your request - <a href=\"http://websvc4:8090/display/AWS/Default+VPC+Removal+Webform\" target=\"_blank\">BCH AWS Default VPC Removal Webform.</a></p>' ) print ( response ) return ( response ) #update DynamoDB Tables #AccountList_dump = json.dumps(account_list) #AccountList = json.loads(AccountList_dump) #table = boto3.resource('dynamodb').Table(DeleteDefaultVpcDB) #table.put_item(Item = AccountList) #response['body'] = (AccountList_dump) #return(response) def switch_role ( acct , role , region , type ): ''' acct = aws_account# role = assume_role_name region = assume_role_region type = service type, ie ec2,s3 ''' ## some services have 'resource' resource_type = [ \"cloudformation\" , \"cloudwatch\" , \"dynamodb\" , \"ec2\" , \"glacier\" , \"iam\" , \"opsworks\" , \"s3\" , \"sns\" , \"sqs\" ] role_arn = \"arn:aws:iam:: %s :role/ %s \" % ( acct , role ) sts_connection = boto3 . client ( 'sts' ) try : assume_role = sts_connection . assume_role ( RoleArn = role_arn , RoleSessionName = \"switch_role_session\" ) except ClientError as e : print ( e . response [ 'Error' ][ 'Message' ]) sys . exit ( 1 ) try : access_key = assume_role [ 'Credentials' ][ 'AccessKeyId' ] secret_key = assume_role [ 'Credentials' ][ 'SecretAccessKey' ] session_token = assume_role [ 'Credentials' ][ 'SessionToken' ] # Creates services using Assumed role credentials if type in resource_type : resource_creds = boto3 . resource ( type , aws_access_key_id = access_key , aws_secret_access_key = secret_key , aws_session_token = session_token , region_name = region ) client_creds = boto3 . client ( type , aws_access_key_id = access_key , aws_secret_access_key = secret_key , aws_session_token = session_token , region_name = region ) else : resource_creds = 'none' client_creds = boto3 . client ( type , aws_access_key_id = access_key , aws_secret_access_key = secret_key , aws_session_token = session_token , region_name = region ) except ClientError as e : return ( e . response [ 'Error' ][ 'Message' ]) return ( resource_creds , client_creds ) def get_regions ( client ): \"\"\" Build a region list \"\"\" reg_list = [] regions = client . describe_regions () #print(regions) data_str = json . dumps ( regions ) resp = json . loads ( data_str ) region_str = json . dumps ( resp [ 'Regions' ]) region = json . loads ( region_str ) for reg in region : reg_list . append ( reg [ 'RegionName' ]) #print (reg_list) return reg_list def get_default_vpcs ( client ): vpc_list = [] vpcs = client . describe_vpcs ( Filters = [ { 'Name' : 'isDefault' , 'Values' : [ 'true' , ], }, ] ) vpcs_str = json . dumps ( vpcs ) resp = json . loads ( vpcs_str ) data = json . dumps ( resp [ 'Vpcs' ]) vpcs = json . loads ( data ) for vpc in vpcs : vpc_list . append ( vpc [ 'VpcId' ]) return vpc_list def del_igw ( ec2 , vpcid ): \"\"\" Detach and delete the internet-gateway \"\"\" vpc_resource = ec2 . Vpc ( vpcid ) igws = vpc_resource . internet_gateways . all () if igws : for igw in igws : try : print ( \"Detaching and Removing igw-id: \" , igw . id ) if ( VERBOSE == 1 ) else \"\" igw . detach_from_vpc ( VpcId = vpcid ) igw . delete ( #DryRun=True ) return ( igw . id + ': detached and deleted' ) except ClientError as e : return ( e . response [ 'Error' ][ 'Message' ]) #else: # return(vpcid + \": no IGW attached\") def del_sub ( ec2 , vpcid ): \"\"\" Delete the subnets \"\"\" vpc_resource = ec2 . Vpc ( vpcid ) subnets = vpc_resource . subnets . all () vpc_subnets = [ ec2 . Subnet ( subnet . id ) for subnet in subnets ] if vpc_subnets : subnets = [] try : for sub in vpc_subnets : print ( \"Removing sub-id: \" , sub . id ) if ( VERBOSE == 1 ) else \"\" subnets . append ( sub . id ) sub . delete ( #DryRun=True ) except ClientError as e : return ( e . response [ 'Error' ][ 'Message' ]) return ( subnets ) def del_rtb ( ec2 , vpcid ): \"\"\" Delete the route-tables \"\"\" vpc_resource = ec2 . Vpc ( vpcid ) rtbs = vpc_resource . route_tables . all () if rtbs : rtables = [] try : for rtb in rtbs : if ( rtb . associations_attribute ): if ( rtb . associations_attribute [ 0 ][ 'Main' ] == True ): print ( rtb . id + \" is the main route table, continue...\" ) if ( VERBOSE == 1 ) else \"\" rtables . append ( 'main route table: ' + rtb . id ) continue else : print ( \"Removing rtb-id: \" , rtb . id ) if ( VERBOSE == 1 ) else \"\" rtables . append ( rtb . id ) table = ec2 . RouteTable ( rtb . id ) table . delete ( #DryRun=True ) except ClientError as e : return ( e . response [ 'Error' ][ 'Message' ]) return ( rtables ) def del_acl ( ec2 , vpcid ): \"\"\" Delete the network-access-lists \"\"\" vpc_resource = ec2 . Vpc ( vpcid ) acls = vpc_resource . network_acls . all () if acls : rtable_acls = [] try : for acl in acls : if acl . is_default : print ( acl . id + \" is the default NACL, continue...\" ) if ( VERBOSE == 1 ) else \"\" rtable_acls . append ( 'default_acl: ' + acl . id ) continue else : print ( \"Removing acl-id: \" , acl . id ) if ( VERBOSE == 1 ) else \"\" rtable_acls . append ( acl . id ) acl . delete ( #DryRun=True ) except ClientError as e : return ( e . response [ 'Error' ][ 'Message' ]) return ( rtable_acls ) def del_sgp ( ec2 , vpcid ): \"\"\" Delete any security-groups \"\"\" vpc_resource = ec2 . Vpc ( vpcid ) sgps = vpc_resource . security_groups . all () if sgps : sgps_list = [] try : for sg in sgps : if sg . group_name == 'default' : print ( sg . id + \" is the default security group, continue...\" ) if ( VERBOSE == 1 ) else \"\" sgps_list . append ( 'default_security_group: ' + sg . id ) continue else : print ( \"Removing sg-id: \" , sg . id ) if ( VERBOSE == 1 ) else \"\" sgps_list . append ( sg . id ) sg . delete ( # DryRun=True ) except ClientError as e : return ( e . response [ 'Error' ][ 'Message' ]) return ( sgps_list ) def del_vpc ( ec2 , vpcid ): \"\"\" Delete the VPC \"\"\" vpc_resource = ec2 . Vpc ( vpcid ) try : print ( \"Removing vpc-id: \" , vpc_resource . id ) vpc_resource . delete ( # DryRun=True ) return ( vpc_resource . id + ' Deleted Successfully.' ) except ClientError as e : return ( e . response [ 'Error' ][ 'Message' ]) print ( \"Please remove dependencies and delete VPC manually.\" ) Confluence Webform \u00b6 Default VPC Removal Webform \u00b6 HTML/JavaScript \u00b6 < div > <!--- Submitted Animation--> < div class = \"aui\" id = \"spinner\" style = \"display: none\" > < aui-spinner size = \"large\" ></ aui-spinner > < div class = \"aui\" id = \"ClonfluenceUserName\" ></ div > < div class = \"aui\" id = \"ClonfluenceUserId\" ></ div > < p > Your request is being processed...Please wait </ p > </ div > <!--- Submitted Animation--> <!--- Webform--> < form class = \"aui\" id = \"RequestForm\" style = \"margin: 40px;\" > < h3 style = \"text-align: center;\" > AWS Account Default VPC Deletion Form </ h3 > < p style = \"text-align: center;\" > Please fill in this form to delete Default VPCs from an AWS Account. in the < a href = \"http://websvc4:8090/display/AWS/BCH+Cloud+Standard+Offerings\" target = \"_blank\" > BCH AWS Organization. </ a ></ p > < label for = \"fname\" > AWS Account Number </ label > < input type = \"text\" id = \"aws_acct\" name = \"FormAwsAcct\" placeholder = \"Valid AWS Account Number\" required > < label for = \"lname\" > AWS Switch Role </ label > < input type = \"text\" id = \"switch_role\" name = \"FormSwitchRole\" placeholder = \"AWS Assume Role - core-network-restricted-access-lambda-role\" required > < label for = \"email\" > Email Address </ label > < input type = \"text\" id = \"email\" name = \"FormEmail\" placeholder = \"Requester BCH Email Address - This is not currently being used but it is a required field\" required > < button class = \"aui-button aui-button-primary\" type = \"button\" id = \"button-spin-2\" onclick = \"runAPIGetRequest()\" > Submit </ button > < p >< small > When you click the \"Submit\" button, an API Call will be made with the information you've provided. If you need any assistance, please reach out to the BCH Cloud Team. </ small ></ p > </ form > <!--- Webform--> <!--- Response Placeholder--> < div class = \"aui\" id = \"submittedMsg\" ></ div > <!--- Response Placeholder--> </ div > < script type = \"text/javascript\" > //Get Information from confluence var UserName = AJS . params . userDisplayName ; var UserId = AJS . params . remoteUser . toUpperCase (); //alert(AJS.Meta.get(\"user-display-name\")); //alert(AJS.Meta.get(\"remote-user\")); function checkRequired () { var i , GetInputs , allowSubmit ; allowSubmit = true ; GetInputs = document . getElementsByTagName ( 'input' ); for ( i = 0 ; i < GetInputs . length ; i += 1 ) { if ( GetInputs [ i ]. hasAttribute ( 'required' )){ if (( GetInputs [ i ]. value == null ) || ( GetInputs [ i ]. value == \"\" )) { GetInputs [ i ]. style . backgroundColor = \"#e6f2ff\" ; allowSubmit = false ; console . log ( \"checkRequired():\" + \"Name:\" + GetInputs [ i ]. name + \" Missing: \" + GetInputs [ i ]. placeholder ) } else { GetInputs [ i ]. style . backgroundColor = \"#ffffff\" ; console . log ( \"checkRequired():\" + \"Name:\" + GetInputs [ i ]. name + \" Type: \" + GetInputs [ i ]. type . toLowerCase () + \" Value: \" + GetInputs [ i ]. value ) } } } return allowSubmit ; } function buildParms () { var GetInputs ; var i ; var parms , tok , parms = \"\" ; GetInputs = document . getElementsByTagName ( 'input' ); for ( i = 0 ; i < GetInputs . length ; i += 1 ) { if ( parms === \"\" ) { tok = \"?\" ;} else { tok = \"&\" ;} parms = parms + tok + GetInputs [ i ]. name + \"=\" + encodeURIComponent ( GetInputs [ i ]. value ); console . log ( GetInputs [ i ]. name + \"=\" + encodeURIComponent ( GetInputs [ i ]. value )); } console . log ( parms ); return parms ; } function runAPIGetRequest () { if ( checkRequired ()) { // Public API DNS Names //myAPI = \"https://609k7k2nn4.execute-api.us-east-1.amazonaws.com/core/getjson\" //myAPI = \"https://609k7k2nn4.execute-api.us-east-1.amazonaws.com/core/defaultvpcremoval\" // Private API DNS Names //myAPI = \"https://609k7k2nn4-vpce-0897455697c88d3cd.execute-api.us-east-1.amazonaws.com/core/getjson\" myAPI = \"https://609k7k2nn4-vpce-0897455697c88d3cd.execute-api.us-east-1.amazonaws.com/core/defaultvpcremoval\" formParms = \"\" ; formParms = buildParms (); // adding Confluence Information formParms = ( formParms + \"&\" + \"UserName\" + \"=\" + UserName + \"&\" + \"UserId\" + \"=\" + UserId ) document . getElementById ( \"RequestForm\" ). style . display = \"none\" ; // hide buttons document . getElementById ( \"spinner\" ). style . display = \"block\" ; // submitted animation document . getElementById ( \"ClonfluenceUserName\" ). innerHTML = UserName document . getElementById ( \"ClonfluenceUserId\" ). innerHTML = UserId var urlWithParms = myAPI + formParms ; xmlhttp = new XMLHttpRequest (); xmlhttp . open ( \"GET\" , urlWithParms , true ); xmlhttp . send (); xmlhttp . onreadystatechange = function () { if ( xmlhttp . readyState == 4 && xmlhttp . status == 200 ) { document . getElementById ( \"submittedMsg\" ). innerHTML = xmlhttp . responseText ; document . getElementById ( \"spinner\" ). style . display = \"none\" ; // submitted animation } else if ( xmlhttp . status != 200 ) { document . getElementById ( \"submittedMsg\" ). innerHTML = '<h6 style=\"color: #e63900; font-weight: bold;\" > There was an issue with your request. Please reach out to BCH Cloud Team</h6>' ; document . getElementById ( \"spinner\" ). style . display = \"none\" ; // submitted animation } }; } } </ script > CSS \u00b6 ```CSS .sectionMacroRow form{ border-radius: 5px; width: 800px; padding: 20px; -moz-box-shadow:5px 5px 5px 5px rgba(0,0,0,0.3); -webkit-box-shadow:5px 5px 5px 5px rgba(0,0,0,0.3); box-shadow:5px 5px 5px 5px rgba(0,0,0,0.3); } .sectionMacroRow form input[type=text], select { width: 100%; padding: 12px 20px; margin: 8px 0; display: block; border: 1px solid #ccc; border-radius: 4px; box-sizing: border-box; } .sectionMacroRow form input[type=submit] { width: 100%; background-color: #42526e; color: white; padding: 14px 20px; margin: 8px 0; display: block; text-align: center; border: 3px solid #deebff; border-radius: 4px; cursor: pointer; } .sectionMacroRow form input[type=submit]:hover { background-color: #0049b0; } ```","title":"Default Vpc Cleanup"},{"location":"cloud/aws/default-vpc-cleanup/#delete-defaultvpc-lambda","text":"This lambda is used to delete all DefaultVPCs and their resources in every region * Log all work * push to BD for historical reference","title":"Delete DefaultVPC Lambda"},{"location":"cloud/aws/default-vpc-cleanup/#iam-roles","text":"In order to provide this solution we need to create IAM roles in both core and child account.","title":"IAM Roles"},{"location":"cloud/aws/default-vpc-cleanup/#core-network-lambda-role","text":"Access DynamoDB Table 'DefaultVPC_Cleanup' used for inventory purposes. Allows lambda to assume role 'core-network-delete-defaultvpc-lambda-role' on other accounts Logging","title":"Core Network  Lambda Role"},{"location":"cloud/aws/default-vpc-cleanup/#core-network-delete-defaultvpc-lambda-policy","text":"{ \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"DefaultVPCCleanupDB\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"dynamodb:DeleteItem\" , \"dynamodb:PutItem\" , \"dynamodb:UpdateItem\" ], \"Resource\" : \"arn:aws:dynamodb:us-east-1:860014166701:table/DefaultVPC_Cleanup\" }, { \"Sid\" : \"SwitchRole\" , \"Effect\" : \"Allow\" , \"Action\" : \"sts:AssumeRole\" , \"Resource\" : [ \"arn:aws:iam::*:role/core-network-delete-defaultvpc-lambda-role\" , \"arn:aws:iam::*:role/core-network-restricted-access-lambda-role\" ] }, { \"Sid\" : \"Logging\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"logs:CreateLogGroup\" , \"logs:PutLogEvents\" ], \"Resource\" : \"*\" } ] }","title":"core-network-delete-defaultvpc-lambda-policy"},{"location":"cloud/aws/default-vpc-cleanup/#trust-relationship","text":"{ \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"Service\" : \"lambda.amazonaws.com\" }, \"Action\" : \"sts:AssumeRole\" } ] }","title":"Trust Relationship"},{"location":"cloud/aws/default-vpc-cleanup/#lambda","text":"import sys import boto3 import botocore import json import requests from datetime import datetime from botocore.exceptions import ClientError # a way to print messages - VERBOSE = 0 will not print messages VERBOSE = 1 DeleteDefaultVpcDB = 'DefaultVPC_Cleanup' print ( 'Loading function ' + datetime . now () . time () . isoformat ()) def lambda_handler ( event , context ): print ( event ) # API Gateway Response response = { \"statusCode\" : 200 , \"headers\" : { \"Access-Control-Allow-Origin\" : \"*\" , \"Content-Type\" : \"application/json;\" , }, \"isBase64Encoded\" : False } if \"FormAwsAcct\" in event [ 'queryStringParameters' ]: #get Org Accounts getOrgAccounts = requests . get ( 'https://7bzbqo4224.execute-api.us-east-1.amazonaws.com/Production/listaccounts' ) OrgAccountsDB = getOrgAccounts . json () #print(OrgAccountsDB) for account in OrgAccountsDB : print ( 'Validating Account: ' + event [ 'queryStringParameters' ][ 'FormAwsAcct' ]) #print(account['Id']) #print(event['queryStringParameters']['FormAwsAcct']) if account [ 'Id' ] == event [ 'queryStringParameters' ][ 'FormAwsAcct' ]: account_id = event [ 'queryStringParameters' ][ 'FormAwsAcct' ] role_id = event [ 'queryStringParameters' ][ 'FormSwitchRole' ] region_id = 'us-east-1' type_id = 'ec2' # get account regions ec2 , client = switch_role ( account_id , role_id , region_id , type_id ) regions = get_regions ( client ) account_list = {} account_list [ 'AccountCleanup' ] = {} account_list [ 'AccountId' ] = account_id account_list [ 'DateTime' ] = datetime . now () . strftime ( \" %d -%m-%Y %H:%M:%S\" ) for region in regions : #if region == 'us-east-1': print ( region ) account_list [ region ] = {} try : ec2 , client = switch_role ( account_id , role_id , region , type_id ) vpcs = get_default_vpcs ( client ) print ( vpcs ) except ClientError as e : #print(e.response['Error']['Message'] + 'this is my msg test' + \"\\n\") ## for any errors add a dynamodb tab with the error msg per region account_list [ 'AccountCleanup' ][ region ] = ( 'GetDefaultVpc: ' + ( e . response [ 'Error' ][ 'Message' ])) continue #else: if ( vpcs ): #print(vpcs) for vpc in vpcs : #print(vpc) print ( \"REGION:\" + region + \" - \" + \"DefaultVPC Id: \" + vpc ) account_list [ region ][ 'DefaultVpcId' ] = ( vpc ) igw = del_igw ( ec2 , vpc ) account_list [ region ][ 'DefaultVpcIgw' ] = ( igw ) subnets = del_sub ( ec2 , vpc ) account_list [ region ][ 'DefaultVpcSubnets' ] = ( subnets ) rtb = del_rtb ( ec2 , vpc ) account_list [ region ][ 'DefaultVpcRouteTable' ] = ( rtb ) rtb_acls = del_acl ( ec2 , vpc ) account_list [ region ][ 'DefaultVpcRouteTableAcls' ] = ( rtb_acls ) secgroups = del_sgp ( ec2 , vpc ) account_list [ region ][ 'DefaultVpcSecurityGroups' ] = ( secgroups ) cleanup = del_vpc ( ec2 , vpc ) account_list [ region ][ 'DefaultVpcCleanup' ] = ( cleanup ) if ( 'Deleted Successfully' in account_list [ region ][ 'DefaultVpcCleanup' ]): account_list [ 'AccountCleanup' ][ region ] = 'Complete' else : account_list [ 'AccountCleanup' ][ region ] = 'Incomplete' else : print ( \"REGION:\" + region + \" - \" + \"DefaultVPC Id: \" + 'None' + \" \\n \" ) account_list [ region ] = { 'DefaultVpcId' : 'None' } account_list [ 'AccountCleanup' ][ region ] = 'Complete' # update DynamoDB Tables AccountList_dump = json . dumps ( account_list ) AccountList = json . loads ( AccountList_dump ) table = boto3 . resource ( 'dynamodb' ) . Table ( DeleteDefaultVpcDB ) table . put_item ( Item = AccountList ) response [ 'body' ] = ( AccountList_dump ) print ( response ) return ( response ) break else : response [ 'body' ] = ( '<p style=\"text-align: center;\">You have provided an invalid BCH Managed AWS Account. Please resubmit your request - <a href=\"http://websvc4:8090/display/AWS/Default+VPC+Removal+Webform\" target=\"_blank\">BCH AWS Default VPC Removal Webform.</a></p>' ) print ( response ) return ( response ) #update DynamoDB Tables #AccountList_dump = json.dumps(account_list) #AccountList = json.loads(AccountList_dump) #table = boto3.resource('dynamodb').Table(DeleteDefaultVpcDB) #table.put_item(Item = AccountList) #response['body'] = (AccountList_dump) #return(response) def switch_role ( acct , role , region , type ): ''' acct = aws_account# role = assume_role_name region = assume_role_region type = service type, ie ec2,s3 ''' ## some services have 'resource' resource_type = [ \"cloudformation\" , \"cloudwatch\" , \"dynamodb\" , \"ec2\" , \"glacier\" , \"iam\" , \"opsworks\" , \"s3\" , \"sns\" , \"sqs\" ] role_arn = \"arn:aws:iam:: %s :role/ %s \" % ( acct , role ) sts_connection = boto3 . client ( 'sts' ) try : assume_role = sts_connection . assume_role ( RoleArn = role_arn , RoleSessionName = \"switch_role_session\" ) except ClientError as e : print ( e . response [ 'Error' ][ 'Message' ]) sys . exit ( 1 ) try : access_key = assume_role [ 'Credentials' ][ 'AccessKeyId' ] secret_key = assume_role [ 'Credentials' ][ 'SecretAccessKey' ] session_token = assume_role [ 'Credentials' ][ 'SessionToken' ] # Creates services using Assumed role credentials if type in resource_type : resource_creds = boto3 . resource ( type , aws_access_key_id = access_key , aws_secret_access_key = secret_key , aws_session_token = session_token , region_name = region ) client_creds = boto3 . client ( type , aws_access_key_id = access_key , aws_secret_access_key = secret_key , aws_session_token = session_token , region_name = region ) else : resource_creds = 'none' client_creds = boto3 . client ( type , aws_access_key_id = access_key , aws_secret_access_key = secret_key , aws_session_token = session_token , region_name = region ) except ClientError as e : return ( e . response [ 'Error' ][ 'Message' ]) return ( resource_creds , client_creds ) def get_regions ( client ): \"\"\" Build a region list \"\"\" reg_list = [] regions = client . describe_regions () #print(regions) data_str = json . dumps ( regions ) resp = json . loads ( data_str ) region_str = json . dumps ( resp [ 'Regions' ]) region = json . loads ( region_str ) for reg in region : reg_list . append ( reg [ 'RegionName' ]) #print (reg_list) return reg_list def get_default_vpcs ( client ): vpc_list = [] vpcs = client . describe_vpcs ( Filters = [ { 'Name' : 'isDefault' , 'Values' : [ 'true' , ], }, ] ) vpcs_str = json . dumps ( vpcs ) resp = json . loads ( vpcs_str ) data = json . dumps ( resp [ 'Vpcs' ]) vpcs = json . loads ( data ) for vpc in vpcs : vpc_list . append ( vpc [ 'VpcId' ]) return vpc_list def del_igw ( ec2 , vpcid ): \"\"\" Detach and delete the internet-gateway \"\"\" vpc_resource = ec2 . Vpc ( vpcid ) igws = vpc_resource . internet_gateways . all () if igws : for igw in igws : try : print ( \"Detaching and Removing igw-id: \" , igw . id ) if ( VERBOSE == 1 ) else \"\" igw . detach_from_vpc ( VpcId = vpcid ) igw . delete ( #DryRun=True ) return ( igw . id + ': detached and deleted' ) except ClientError as e : return ( e . response [ 'Error' ][ 'Message' ]) #else: # return(vpcid + \": no IGW attached\") def del_sub ( ec2 , vpcid ): \"\"\" Delete the subnets \"\"\" vpc_resource = ec2 . Vpc ( vpcid ) subnets = vpc_resource . subnets . all () vpc_subnets = [ ec2 . Subnet ( subnet . id ) for subnet in subnets ] if vpc_subnets : subnets = [] try : for sub in vpc_subnets : print ( \"Removing sub-id: \" , sub . id ) if ( VERBOSE == 1 ) else \"\" subnets . append ( sub . id ) sub . delete ( #DryRun=True ) except ClientError as e : return ( e . response [ 'Error' ][ 'Message' ]) return ( subnets ) def del_rtb ( ec2 , vpcid ): \"\"\" Delete the route-tables \"\"\" vpc_resource = ec2 . Vpc ( vpcid ) rtbs = vpc_resource . route_tables . all () if rtbs : rtables = [] try : for rtb in rtbs : if ( rtb . associations_attribute ): if ( rtb . associations_attribute [ 0 ][ 'Main' ] == True ): print ( rtb . id + \" is the main route table, continue...\" ) if ( VERBOSE == 1 ) else \"\" rtables . append ( 'main route table: ' + rtb . id ) continue else : print ( \"Removing rtb-id: \" , rtb . id ) if ( VERBOSE == 1 ) else \"\" rtables . append ( rtb . id ) table = ec2 . RouteTable ( rtb . id ) table . delete ( #DryRun=True ) except ClientError as e : return ( e . response [ 'Error' ][ 'Message' ]) return ( rtables ) def del_acl ( ec2 , vpcid ): \"\"\" Delete the network-access-lists \"\"\" vpc_resource = ec2 . Vpc ( vpcid ) acls = vpc_resource . network_acls . all () if acls : rtable_acls = [] try : for acl in acls : if acl . is_default : print ( acl . id + \" is the default NACL, continue...\" ) if ( VERBOSE == 1 ) else \"\" rtable_acls . append ( 'default_acl: ' + acl . id ) continue else : print ( \"Removing acl-id: \" , acl . id ) if ( VERBOSE == 1 ) else \"\" rtable_acls . append ( acl . id ) acl . delete ( #DryRun=True ) except ClientError as e : return ( e . response [ 'Error' ][ 'Message' ]) return ( rtable_acls ) def del_sgp ( ec2 , vpcid ): \"\"\" Delete any security-groups \"\"\" vpc_resource = ec2 . Vpc ( vpcid ) sgps = vpc_resource . security_groups . all () if sgps : sgps_list = [] try : for sg in sgps : if sg . group_name == 'default' : print ( sg . id + \" is the default security group, continue...\" ) if ( VERBOSE == 1 ) else \"\" sgps_list . append ( 'default_security_group: ' + sg . id ) continue else : print ( \"Removing sg-id: \" , sg . id ) if ( VERBOSE == 1 ) else \"\" sgps_list . append ( sg . id ) sg . delete ( # DryRun=True ) except ClientError as e : return ( e . response [ 'Error' ][ 'Message' ]) return ( sgps_list ) def del_vpc ( ec2 , vpcid ): \"\"\" Delete the VPC \"\"\" vpc_resource = ec2 . Vpc ( vpcid ) try : print ( \"Removing vpc-id: \" , vpc_resource . id ) vpc_resource . delete ( # DryRun=True ) return ( vpc_resource . id + ' Deleted Successfully.' ) except ClientError as e : return ( e . response [ 'Error' ][ 'Message' ]) print ( \"Please remove dependencies and delete VPC manually.\" )","title":"Lambda"},{"location":"cloud/aws/default-vpc-cleanup/#confluence-webform","text":"","title":"Confluence Webform"},{"location":"cloud/aws/default-vpc-cleanup/#default-vpc-removal-webform","text":"","title":"Default VPC Removal Webform"},{"location":"cloud/aws/default-vpc-cleanup/#htmljavascript","text":"< div > <!--- Submitted Animation--> < div class = \"aui\" id = \"spinner\" style = \"display: none\" > < aui-spinner size = \"large\" ></ aui-spinner > < div class = \"aui\" id = \"ClonfluenceUserName\" ></ div > < div class = \"aui\" id = \"ClonfluenceUserId\" ></ div > < p > Your request is being processed...Please wait </ p > </ div > <!--- Submitted Animation--> <!--- Webform--> < form class = \"aui\" id = \"RequestForm\" style = \"margin: 40px;\" > < h3 style = \"text-align: center;\" > AWS Account Default VPC Deletion Form </ h3 > < p style = \"text-align: center;\" > Please fill in this form to delete Default VPCs from an AWS Account. in the < a href = \"http://websvc4:8090/display/AWS/BCH+Cloud+Standard+Offerings\" target = \"_blank\" > BCH AWS Organization. </ a ></ p > < label for = \"fname\" > AWS Account Number </ label > < input type = \"text\" id = \"aws_acct\" name = \"FormAwsAcct\" placeholder = \"Valid AWS Account Number\" required > < label for = \"lname\" > AWS Switch Role </ label > < input type = \"text\" id = \"switch_role\" name = \"FormSwitchRole\" placeholder = \"AWS Assume Role - core-network-restricted-access-lambda-role\" required > < label for = \"email\" > Email Address </ label > < input type = \"text\" id = \"email\" name = \"FormEmail\" placeholder = \"Requester BCH Email Address - This is not currently being used but it is a required field\" required > < button class = \"aui-button aui-button-primary\" type = \"button\" id = \"button-spin-2\" onclick = \"runAPIGetRequest()\" > Submit </ button > < p >< small > When you click the \"Submit\" button, an API Call will be made with the information you've provided. If you need any assistance, please reach out to the BCH Cloud Team. </ small ></ p > </ form > <!--- Webform--> <!--- Response Placeholder--> < div class = \"aui\" id = \"submittedMsg\" ></ div > <!--- Response Placeholder--> </ div > < script type = \"text/javascript\" > //Get Information from confluence var UserName = AJS . params . userDisplayName ; var UserId = AJS . params . remoteUser . toUpperCase (); //alert(AJS.Meta.get(\"user-display-name\")); //alert(AJS.Meta.get(\"remote-user\")); function checkRequired () { var i , GetInputs , allowSubmit ; allowSubmit = true ; GetInputs = document . getElementsByTagName ( 'input' ); for ( i = 0 ; i < GetInputs . length ; i += 1 ) { if ( GetInputs [ i ]. hasAttribute ( 'required' )){ if (( GetInputs [ i ]. value == null ) || ( GetInputs [ i ]. value == \"\" )) { GetInputs [ i ]. style . backgroundColor = \"#e6f2ff\" ; allowSubmit = false ; console . log ( \"checkRequired():\" + \"Name:\" + GetInputs [ i ]. name + \" Missing: \" + GetInputs [ i ]. placeholder ) } else { GetInputs [ i ]. style . backgroundColor = \"#ffffff\" ; console . log ( \"checkRequired():\" + \"Name:\" + GetInputs [ i ]. name + \" Type: \" + GetInputs [ i ]. type . toLowerCase () + \" Value: \" + GetInputs [ i ]. value ) } } } return allowSubmit ; } function buildParms () { var GetInputs ; var i ; var parms , tok , parms = \"\" ; GetInputs = document . getElementsByTagName ( 'input' ); for ( i = 0 ; i < GetInputs . length ; i += 1 ) { if ( parms === \"\" ) { tok = \"?\" ;} else { tok = \"&\" ;} parms = parms + tok + GetInputs [ i ]. name + \"=\" + encodeURIComponent ( GetInputs [ i ]. value ); console . log ( GetInputs [ i ]. name + \"=\" + encodeURIComponent ( GetInputs [ i ]. value )); } console . log ( parms ); return parms ; } function runAPIGetRequest () { if ( checkRequired ()) { // Public API DNS Names //myAPI = \"https://609k7k2nn4.execute-api.us-east-1.amazonaws.com/core/getjson\" //myAPI = \"https://609k7k2nn4.execute-api.us-east-1.amazonaws.com/core/defaultvpcremoval\" // Private API DNS Names //myAPI = \"https://609k7k2nn4-vpce-0897455697c88d3cd.execute-api.us-east-1.amazonaws.com/core/getjson\" myAPI = \"https://609k7k2nn4-vpce-0897455697c88d3cd.execute-api.us-east-1.amazonaws.com/core/defaultvpcremoval\" formParms = \"\" ; formParms = buildParms (); // adding Confluence Information formParms = ( formParms + \"&\" + \"UserName\" + \"=\" + UserName + \"&\" + \"UserId\" + \"=\" + UserId ) document . getElementById ( \"RequestForm\" ). style . display = \"none\" ; // hide buttons document . getElementById ( \"spinner\" ). style . display = \"block\" ; // submitted animation document . getElementById ( \"ClonfluenceUserName\" ). innerHTML = UserName document . getElementById ( \"ClonfluenceUserId\" ). innerHTML = UserId var urlWithParms = myAPI + formParms ; xmlhttp = new XMLHttpRequest (); xmlhttp . open ( \"GET\" , urlWithParms , true ); xmlhttp . send (); xmlhttp . onreadystatechange = function () { if ( xmlhttp . readyState == 4 && xmlhttp . status == 200 ) { document . getElementById ( \"submittedMsg\" ). innerHTML = xmlhttp . responseText ; document . getElementById ( \"spinner\" ). style . display = \"none\" ; // submitted animation } else if ( xmlhttp . status != 200 ) { document . getElementById ( \"submittedMsg\" ). innerHTML = '<h6 style=\"color: #e63900; font-weight: bold;\" > There was an issue with your request. Please reach out to BCH Cloud Team</h6>' ; document . getElementById ( \"spinner\" ). style . display = \"none\" ; // submitted animation } }; } } </ script >","title":"HTML/JavaScript"},{"location":"cloud/aws/default-vpc-cleanup/#css","text":"```CSS .sectionMacroRow form{ border-radius: 5px; width: 800px; padding: 20px; -moz-box-shadow:5px 5px 5px 5px rgba(0,0,0,0.3); -webkit-box-shadow:5px 5px 5px 5px rgba(0,0,0,0.3); box-shadow:5px 5px 5px 5px rgba(0,0,0,0.3); } .sectionMacroRow form input[type=text], select { width: 100%; padding: 12px 20px; margin: 8px 0; display: block; border: 1px solid #ccc; border-radius: 4px; box-sizing: border-box; } .sectionMacroRow form input[type=submit] { width: 100%; background-color: #42526e; color: white; padding: 14px 20px; margin: 8px 0; display: block; text-align: center; border: 3px solid #deebff; border-radius: 4px; cursor: pointer; } .sectionMacroRow form input[type=submit]:hover { background-color: #0049b0; } ```","title":"CSS"},{"location":"cloud/aws/ec2-dynamic-dns/","text":"EC2 Dynamic DNS provides a simple integration betwee private and hybrid cloud. Dynamic DNS solutions for EC2's and ALB's - Tag Key: DNSName - Tag Value: must not include special characters Combine into a functional lambda Tagging solution - 3 into One - VPC must be shared -- maybe look for shared tag on remote accounts -- Function to tell the difference between EC2's Tags and ALBs split function and pull fuctions as needed Import event data as json def load_event ( file_name ): \"\"\"This function load file as variable.\"\"\" try : with open ( file_name ) as fileobj : event = json . loads ( fileobj . read ()) except ClientError as e : print ( e . response [ 'Error' ][ 'Message' ]) return event CloudWatch EC2 State Event \u00b6 { \"version\" : \"0\" , \"id\" : \"44265205-9501-dd5f-39b5-6af68926b286\" , \"detail-type\" : \"EC2 Instance State-change Notification\" , \"source\" : \"aws.ec2\" , \"account\" : \"089714066692\" , \"time\" : \"2021-08-29T17:03:52Z\" , \"region\" : \"us-east-1\" , \"resources\" :[ \"arn:aws:ec2:us-east-1:089714066692:instance/i-0389a92bd5b886c39\" ], \"detail\" :{ \"instance-id\" : \"i-0389a92bd5b886c39\" , \"state\" : \"running\" } } Steps \u00b6 Create VPC https://gitlab.com/gtz4all/aws-three-tier-vpc-cf/-/blob/main/gtz4all-three-tier-vpc-nested-conditions.yml Create Hosted Zone based on project name and domain sbox.gtz4all.com Associate Private Zone with VPC Create Lambda Role sbox-dev-ddns-lambda-role/policy { \"Statement\" : [ { \"Action\" : [ \"logs:CreateLogStream\" , \"ec2:Describe*\" , \"ec2:CreateTags\" , \"ec2:DeleteTags\" , \"route53:*\" , \"dynamodb:*\" , \"logs:CreateLogGroup\" , \"logs:PutLogEvents\" ], \"Condition\" : { \"ForAllValues:StringEquals\" : { \"aws:TagKeys\" : [ \"DDNS\" ] }, \"StringEquals\" : { \"aws:RequestedRegion\" : [ \"us-east-1\" , \"us-east-2\" ] } }, \"Effect\" : \"Allow\" , \"Resource\" : \"*\" , \"Sid\" : \"sboxDDNSNetwork\" }, { \"Action\" : \"sts:AssumeRole\" , \"Condition\" : { \"ForAllValues:StringEquals\" : { \"aws:TagKeys\" : [ \"DDNS\" ] }, \"StringEquals\" : { \"aws:RequestedRegion\" : [ \"us-east-1\" , \"us-east-2\" ] } }, \"Effect\" : \"Allow\" , \"Resource\" : \"*\" , \"Sid\" : \"sboxDDNSClient\" } ], \"Version\" : \"2012-10-17\" } Create lambda and associate created role Adjust timeout to 5min Create EventBridge sbox-dev-ec2-state","title":"EC2 Cross-Account Dynamic DNS Design"},{"location":"cloud/aws/ec2-dynamic-dns/#cloudwatch-ec2-state-event","text":"{ \"version\" : \"0\" , \"id\" : \"44265205-9501-dd5f-39b5-6af68926b286\" , \"detail-type\" : \"EC2 Instance State-change Notification\" , \"source\" : \"aws.ec2\" , \"account\" : \"089714066692\" , \"time\" : \"2021-08-29T17:03:52Z\" , \"region\" : \"us-east-1\" , \"resources\" :[ \"arn:aws:ec2:us-east-1:089714066692:instance/i-0389a92bd5b886c39\" ], \"detail\" :{ \"instance-id\" : \"i-0389a92bd5b886c39\" , \"state\" : \"running\" } }","title":"CloudWatch EC2 State Event"},{"location":"cloud/aws/ec2-dynamic-dns/#steps","text":"Create VPC https://gitlab.com/gtz4all/aws-three-tier-vpc-cf/-/blob/main/gtz4all-three-tier-vpc-nested-conditions.yml Create Hosted Zone based on project name and domain sbox.gtz4all.com Associate Private Zone with VPC Create Lambda Role sbox-dev-ddns-lambda-role/policy { \"Statement\" : [ { \"Action\" : [ \"logs:CreateLogStream\" , \"ec2:Describe*\" , \"ec2:CreateTags\" , \"ec2:DeleteTags\" , \"route53:*\" , \"dynamodb:*\" , \"logs:CreateLogGroup\" , \"logs:PutLogEvents\" ], \"Condition\" : { \"ForAllValues:StringEquals\" : { \"aws:TagKeys\" : [ \"DDNS\" ] }, \"StringEquals\" : { \"aws:RequestedRegion\" : [ \"us-east-1\" , \"us-east-2\" ] } }, \"Effect\" : \"Allow\" , \"Resource\" : \"*\" , \"Sid\" : \"sboxDDNSNetwork\" }, { \"Action\" : \"sts:AssumeRole\" , \"Condition\" : { \"ForAllValues:StringEquals\" : { \"aws:TagKeys\" : [ \"DDNS\" ] }, \"StringEquals\" : { \"aws:RequestedRegion\" : [ \"us-east-1\" , \"us-east-2\" ] } }, \"Effect\" : \"Allow\" , \"Resource\" : \"*\" , \"Sid\" : \"sboxDDNSClient\" } ], \"Version\" : \"2012-10-17\" } Create lambda and associate created role Adjust timeout to 5min Create EventBridge sbox-dev-ec2-state","title":"Steps"},{"location":"cloud/aws/intro/","text":"AWS is currently the leading cloud provider.","title":"Introduction"},{"location":"cloud/aws/lambda-functions/","text":"Headline \u00b6 test 1 ...","title":"Lambda Functions"},{"location":"cloud/aws/lambda-functions/#headline","text":"test 1 ...","title":"Headline"},{"location":"cloud/aws/three-tier-vpc/","text":"An AWS Three-Tier VPC Design provides a baseline to deploy software applications that consist of frontend, backend and database.This design provides Availability Zone High Availability and Fault Tolerant. In addition this design also provides the required topoligy for a highly secured environment. Public Tier \u00b6 The Public Tier has both inbound/outbound VPC External access. It is used to provide external services and an entry point into the environment Recommended Resources : * Public Application Load Balancer Users will only be allowed to access application via public Application Load Balancer * NAT Gateways Provides outbound access for private tier. This is required for software updates or patches * Bastion Hosts A bastion host provides secure management access into all three tiers. These servers should be secured with MFA. Private Tier \u00b6 The Private Tier only has outbound VPC External Access which is mainly used for software patches and updates. This is where the application servers and internal load balancer reside. Recommended Resources : * Internal Application Load Balancer This type of ALBs and NLBs are used to provide a Highly Available Service to the Public Tier ALBs * Application Servers EC2s, Lambda or EKS, ECS Local Tier \u00b6 This tier does not have inbound/outbould VPC External Access. It is meant to host database services. Services inside this tier should only be access by the application services. Recommended Resources : * Database Instances Cloudformation \u00b6 Cloudformation is a tool to quickly and consistently deploy infrastructure by building the designed resources and managing them as stacks. it is a declarative programming language with some basic features specific to this use case. Componentes \u00b6 Metadata: AWS::CloudFormation::Interface is used to modify how the AWS Cloudformation groups, labels and orders user input parameters. https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html Metadata : AWS::CloudFormation::Interface : ParameterGroups : - Label : default : \"Availability Zones\" Parameters : - NumberOfAZs * Parametes: Parameters are used to request user input custom values which are later used by the Resources section to build infrastructure. https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html Parameters : NumberOfAZs : Type : Number AllowedValues : - 2 - 4 Default : 2 Conditions: Conditions are used to set parameters based on other parameter's values provided by the user. These conditions are later used to set conditial resources. ex: If Condition: Value is true, create resource. https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/conditions-section-structure.html ## Conditions are either True or False and are used to choose whether a rosource should be created or not. Conditions : ## Build4AZs is True if NumberofAZs is set to '4' Build4AZs : !Equals [ !Ref NumberOfAZs , 4 ] ## Build4AZs is True if NumberofAZs is set to '4' ProdVPC : !Equals [ !Ref Environment , prod ] ## BuildProdEnvironemt is True if both Build4AZs and ProdVPC are True BuildProdEnvironemt : !And [ !Condition Build4AZs , !Condition ProdVPC ] Resources : PublicSubnet3 : Type : AWS::EC2::Subnet ## PublicSubnet3 will be created only if Build4AZs is True Condition : Build4AZs Built-in Intrinsic Functions \u00b6 An intrinsic function used to perform a mathematical, character, or logical operation against a data item whose value is derived automatically during execution. Fn::Cidr ( !Cidr ) \u00b6 This Subnetting calculator function returns an CIDR address block list based on the `count provided. !Cidr [ ipBlock, count, cidrBits ] Parameters ipBlock : CIDR address block to be split into smaller CIDR blocks. count : The number of smaller CIDRs blocks to generate. Valid range 1 - 256. cidrBits : The number of subnet bits for the CIDR. Example Requirementes: create 12(count) /24s(cidrBits) out of a /16 (ipBlock) and pick the 6 th (!Select) CIDR block. !Select [ 5, !Cidr [ \"10.10.0.0/16\", 12, 5 ] Generated Table - !Select - select the 6th block - starts from 0 - 7 if there are 8 blocks. ```!Select [ 5,``` - !Cidr [CidrBlock = (/16), 12 = break /16 into 12 block, what size for each block? look at host-bits... ```!Cidr [ \"10.10.0.0/16\", 12,``` - SubnetBits/cidrBits = 8 - which means host-bits of 8, (/24s) ```8]]``` ***cidrBit Table*** Suffix|IPs|CIDR|Borrowed Bits|Binary -|-|-|-|- .255|1 |/32|0|11111111 .254|2 |/31|1|11111110 .252|4 |/30|2|11111100 .248|8 |/29|3|11111000 .240|16 |/28|4|11110000 .224|32 |/27|5|11100000 .192|64 |/26|6|11000000 .128|128|/25|7|10000000 .0|256|/24|8|00000000 Count|CidrBlocks|Select -|-|- 0|10.10.0.0/24| 1|10.10.1.0/24| 2|10.10.2.0/24| 3|10.10.3.0/24| 4|10.10.4.0/24| 5|10.10.5.0/24|!Select 6|10.10.6.0/24| 7|10.10.7.0/24| 8|10.10.8.0/24| 9|10.10.9.0/24| 10|10.10.10.0/24| 11|10.10.11.0/24| 12|10.10.12.0/24| Conditional Subnetting !Select [ !If [ BuildProd , 0 , 0 ], !Cidr [ !Ref CidrBlock , !If [ BuildProd , 16 , 8 ], !If [ BuildProd , 5 , 6 ]]] Conditial !Cidr Condition !if - if BuildPord is True - Create 16 /27s (Host-Bits - 5 ) blocks - if BuildPord is False - Create 8 /26s (Host-Bits - 6 ) blocks !Select [ !If [ BuildProd , 0 , 0 ], !Cidr [ !Ref CidrBlock , !If [ BuildProd , 16 , 8 ], !If [ BuildProd , 5 , 6 ]]] !If Condition|Boolean|Value -|-|- BluidProd|True|firstValue BluidProd|False|secondValue ***!Cidr*** ipBlock|count|cidrBits -|-|- !Ref CidrBlock|!If [BuildProd, 16, 8]|!If [BuildProd, 5, 6] Fn::GetAZs (!GetAZs) \u00b6 The intrinsic function Fn::GetAZs returns a Availability Zone Name List based for a region. AZ Name to AZ ID mapping is different per account. !GetAZs \"\" in us-east-1 returns a list of all Availability Zones: [ \"us-east-1a\", \"us-east-1b\", \"us-east-1c\", \"us-east-1d\", \"us-east-1e\", \"us-east-1f\" ] 0 1 2 3 4 5 \"us-east-1a\" \"us-east-1b\" \"us-east-1c\" \"us-east-1d\" \"us-east-1e\" \"us-east-1f\" !Select can pick an AZ based on the order list. !Select [3, !GetAZs ''] returns \"us-east-1d\" . Three Tier VPC Cloudformation Template \u00b6 This template builds the Three Tier VPC based on the above topology using the built-in cloudformation functions recently mentioned. Gtz4all Three Tier VPC Template AWSTemplateFormatVersion : 2010-09-09 Description : GTZ Three Tier VPC Template Metadata : AWS::CloudFormation::Interface : ParameterGroups : - Label : default : \"Stack Name - --three-tier-vpc-cf (i.e jira-dev-three-tier-vpc-cf)\" - Label : default : \"|\" - Label : default : \"VPC Configuration\" Parameters : - ProjectName - Environment - CidrBlock - Label : default : \"Subnet 8 = /24(/20 CidrBlock Required), 7 = /25 (/21 CidrBlock Required), 6 = /26 (/22 CidrBlock Required)\" Parameters : - SubnetBits - Label : default : \"Availability Zones\" Parameters : - NumberOfAZs Parameters : ProjectName : Description : Project Name ( ex. jira / sfpt / sec ) Type : String CidrBlock : AllowedPattern : ^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\\/(1[6-9]|2[0-8]))$ ConstraintDescription : CIDR block parameter must be in the form x.x.x.x/16-28 Description : VPC valid IP CIDR Blocks /20's, /21's and /22's( ex. 10.168.0.0/20) Default : 10.168.0.0/20 Type : String SubnetBits : Type : Number AllowedValues : - 8 - 7 - 6 Default : 8 Description : Subnet Bits 8 = /24(/20 CidrBlock Required), 7 = /25 (/21 CidrBlock Required), 6 = /26 (/22 CidrBlock Required) NumberOfAZs : Type : Number AllowedValues : - 2 - 4 Default : 2 Description : Availability Zones - 2 = 2 per Tier, 4 = 4 per Tier Environment : Description : Choose VPC Type Type : String AllowedValues : - dev - test - prod Conditions : Build4AZs : !Equals [ !Ref NumberOfAZs , 4 ] ############################################### ### Three Tier VPC #### ############################################### Resources : VPC : Type : AWS::EC2::VPC Properties : CidrBlock : { Ref : CidrBlock } EnableDnsHostnames : true EnableDnsSupport : true Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , !Ref Environment , \"three-tier\" , \"vpc\" ]] DhcpOptions : Type : AWS::EC2::DHCPOptions Properties : DomainName : !Join [ '-' , [ !Ref ProjectName , \"three-tier.org\" ]] DomainNameServers : - AmazonProvidedDNS Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , !Ref Environment , \"three-tier\" , \"dopt\" ]] VPCDHCPOptionsAssociation : Type : AWS::EC2::VPCDHCPOptionsAssociation Properties : VpcId : { Ref : VPC } DhcpOptionsId : { Ref : DhcpOptions } InternetGateway : Type : AWS::EC2::InternetGateway DependsOn : VPC Properties : Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , !Ref Environment , \"three-tier\" , \"igw\" ]] VPCGatewayAttachment : Type : AWS::EC2::VPCGatewayAttachment Properties : VpcId : !Ref VPC InternetGatewayId : !Ref InternetGateway ############################################### ### Public/ALB Tier (inbound/outbound) #### ############################################### PublicRouteTableA : Type : AWS::EC2::RouteTable DependsOn : VPCGatewayAttachment Properties : Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , \"public\" , \"rt-1a\" ]] VpcId : !Ref VPC PublicRouteTableB : Type : AWS::EC2::RouteTable DependsOn : VPCGatewayAttachment Properties : Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , \"public\" , \"rt-1b\" ]] VpcId : !Ref VPC ############################################### PublicSubnet1 : Type : AWS::EC2::Subnet Properties : AvailabilityZone : !Select [ 0 , !GetAZs '' ] CidrBlock : !Select [ 0 , !Cidr [ !Ref CidrBlock , 12 , !Ref SubnetBits ]] MapPublicIpOnLaunch : false Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , \"public\" , \"subnet-1a\" ]] VpcId : !Ref VPC PublicSubnet2 : Type : AWS::EC2::Subnet Properties : AvailabilityZone : !Select [ 1 , !GetAZs '' ] CidrBlock : !Select [ 1 , !Cidr [ !Ref CidrBlock , 12 , !Ref SubnetBits ]] MapPublicIpOnLaunch : false Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , \"public\" , \"subnet-1b\" ]] VpcId : !Ref VPC PublicSubnet3 : Type : AWS::EC2::Subnet Condition : Build4AZs Properties : AvailabilityZone : !Select [ 2 , !GetAZs '' ] CidrBlock : !Select [ 2 , !Cidr [ !Ref CidrBlock , 12 , !Ref SubnetBits ]] MapPublicIpOnLaunch : false Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , \"public\" , \"subnet-1c\" ]] VpcId : !Ref VPC PublicSubnet4 : Type : AWS::EC2::Subnet Condition : Build4AZs Properties : AvailabilityZone : !Select [ 3 , !GetAZs '' ] CidrBlock : !Select [ 3 , !Cidr [ !Ref CidrBlock , 12 , !Ref SubnetBits ]] MapPublicIpOnLaunch : false Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , \"public\" , \"subnet-1d\" ]] VpcId : !Ref VPC ############################################### PublicSubnetAssoc1 : Type : AWS::EC2::SubnetRouteTableAssociation Properties : RouteTableId : !Ref PublicRouteTableA SubnetId : !Ref PublicSubnet1 PublicSubnetAssoc2 : Type : AWS::EC2::SubnetRouteTableAssociation Properties : RouteTableId : !Ref PublicRouteTableB SubnetId : !Ref PublicSubnet2 PublicSubnetAssoc3 : Type : AWS::EC2::SubnetRouteTableAssociation Condition : Build4AZs Properties : RouteTableId : !Ref PublicRouteTableA SubnetId : !Ref PublicSubnet3 PublicSubnetAssoc4 : Type : AWS::EC2::SubnetRouteTableAssociation Condition : Build4AZs Properties : RouteTableId : !Ref PublicRouteTableB SubnetId : !Ref PublicSubnet4 ############################################### ### Private/Application Tier (inbound) #### ############################################### AppRouteTableA : Type : AWS::EC2::RouteTable Properties : Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , \"private\" , \"rt-1a\" ]] VpcId : Ref : VPC AppRouteTableB : Type : AWS::EC2::RouteTable Properties : Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , \"private\" , \"rt-1b\" ]] VpcId : Ref : VPC ############################################### AppSubnet1 : Type : AWS::EC2::Subnet Properties : AvailabilityZone : !Select [ 0 , !GetAZs '' ] CidrBlock : !Select [ 4 , !Cidr [ !Ref CidrBlock , 12 , !Ref SubnetBits ]] MapPublicIpOnLaunch : false Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , \"private\" , \"subnet-1a\" ]] VpcId : !Ref VPC AppSubnet2 : Type : AWS::EC2::Subnet Properties : AvailabilityZone : !Select [ 1 , !GetAZs '' ] CidrBlock : !Select [ 5 , !Cidr [ !Ref CidrBlock , 12 , !Ref SubnetBits ]] MapPublicIpOnLaunch : false Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , \"private\" , \"subnet-1b\" ]] VpcId : !Ref VPC AppSubnet3 : Type : AWS::EC2::Subnet Condition : Build4AZs Properties : AvailabilityZone : !Select [ 2 , !GetAZs '' ] CidrBlock : !Select [ 6 , !Cidr [ !Ref CidrBlock , 12 , !Ref SubnetBits ]] MapPublicIpOnLaunch : false Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , \"private\" , \"subnet-1c\" ]] VpcId : !Ref VPC AppSubnet4 : Type : AWS::EC2::Subnet Condition : Build4AZs Properties : AvailabilityZone : !Select [ 3 , !GetAZs '' ] CidrBlock : !Select [ 7 , !Cidr [ !Ref CidrBlock , 12 , !Ref SubnetBits ]] MapPublicIpOnLaunch : false Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , \"private\" , \"subnet-1d\" ]] VpcId : !Ref VPC ############################################### AppSubnetAssoc1 : Type : AWS::EC2::SubnetRouteTableAssociation Properties : RouteTableId : !Ref AppRouteTableA SubnetId : !Ref AppSubnet1 AppSubnetAssoc2 : Type : AWS::EC2::SubnetRouteTableAssociation Properties : RouteTableId : !Ref AppRouteTableB SubnetId : !Ref AppSubnet2 AppSubnetAssoc3 : Type : AWS::EC2::SubnetRouteTableAssociation Condition : Build4AZs Properties : RouteTableId : !Ref AppRouteTableA SubnetId : !Ref AppSubnet3 AppSubnetAssoc4 : Type : AWS::EC2::SubnetRouteTableAssociation Condition : Build4AZs Properties : RouteTableId : !Ref AppRouteTableB SubnetId : !Ref AppSubnet4 ############################################### NatGatewayAEIP : Type : 'AWS::EC2::EIP' DependsOn : VPCGatewayAttachment Properties : Domain : vpc NatGatewayA : Type : 'AWS::EC2::NatGateway' DependsOn : NatGatewayAEIP Properties : AllocationId : Fn::GetAtt : - NatGatewayAEIP - AllocationId SubnetId : !Ref AppSubnet1 Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , \"natgw\" , \"1a\" ]] ############################################### NatGatewayBEIP : Type : 'AWS::EC2::EIP' DependsOn : VPCGatewayAttachment Properties : Domain : vpc NatGatewayB : Type : 'AWS::EC2::NatGateway' DependsOn : NatGatewayBEIP Properties : AllocationId : Fn::GetAtt : - NatGatewayBEIP - AllocationId SubnetId : !Ref AppSubnet2 Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , \"natgw\" , \"1b\" ]] ############################################### AppRouteTableARoute : Type : 'AWS::EC2::Route' DependsOn : VPCGatewayAttachment Properties : RouteTableId : !Ref AppRouteTableA DestinationCidrBlock : '0.0.0.0/0' NatGatewayId : !Ref NatGatewayA AppRouteTableBRoute : Type : 'AWS::EC2::Route' DependsOn : VPCGatewayAttachment Properties : RouteTableId : !Ref AppRouteTableB DestinationCidrBlock : '0.0.0.0/0' NatGatewayId : !Ref NatGatewayB ############################################### ### Local/Database Tier (local-access) #### ############################################### BDRouteTableA : Type : AWS::EC2::RouteTable Properties : Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , \"local\" , \"rt-1a\" ]] VpcId : Ref : VPC BDRouteTableB : Type : AWS::EC2::RouteTable Properties : Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , \"local\" , \"rt-1b\" ]] VpcId : Ref : VPC ############################################### BDSubnet1 : Type : AWS::EC2::Subnet Properties : AvailabilityZone : !Select [ 0 , !GetAZs '' ] CidrBlock : !Select [ 8 , !Cidr [ !Ref CidrBlock , 12 , !Ref SubnetBits ]] MapPublicIpOnLaunch : false Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , \"local\" , \"subnet-1a\" ]] VpcId : !Ref VPC BDSubnet2 : Type : AWS::EC2::Subnet Properties : AvailabilityZone : !Select [ 1 , !GetAZs '' ] CidrBlock : !Select [ 9 , !Cidr [ !Ref CidrBlock , 12 , !Ref SubnetBits ]] MapPublicIpOnLaunch : false Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , \"local\" , \"subnet-1b\" ]] VpcId : !Ref VPC BDSubnet3 : Type : AWS::EC2::Subnet Condition : Build4AZs Properties : AvailabilityZone : !Select [ 2 , !GetAZs '' ] CidrBlock : !Select [ 10 , !Cidr [ !Ref CidrBlock , 12 , !Ref SubnetBits ]] MapPublicIpOnLaunch : false Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , \"local\" , \"subnet-1c\" ]] VpcId : !Ref VPC BDSubnet4 : Type : AWS::EC2::Subnet Condition : Build4AZs Properties : AvailabilityZone : !Select [ 3 , !GetAZs '' ] CidrBlock : !Select [ 11 , !Cidr [ !Ref CidrBlock , 12 , !Ref SubnetBits ]] MapPublicIpOnLaunch : false Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , \"local\" , \"subnet-1d\" ]] VpcId : !Ref VPC ############################################### BDSubnetAssoc1 : Type : AWS::EC2::SubnetRouteTableAssociation Properties : RouteTableId : !Ref BDRouteTableA SubnetId : !Ref BDSubnet1 BDSubnetAssoc2 : Type : AWS::EC2::SubnetRouteTableAssociation Properties : RouteTableId : !Ref BDRouteTableB SubnetId : !Ref BDSubnet2 BDSubnetAssoc3 : Type : AWS::EC2::SubnetRouteTableAssociation Condition : Build4AZs Properties : RouteTableId : !Ref BDRouteTableA SubnetId : !Ref BDSubnet3 BDSubnetAssoc4 : Type : AWS::EC2::SubnetRouteTableAssociation Condition : Build4AZs Properties : RouteTableId : !Ref BDRouteTableB SubnetId : !Ref BDSubnet4 ############################################### PublicRouteTableRouteA : Type : 'AWS::EC2::Route' Properties : RouteTableId : !Ref PublicRouteTableA DestinationCidrBlock : '0.0.0.0/0' GatewayId : !Ref InternetGateway PublicRouteTableRouteB : Type : 'AWS::EC2::Route' Properties : RouteTableId : !Ref PublicRouteTableB DestinationCidrBlock : '0.0.0.0/0' GatewayId : !Ref InternetGateway ```` </div> #### Cloudformation Nested Functions The above template is prone to error depending on the user inputs. Howerver there are complicated ways to reduce some of them. * ***Problem*** : Assume user provides a CidrBlock : ``` 10.168.64.0/22``` and CidrBits:```8```. In order to generate enough Cidr Blocks for 4 AZs, the templace is requesting ```12``` smaller blocks. Since The ```/22``` CidrBlock only contains 4 ```/24's```, the cloudformation stack build will fail. * ***Solution*** : One possible solution would be to pick from a list of ```AllowedValues``` in the ```Parameters`` section and use ```Conditions``` to set ```CidrBits``` based on user choice using nested conditions in ```resources```. ``` yaml Conditions : CidrBit8 : !Equals [ !Ref Cidr , 20 ] CidrBit7 : !Equals [ !Ref Cidr , 21 ] CidrBit6 : !Equals [ !Ref Cidr , 22 ] Resources : !If [ CidrBit8 , 8 , !If [ CidrBit7 , 7 , 6 ]] * ```!If [CidrBit8, 8, !If [CidrBit7, 7, 6]]``` if CidrBit8 == True: cidrBit == 8 elif CidrBit7 == True: cidrBit == 7 else: cidrBit = 6 ###### Cloudformation Snippet AWSTemplateFormatVersion : 2010-09-09 Description : GTZ Three Tier VPC Template Metadata : AWS::CloudFormation::Interface : ParameterGroups : - Label : default : \"Stack Name - --three-tier-vpc-cf (i.e jira-dev-three-tier-vpc-cf)\" - Label : default : \"|\" - Label : default : \"VPC Configuration\" Parameters : - ProjectName - Environment - CidrBlock - Label : default : \"Subnet 8 = /24(/20 CidrBlock Required), 7 = /25 (/21 CidrBlock Required), 6 = /26 (/22 CidrBlock Required)\" Parameters : - CidrMask - Label : default : \"Availability Zones\" Parameters : - NumberOfAZs Parameters : ProjectName : Description : Project Name ( ex. jira / sfpt / sec ) Type : String CidrBlock1 : ConstraintDescription : CIDR block parameter must be in the form x.x.x.x Description : VPC valid IP Block ( ex. 10.168.0.0) Default : 10.168.0.0 Type : String Cidr : Type : Number AllowedValues : - 20 - 22 - 21 Default : 20 Description : VPC valid IP CIDR Blocks /20's ( /24's smaller blocks ) , /21's ( /25's smaller blocks )and /22's ( /26's smaller blocks ) NumberOfAZs : Type : Number AllowedValues : - 2 - 4 Default : 2 Description : Availability Zones - 2 = 2 per Tier, 4 = 4 per Tier Environment : Description : Choose VPC Type Type : String AllowedValues : - dev - test - prod Conditions : Build4AZs : !Equals [ !Ref NumberOfAZs , 4 ] CidrBit8 : !Equals [ !Ref Cidr , 20 ] CidrBit7 : !Equals [ !Ref Cidr , 21 ] CidrBit6 : !Equals [ !Ref Cidr , 22 ] ############################################### ### Three Tier VPC #### ############################################### Resources : VPC : Type : AWS::EC2::VPC Properties : CidrBlock : !Join [ '' , [ !Ref CidrBlock1 , \"/\" , !Ref Cidr ]] EnableDnsHostnames : true EnableDnsSupport : true Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , !Ref Environment , \"three-tier\" , \"vpc\" ]] DhcpOptions : Type : AWS::EC2::DHCPOptions Properties : DomainName : !Join [ '-' , [ !Ref ProjectName , \"three-tier.org\" ]] DomainNameServers : - AmazonProvidedDNS Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , !Ref Environment , \"three-tier\" , \"dopt\" ]] VPCDHCPOptionsAssociation : Type : AWS::EC2::VPCDHCPOptionsAssociation Properties : VpcId : { Ref : VPC } DhcpOptionsId : { Ref : DhcpOptions } InternetGateway : Type : AWS::EC2::InternetGateway DependsOn : VPC Properties : Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , !Ref Environment , \"three-tier\" , \"igw\" ]] VPCGatewayAttachment : Type : AWS::EC2::VPCGatewayAttachment Properties : VpcId : !Ref VPC InternetGatewayId : !Ref InternetGateway ############################################### ### Public/ALB Tier (inbound/outbound) #### ############################################### PublicRouteTableA : Type : AWS::EC2::RouteTable DependsOn : VPCGatewayAttachment Properties : Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , \"public\" , \"rt-1a\" ]] VpcId : !Ref VPC PublicRouteTableB : Type : AWS::EC2::RouteTable DependsOn : VPCGatewayAttachment Properties : Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , \"public\" , \"rt-1b\" ]] VpcId : !Ref VPC ############################################### PublicSubnet1 : Type : AWS::EC2::Subnet Properties : AvailabilityZone : !Select [ 0 , !GetAZs '' ] CidrBlock : !Select [ 0 , !Cidr [ !Join [ '' , [ !Ref CidrBlock1 , \"/\" , !Ref Cidr ]], 12 , !If [ CidrBit8 , 8 , !If [ CidrBit7 , 7 , 6 ]]]] MapPublicIpOnLaunch : false Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , \"public\" , \"subnet-1a\" ]] VpcId : !Ref VPC PublicSubnet2 : Type : AWS::EC2::Subnet Properties : AvailabilityZone : !Select [ 1 , !GetAZs '' ] CidrBlock : !Select [ 1 , !Cidr [ !Join [ '' , [ !Ref CidrBlock1 , \"/\" , !Ref Cidr ]], 12 , !If [ CidrBit8 , 8 , !If [ CidrBit7 , 7 , 6 ]]]] MapPublicIpOnLaunch : false Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , \"public\" , \"subnet-1b\" ]] VpcId : !Ref VPC PublicSubnet3 : Type : AWS::EC2::Subnet Condition : Build4AZs Properties : AvailabilityZone : !Select [ 2 , !GetAZs '' ] CidrBlock : !Select [ 2 , !Cidr [ !Join [ '' , [ !Ref CidrBlock1 , \"/\" , !Ref Cidr ]], 12 , !If [ CidrBit8 , 8 , !If [ CidrBit7 , 7 , 6 ]]]] MapPublicIpOnLaunch : false Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , \"public\" , \"subnet-1c\" ]] VpcId : !Ref VPC PublicSubnet4 : Type : AWS::EC2::Subnet Condition : Build4AZs Properties : AvailabilityZone : !Select [ 3 , !GetAZs '' ] CidrBlock : !Select [ 3 , !Cidr [ !Join [ '' , [ !Ref CidrBlock1 , \"/\" , !Ref Cidr ]], 12 , !If [ CidrBit8 , 8 , !If [ CidrBit7 , 7 , 6 ]]]] MapPublicIpOnLaunch : false Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , \"public\" , \"subnet-1d\" ]] VpcId : !Ref VPC ############################################### PublicSubnetAssoc1 : Type : AWS::EC2::SubnetRouteTableAssociation Properties : RouteTableId : !Ref PublicRouteTableA SubnetId : !Ref PublicSubnet1 PublicSubnetAssoc2 : Type : AWS::EC2::SubnetRouteTableAssociation Properties : RouteTableId : !Ref PublicRouteTableB SubnetId : !Ref PublicSubnet2 PublicSubnetAssoc3 : Type : AWS::EC2::SubnetRouteTableAssociation Condition : Build4AZs Properties : RouteTableId : !Ref PublicRouteTableA SubnetId : !Ref PublicSubnet3 PublicSubnetAssoc4 : Type : AWS::EC2::SubnetRouteTableAssociation Condition : Build4AZs Properties : RouteTableId : !Ref PublicRouteTableB SubnetId : !Ref PublicSubnet4","title":"Three tier vpc"},{"location":"cloud/aws/three-tier-vpc/#public-tier","text":"The Public Tier has both inbound/outbound VPC External access. It is used to provide external services and an entry point into the environment Recommended Resources : * Public Application Load Balancer Users will only be allowed to access application via public Application Load Balancer * NAT Gateways Provides outbound access for private tier. This is required for software updates or patches * Bastion Hosts A bastion host provides secure management access into all three tiers. These servers should be secured with MFA.","title":"Public Tier"},{"location":"cloud/aws/three-tier-vpc/#private-tier","text":"The Private Tier only has outbound VPC External Access which is mainly used for software patches and updates. This is where the application servers and internal load balancer reside. Recommended Resources : * Internal Application Load Balancer This type of ALBs and NLBs are used to provide a Highly Available Service to the Public Tier ALBs * Application Servers EC2s, Lambda or EKS, ECS","title":"Private Tier"},{"location":"cloud/aws/three-tier-vpc/#local-tier","text":"This tier does not have inbound/outbould VPC External Access. It is meant to host database services. Services inside this tier should only be access by the application services. Recommended Resources : * Database Instances","title":"Local Tier"},{"location":"cloud/aws/three-tier-vpc/#cloudformation","text":"Cloudformation is a tool to quickly and consistently deploy infrastructure by building the designed resources and managing them as stacks. it is a declarative programming language with some basic features specific to this use case.","title":"Cloudformation"},{"location":"cloud/aws/three-tier-vpc/#componentes","text":"Metadata: AWS::CloudFormation::Interface is used to modify how the AWS Cloudformation groups, labels and orders user input parameters. https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html Metadata : AWS::CloudFormation::Interface : ParameterGroups : - Label : default : \"Availability Zones\" Parameters : - NumberOfAZs * Parametes: Parameters are used to request user input custom values which are later used by the Resources section to build infrastructure. https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html Parameters : NumberOfAZs : Type : Number AllowedValues : - 2 - 4 Default : 2 Conditions: Conditions are used to set parameters based on other parameter's values provided by the user. These conditions are later used to set conditial resources. ex: If Condition: Value is true, create resource. https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/conditions-section-structure.html ## Conditions are either True or False and are used to choose whether a rosource should be created or not. Conditions : ## Build4AZs is True if NumberofAZs is set to '4' Build4AZs : !Equals [ !Ref NumberOfAZs , 4 ] ## Build4AZs is True if NumberofAZs is set to '4' ProdVPC : !Equals [ !Ref Environment , prod ] ## BuildProdEnvironemt is True if both Build4AZs and ProdVPC are True BuildProdEnvironemt : !And [ !Condition Build4AZs , !Condition ProdVPC ] Resources : PublicSubnet3 : Type : AWS::EC2::Subnet ## PublicSubnet3 will be created only if Build4AZs is True Condition : Build4AZs","title":"Componentes"},{"location":"cloud/aws/three-tier-vpc/#built-in-intrinsic-functions","text":"An intrinsic function used to perform a mathematical, character, or logical operation against a data item whose value is derived automatically during execution.","title":"Built-in Intrinsic Functions"},{"location":"cloud/aws/three-tier-vpc/#fncidr-cidr","text":"This Subnetting calculator function returns an CIDR address block list based on the `count provided. !Cidr [ ipBlock, count, cidrBits ] Parameters ipBlock : CIDR address block to be split into smaller CIDR blocks. count : The number of smaller CIDRs blocks to generate. Valid range 1 - 256. cidrBits : The number of subnet bits for the CIDR. Example Requirementes: create 12(count) /24s(cidrBits) out of a /16 (ipBlock) and pick the 6 th (!Select) CIDR block. !Select [ 5, !Cidr [ \"10.10.0.0/16\", 12, 5 ] Generated Table - !Select - select the 6th block - starts from 0 - 7 if there are 8 blocks. ```!Select [ 5,``` - !Cidr [CidrBlock = (/16), 12 = break /16 into 12 block, what size for each block? look at host-bits... ```!Cidr [ \"10.10.0.0/16\", 12,``` - SubnetBits/cidrBits = 8 - which means host-bits of 8, (/24s) ```8]]``` ***cidrBit Table*** Suffix|IPs|CIDR|Borrowed Bits|Binary -|-|-|-|- .255|1 |/32|0|11111111 .254|2 |/31|1|11111110 .252|4 |/30|2|11111100 .248|8 |/29|3|11111000 .240|16 |/28|4|11110000 .224|32 |/27|5|11100000 .192|64 |/26|6|11000000 .128|128|/25|7|10000000 .0|256|/24|8|00000000 Count|CidrBlocks|Select -|-|- 0|10.10.0.0/24| 1|10.10.1.0/24| 2|10.10.2.0/24| 3|10.10.3.0/24| 4|10.10.4.0/24| 5|10.10.5.0/24|!Select 6|10.10.6.0/24| 7|10.10.7.0/24| 8|10.10.8.0/24| 9|10.10.9.0/24| 10|10.10.10.0/24| 11|10.10.11.0/24| 12|10.10.12.0/24| Conditional Subnetting !Select [ !If [ BuildProd , 0 , 0 ], !Cidr [ !Ref CidrBlock , !If [ BuildProd , 16 , 8 ], !If [ BuildProd , 5 , 6 ]]] Conditial !Cidr Condition !if - if BuildPord is True - Create 16 /27s (Host-Bits - 5 ) blocks - if BuildPord is False - Create 8 /26s (Host-Bits - 6 ) blocks !Select [ !If [ BuildProd , 0 , 0 ], !Cidr [ !Ref CidrBlock , !If [ BuildProd , 16 , 8 ], !If [ BuildProd , 5 , 6 ]]] !If Condition|Boolean|Value -|-|- BluidProd|True|firstValue BluidProd|False|secondValue ***!Cidr*** ipBlock|count|cidrBits -|-|- !Ref CidrBlock|!If [BuildProd, 16, 8]|!If [BuildProd, 5, 6]","title":"Fn::Cidr ( !Cidr )"},{"location":"cloud/aws/three-tier-vpc/#fngetazs-getazs","text":"The intrinsic function Fn::GetAZs returns a Availability Zone Name List based for a region. AZ Name to AZ ID mapping is different per account. !GetAZs \"\" in us-east-1 returns a list of all Availability Zones: [ \"us-east-1a\", \"us-east-1b\", \"us-east-1c\", \"us-east-1d\", \"us-east-1e\", \"us-east-1f\" ] 0 1 2 3 4 5 \"us-east-1a\" \"us-east-1b\" \"us-east-1c\" \"us-east-1d\" \"us-east-1e\" \"us-east-1f\" !Select can pick an AZ based on the order list. !Select [3, !GetAZs ''] returns \"us-east-1d\" .","title":"Fn::GetAZs (!GetAZs)"},{"location":"cloud/aws/three-tier-vpc/#three-tier-vpc-cloudformation-template","text":"This template builds the Three Tier VPC based on the above topology using the built-in cloudformation functions recently mentioned. Gtz4all Three Tier VPC Template AWSTemplateFormatVersion : 2010-09-09 Description : GTZ Three Tier VPC Template Metadata : AWS::CloudFormation::Interface : ParameterGroups : - Label : default : \"Stack Name - --three-tier-vpc-cf (i.e jira-dev-three-tier-vpc-cf)\" - Label : default : \"|\" - Label : default : \"VPC Configuration\" Parameters : - ProjectName - Environment - CidrBlock - Label : default : \"Subnet 8 = /24(/20 CidrBlock Required), 7 = /25 (/21 CidrBlock Required), 6 = /26 (/22 CidrBlock Required)\" Parameters : - SubnetBits - Label : default : \"Availability Zones\" Parameters : - NumberOfAZs Parameters : ProjectName : Description : Project Name ( ex. jira / sfpt / sec ) Type : String CidrBlock : AllowedPattern : ^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\\/(1[6-9]|2[0-8]))$ ConstraintDescription : CIDR block parameter must be in the form x.x.x.x/16-28 Description : VPC valid IP CIDR Blocks /20's, /21's and /22's( ex. 10.168.0.0/20) Default : 10.168.0.0/20 Type : String SubnetBits : Type : Number AllowedValues : - 8 - 7 - 6 Default : 8 Description : Subnet Bits 8 = /24(/20 CidrBlock Required), 7 = /25 (/21 CidrBlock Required), 6 = /26 (/22 CidrBlock Required) NumberOfAZs : Type : Number AllowedValues : - 2 - 4 Default : 2 Description : Availability Zones - 2 = 2 per Tier, 4 = 4 per Tier Environment : Description : Choose VPC Type Type : String AllowedValues : - dev - test - prod Conditions : Build4AZs : !Equals [ !Ref NumberOfAZs , 4 ] ############################################### ### Three Tier VPC #### ############################################### Resources : VPC : Type : AWS::EC2::VPC Properties : CidrBlock : { Ref : CidrBlock } EnableDnsHostnames : true EnableDnsSupport : true Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , !Ref Environment , \"three-tier\" , \"vpc\" ]] DhcpOptions : Type : AWS::EC2::DHCPOptions Properties : DomainName : !Join [ '-' , [ !Ref ProjectName , \"three-tier.org\" ]] DomainNameServers : - AmazonProvidedDNS Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , !Ref Environment , \"three-tier\" , \"dopt\" ]] VPCDHCPOptionsAssociation : Type : AWS::EC2::VPCDHCPOptionsAssociation Properties : VpcId : { Ref : VPC } DhcpOptionsId : { Ref : DhcpOptions } InternetGateway : Type : AWS::EC2::InternetGateway DependsOn : VPC Properties : Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , !Ref Environment , \"three-tier\" , \"igw\" ]] VPCGatewayAttachment : Type : AWS::EC2::VPCGatewayAttachment Properties : VpcId : !Ref VPC InternetGatewayId : !Ref InternetGateway ############################################### ### Public/ALB Tier (inbound/outbound) #### ############################################### PublicRouteTableA : Type : AWS::EC2::RouteTable DependsOn : VPCGatewayAttachment Properties : Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , \"public\" , \"rt-1a\" ]] VpcId : !Ref VPC PublicRouteTableB : Type : AWS::EC2::RouteTable DependsOn : VPCGatewayAttachment Properties : Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , \"public\" , \"rt-1b\" ]] VpcId : !Ref VPC ############################################### PublicSubnet1 : Type : AWS::EC2::Subnet Properties : AvailabilityZone : !Select [ 0 , !GetAZs '' ] CidrBlock : !Select [ 0 , !Cidr [ !Ref CidrBlock , 12 , !Ref SubnetBits ]] MapPublicIpOnLaunch : false Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , \"public\" , \"subnet-1a\" ]] VpcId : !Ref VPC PublicSubnet2 : Type : AWS::EC2::Subnet Properties : AvailabilityZone : !Select [ 1 , !GetAZs '' ] CidrBlock : !Select [ 1 , !Cidr [ !Ref CidrBlock , 12 , !Ref SubnetBits ]] MapPublicIpOnLaunch : false Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , \"public\" , \"subnet-1b\" ]] VpcId : !Ref VPC PublicSubnet3 : Type : AWS::EC2::Subnet Condition : Build4AZs Properties : AvailabilityZone : !Select [ 2 , !GetAZs '' ] CidrBlock : !Select [ 2 , !Cidr [ !Ref CidrBlock , 12 , !Ref SubnetBits ]] MapPublicIpOnLaunch : false Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , \"public\" , \"subnet-1c\" ]] VpcId : !Ref VPC PublicSubnet4 : Type : AWS::EC2::Subnet Condition : Build4AZs Properties : AvailabilityZone : !Select [ 3 , !GetAZs '' ] CidrBlock : !Select [ 3 , !Cidr [ !Ref CidrBlock , 12 , !Ref SubnetBits ]] MapPublicIpOnLaunch : false Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , \"public\" , \"subnet-1d\" ]] VpcId : !Ref VPC ############################################### PublicSubnetAssoc1 : Type : AWS::EC2::SubnetRouteTableAssociation Properties : RouteTableId : !Ref PublicRouteTableA SubnetId : !Ref PublicSubnet1 PublicSubnetAssoc2 : Type : AWS::EC2::SubnetRouteTableAssociation Properties : RouteTableId : !Ref PublicRouteTableB SubnetId : !Ref PublicSubnet2 PublicSubnetAssoc3 : Type : AWS::EC2::SubnetRouteTableAssociation Condition : Build4AZs Properties : RouteTableId : !Ref PublicRouteTableA SubnetId : !Ref PublicSubnet3 PublicSubnetAssoc4 : Type : AWS::EC2::SubnetRouteTableAssociation Condition : Build4AZs Properties : RouteTableId : !Ref PublicRouteTableB SubnetId : !Ref PublicSubnet4 ############################################### ### Private/Application Tier (inbound) #### ############################################### AppRouteTableA : Type : AWS::EC2::RouteTable Properties : Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , \"private\" , \"rt-1a\" ]] VpcId : Ref : VPC AppRouteTableB : Type : AWS::EC2::RouteTable Properties : Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , \"private\" , \"rt-1b\" ]] VpcId : Ref : VPC ############################################### AppSubnet1 : Type : AWS::EC2::Subnet Properties : AvailabilityZone : !Select [ 0 , !GetAZs '' ] CidrBlock : !Select [ 4 , !Cidr [ !Ref CidrBlock , 12 , !Ref SubnetBits ]] MapPublicIpOnLaunch : false Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , \"private\" , \"subnet-1a\" ]] VpcId : !Ref VPC AppSubnet2 : Type : AWS::EC2::Subnet Properties : AvailabilityZone : !Select [ 1 , !GetAZs '' ] CidrBlock : !Select [ 5 , !Cidr [ !Ref CidrBlock , 12 , !Ref SubnetBits ]] MapPublicIpOnLaunch : false Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , \"private\" , \"subnet-1b\" ]] VpcId : !Ref VPC AppSubnet3 : Type : AWS::EC2::Subnet Condition : Build4AZs Properties : AvailabilityZone : !Select [ 2 , !GetAZs '' ] CidrBlock : !Select [ 6 , !Cidr [ !Ref CidrBlock , 12 , !Ref SubnetBits ]] MapPublicIpOnLaunch : false Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , \"private\" , \"subnet-1c\" ]] VpcId : !Ref VPC AppSubnet4 : Type : AWS::EC2::Subnet Condition : Build4AZs Properties : AvailabilityZone : !Select [ 3 , !GetAZs '' ] CidrBlock : !Select [ 7 , !Cidr [ !Ref CidrBlock , 12 , !Ref SubnetBits ]] MapPublicIpOnLaunch : false Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , \"private\" , \"subnet-1d\" ]] VpcId : !Ref VPC ############################################### AppSubnetAssoc1 : Type : AWS::EC2::SubnetRouteTableAssociation Properties : RouteTableId : !Ref AppRouteTableA SubnetId : !Ref AppSubnet1 AppSubnetAssoc2 : Type : AWS::EC2::SubnetRouteTableAssociation Properties : RouteTableId : !Ref AppRouteTableB SubnetId : !Ref AppSubnet2 AppSubnetAssoc3 : Type : AWS::EC2::SubnetRouteTableAssociation Condition : Build4AZs Properties : RouteTableId : !Ref AppRouteTableA SubnetId : !Ref AppSubnet3 AppSubnetAssoc4 : Type : AWS::EC2::SubnetRouteTableAssociation Condition : Build4AZs Properties : RouteTableId : !Ref AppRouteTableB SubnetId : !Ref AppSubnet4 ############################################### NatGatewayAEIP : Type : 'AWS::EC2::EIP' DependsOn : VPCGatewayAttachment Properties : Domain : vpc NatGatewayA : Type : 'AWS::EC2::NatGateway' DependsOn : NatGatewayAEIP Properties : AllocationId : Fn::GetAtt : - NatGatewayAEIP - AllocationId SubnetId : !Ref AppSubnet1 Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , \"natgw\" , \"1a\" ]] ############################################### NatGatewayBEIP : Type : 'AWS::EC2::EIP' DependsOn : VPCGatewayAttachment Properties : Domain : vpc NatGatewayB : Type : 'AWS::EC2::NatGateway' DependsOn : NatGatewayBEIP Properties : AllocationId : Fn::GetAtt : - NatGatewayBEIP - AllocationId SubnetId : !Ref AppSubnet2 Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , \"natgw\" , \"1b\" ]] ############################################### AppRouteTableARoute : Type : 'AWS::EC2::Route' DependsOn : VPCGatewayAttachment Properties : RouteTableId : !Ref AppRouteTableA DestinationCidrBlock : '0.0.0.0/0' NatGatewayId : !Ref NatGatewayA AppRouteTableBRoute : Type : 'AWS::EC2::Route' DependsOn : VPCGatewayAttachment Properties : RouteTableId : !Ref AppRouteTableB DestinationCidrBlock : '0.0.0.0/0' NatGatewayId : !Ref NatGatewayB ############################################### ### Local/Database Tier (local-access) #### ############################################### BDRouteTableA : Type : AWS::EC2::RouteTable Properties : Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , \"local\" , \"rt-1a\" ]] VpcId : Ref : VPC BDRouteTableB : Type : AWS::EC2::RouteTable Properties : Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , \"local\" , \"rt-1b\" ]] VpcId : Ref : VPC ############################################### BDSubnet1 : Type : AWS::EC2::Subnet Properties : AvailabilityZone : !Select [ 0 , !GetAZs '' ] CidrBlock : !Select [ 8 , !Cidr [ !Ref CidrBlock , 12 , !Ref SubnetBits ]] MapPublicIpOnLaunch : false Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , \"local\" , \"subnet-1a\" ]] VpcId : !Ref VPC BDSubnet2 : Type : AWS::EC2::Subnet Properties : AvailabilityZone : !Select [ 1 , !GetAZs '' ] CidrBlock : !Select [ 9 , !Cidr [ !Ref CidrBlock , 12 , !Ref SubnetBits ]] MapPublicIpOnLaunch : false Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , \"local\" , \"subnet-1b\" ]] VpcId : !Ref VPC BDSubnet3 : Type : AWS::EC2::Subnet Condition : Build4AZs Properties : AvailabilityZone : !Select [ 2 , !GetAZs '' ] CidrBlock : !Select [ 10 , !Cidr [ !Ref CidrBlock , 12 , !Ref SubnetBits ]] MapPublicIpOnLaunch : false Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , \"local\" , \"subnet-1c\" ]] VpcId : !Ref VPC BDSubnet4 : Type : AWS::EC2::Subnet Condition : Build4AZs Properties : AvailabilityZone : !Select [ 3 , !GetAZs '' ] CidrBlock : !Select [ 11 , !Cidr [ !Ref CidrBlock , 12 , !Ref SubnetBits ]] MapPublicIpOnLaunch : false Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , \"local\" , \"subnet-1d\" ]] VpcId : !Ref VPC ############################################### BDSubnetAssoc1 : Type : AWS::EC2::SubnetRouteTableAssociation Properties : RouteTableId : !Ref BDRouteTableA SubnetId : !Ref BDSubnet1 BDSubnetAssoc2 : Type : AWS::EC2::SubnetRouteTableAssociation Properties : RouteTableId : !Ref BDRouteTableB SubnetId : !Ref BDSubnet2 BDSubnetAssoc3 : Type : AWS::EC2::SubnetRouteTableAssociation Condition : Build4AZs Properties : RouteTableId : !Ref BDRouteTableA SubnetId : !Ref BDSubnet3 BDSubnetAssoc4 : Type : AWS::EC2::SubnetRouteTableAssociation Condition : Build4AZs Properties : RouteTableId : !Ref BDRouteTableB SubnetId : !Ref BDSubnet4 ############################################### PublicRouteTableRouteA : Type : 'AWS::EC2::Route' Properties : RouteTableId : !Ref PublicRouteTableA DestinationCidrBlock : '0.0.0.0/0' GatewayId : !Ref InternetGateway PublicRouteTableRouteB : Type : 'AWS::EC2::Route' Properties : RouteTableId : !Ref PublicRouteTableB DestinationCidrBlock : '0.0.0.0/0' GatewayId : !Ref InternetGateway ```` </div> #### Cloudformation Nested Functions The above template is prone to error depending on the user inputs. Howerver there are complicated ways to reduce some of them. * ***Problem*** : Assume user provides a CidrBlock : ``` 10.168.64.0/22``` and CidrBits:```8```. In order to generate enough Cidr Blocks for 4 AZs, the templace is requesting ```12``` smaller blocks. Since The ```/22``` CidrBlock only contains 4 ```/24's```, the cloudformation stack build will fail. * ***Solution*** : One possible solution would be to pick from a list of ```AllowedValues``` in the ```Parameters`` section and use ```Conditions``` to set ```CidrBits``` based on user choice using nested conditions in ```resources```. ``` yaml Conditions : CidrBit8 : !Equals [ !Ref Cidr , 20 ] CidrBit7 : !Equals [ !Ref Cidr , 21 ] CidrBit6 : !Equals [ !Ref Cidr , 22 ] Resources : !If [ CidrBit8 , 8 , !If [ CidrBit7 , 7 , 6 ]] * ```!If [CidrBit8, 8, !If [CidrBit7, 7, 6]]``` if CidrBit8 == True: cidrBit == 8 elif CidrBit7 == True: cidrBit == 7 else: cidrBit = 6 ###### Cloudformation Snippet AWSTemplateFormatVersion : 2010-09-09 Description : GTZ Three Tier VPC Template Metadata : AWS::CloudFormation::Interface : ParameterGroups : - Label : default : \"Stack Name - --three-tier-vpc-cf (i.e jira-dev-three-tier-vpc-cf)\" - Label : default : \"|\" - Label : default : \"VPC Configuration\" Parameters : - ProjectName - Environment - CidrBlock - Label : default : \"Subnet 8 = /24(/20 CidrBlock Required), 7 = /25 (/21 CidrBlock Required), 6 = /26 (/22 CidrBlock Required)\" Parameters : - CidrMask - Label : default : \"Availability Zones\" Parameters : - NumberOfAZs Parameters : ProjectName : Description : Project Name ( ex. jira / sfpt / sec ) Type : String CidrBlock1 : ConstraintDescription : CIDR block parameter must be in the form x.x.x.x Description : VPC valid IP Block ( ex. 10.168.0.0) Default : 10.168.0.0 Type : String Cidr : Type : Number AllowedValues : - 20 - 22 - 21 Default : 20 Description : VPC valid IP CIDR Blocks /20's ( /24's smaller blocks ) , /21's ( /25's smaller blocks )and /22's ( /26's smaller blocks ) NumberOfAZs : Type : Number AllowedValues : - 2 - 4 Default : 2 Description : Availability Zones - 2 = 2 per Tier, 4 = 4 per Tier Environment : Description : Choose VPC Type Type : String AllowedValues : - dev - test - prod Conditions : Build4AZs : !Equals [ !Ref NumberOfAZs , 4 ] CidrBit8 : !Equals [ !Ref Cidr , 20 ] CidrBit7 : !Equals [ !Ref Cidr , 21 ] CidrBit6 : !Equals [ !Ref Cidr , 22 ] ############################################### ### Three Tier VPC #### ############################################### Resources : VPC : Type : AWS::EC2::VPC Properties : CidrBlock : !Join [ '' , [ !Ref CidrBlock1 , \"/\" , !Ref Cidr ]] EnableDnsHostnames : true EnableDnsSupport : true Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , !Ref Environment , \"three-tier\" , \"vpc\" ]] DhcpOptions : Type : AWS::EC2::DHCPOptions Properties : DomainName : !Join [ '-' , [ !Ref ProjectName , \"three-tier.org\" ]] DomainNameServers : - AmazonProvidedDNS Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , !Ref Environment , \"three-tier\" , \"dopt\" ]] VPCDHCPOptionsAssociation : Type : AWS::EC2::VPCDHCPOptionsAssociation Properties : VpcId : { Ref : VPC } DhcpOptionsId : { Ref : DhcpOptions } InternetGateway : Type : AWS::EC2::InternetGateway DependsOn : VPC Properties : Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , !Ref Environment , \"three-tier\" , \"igw\" ]] VPCGatewayAttachment : Type : AWS::EC2::VPCGatewayAttachment Properties : VpcId : !Ref VPC InternetGatewayId : !Ref InternetGateway ############################################### ### Public/ALB Tier (inbound/outbound) #### ############################################### PublicRouteTableA : Type : AWS::EC2::RouteTable DependsOn : VPCGatewayAttachment Properties : Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , \"public\" , \"rt-1a\" ]] VpcId : !Ref VPC PublicRouteTableB : Type : AWS::EC2::RouteTable DependsOn : VPCGatewayAttachment Properties : Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , \"public\" , \"rt-1b\" ]] VpcId : !Ref VPC ############################################### PublicSubnet1 : Type : AWS::EC2::Subnet Properties : AvailabilityZone : !Select [ 0 , !GetAZs '' ] CidrBlock : !Select [ 0 , !Cidr [ !Join [ '' , [ !Ref CidrBlock1 , \"/\" , !Ref Cidr ]], 12 , !If [ CidrBit8 , 8 , !If [ CidrBit7 , 7 , 6 ]]]] MapPublicIpOnLaunch : false Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , \"public\" , \"subnet-1a\" ]] VpcId : !Ref VPC PublicSubnet2 : Type : AWS::EC2::Subnet Properties : AvailabilityZone : !Select [ 1 , !GetAZs '' ] CidrBlock : !Select [ 1 , !Cidr [ !Join [ '' , [ !Ref CidrBlock1 , \"/\" , !Ref Cidr ]], 12 , !If [ CidrBit8 , 8 , !If [ CidrBit7 , 7 , 6 ]]]] MapPublicIpOnLaunch : false Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , \"public\" , \"subnet-1b\" ]] VpcId : !Ref VPC PublicSubnet3 : Type : AWS::EC2::Subnet Condition : Build4AZs Properties : AvailabilityZone : !Select [ 2 , !GetAZs '' ] CidrBlock : !Select [ 2 , !Cidr [ !Join [ '' , [ !Ref CidrBlock1 , \"/\" , !Ref Cidr ]], 12 , !If [ CidrBit8 , 8 , !If [ CidrBit7 , 7 , 6 ]]]] MapPublicIpOnLaunch : false Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , \"public\" , \"subnet-1c\" ]] VpcId : !Ref VPC PublicSubnet4 : Type : AWS::EC2::Subnet Condition : Build4AZs Properties : AvailabilityZone : !Select [ 3 , !GetAZs '' ] CidrBlock : !Select [ 3 , !Cidr [ !Join [ '' , [ !Ref CidrBlock1 , \"/\" , !Ref Cidr ]], 12 , !If [ CidrBit8 , 8 , !If [ CidrBit7 , 7 , 6 ]]]] MapPublicIpOnLaunch : false Tags : - Key : Name Value : !Join [ '-' , [ !Ref ProjectName , \"public\" , \"subnet-1d\" ]] VpcId : !Ref VPC ############################################### PublicSubnetAssoc1 : Type : AWS::EC2::SubnetRouteTableAssociation Properties : RouteTableId : !Ref PublicRouteTableA SubnetId : !Ref PublicSubnet1 PublicSubnetAssoc2 : Type : AWS::EC2::SubnetRouteTableAssociation Properties : RouteTableId : !Ref PublicRouteTableB SubnetId : !Ref PublicSubnet2 PublicSubnetAssoc3 : Type : AWS::EC2::SubnetRouteTableAssociation Condition : Build4AZs Properties : RouteTableId : !Ref PublicRouteTableA SubnetId : !Ref PublicSubnet3 PublicSubnetAssoc4 : Type : AWS::EC2::SubnetRouteTableAssociation Condition : Build4AZs Properties : RouteTableId : !Ref PublicRouteTableB SubnetId : !Ref PublicSubnet4","title":"Three Tier VPC Cloudformation Template"},{"location":"cloud/azure/azure-cli/","text":"Get Subscription Details az account list ## Output [ { \"cloudName\": \"AzureCloud\", \"id\": \"4cedc5dd-e3ad-468d-bf66-32e31bdb9148\", \"isDefault\": true, \"name\": \"P1-Real Hands-On Labs\", \"state\": \"Enabled\", \"tenantId\": \"3617ef9b-98b4-40d9-ba43-e1ed6709cf0d\", \"user\": { \"name\": \"cloud_user_p_27fd299f@azurelabs.linuxacademy.com\", \"type\": \"user\" } } ] Get Subscription Id az account list --query '[].id' -o tsv Get resource group Name az group list --query '[].name' -o tsv Get resource group location az group list --query '[].location' -o tsv","title":"Azure CLI Useful Commands"},{"location":"cloud/azure/azure-ngfw/","text":"Azure Firewall forced tunneling \u00b6 When you configure a new Azure Firewall, you can route all Internet-bound traffic to a designated next hop instead of going directly to the Internet. Forced tunneling lets you redirect all internet bound traffic from Azure Firewall to your on-premises firewall or to chain it to a nearby network virtual appliance (NVA)for additional inspection. You enable a firewall for forced tunneling when you create a new firewall. As of today, it is not possible to migrate an existing firewall deployment to a forced tunneling mode. To support forced tunneling, service management traffic is separated from customer traffic. An additional dedicated subnet named AzureFirewallManagementSubnet is required with its own associated public IP address. The only route allowed on this subnet is a default route to the internet, and Border Gateway Protocol (BGP) route propagation must be disabled. Within this configuration, the AzureFirewallSubnet can now include routes to any on-premises firewall or NVA to process traffic before it's passed to the internet. You can also publish these routes via BGP to AzureFirewallSubnet if BGP route propagation is enabled on this subnet.","title":"Azure Firewall Notes"},{"location":"cloud/azure/azure-ngfw/#azure-firewall-forced-tunneling","text":"When you configure a new Azure Firewall, you can route all Internet-bound traffic to a designated next hop instead of going directly to the Internet. Forced tunneling lets you redirect all internet bound traffic from Azure Firewall to your on-premises firewall or to chain it to a nearby network virtual appliance (NVA)for additional inspection. You enable a firewall for forced tunneling when you create a new firewall. As of today, it is not possible to migrate an existing firewall deployment to a forced tunneling mode. To support forced tunneling, service management traffic is separated from customer traffic. An additional dedicated subnet named AzureFirewallManagementSubnet is required with its own associated public IP address. The only route allowed on this subnet is a default route to the internet, and Border Gateway Protocol (BGP) route propagation must be disabled. Within this configuration, the AzureFirewallSubnet can now include routes to any on-premises firewall or NVA to process traffic before it's passed to the internet. You can also publish these routes via BGP to AzureFirewallSubnet if BGP route propagation is enabled on this subnet.","title":"Azure Firewall forced tunneling"},{"location":"cloud/azure/azure-wan/","text":"{: style=\"height:150px;width:150px\"} Virtual Hub Deployment \u00b6 IP Addressing https://docs.microsoft.com/en-us/azure/virtual-wan/virtual-wan-faq#what-is-the-recommended-hub-address-space-during-hub-creation The recommended Virtual WAN hub address space is /23. Virtual WAN hub assigns subnets to various gateways (ExpressRoute, Site-to-site VPN, Point-to-site VPN, Azure Firewall, Virtual hub Router). For scenarios where NVAs are deployed inside a virtual hub, a /28 is typically carved out for the NVA instances. However, if the user were to provision multiple NVAs, a /27 subnet may be assigned. Therefore, keeping a future architecture in mind, while Virtual WAN hubs are deployed with a minimum size of /24, the recommended hub address space at creation time for user to input is /23. BGP Configuration Basics Region: South Central US Name: vhub2 Hub private address space: 10.0.0.0/24 Virtual hub capacity: 2 Routing Infrastructure Units, 3 Gbps Router, Supports 2000 VMs Hub routing preference: ExpressRoute Router ASN: 65515 ##<--- This ASN cannot be changed Site to site Site to site (VPN gateway): Enabled AS Number: 64525 ##<--- This ASN can be changed during build. It cannot be modified later. Gateway scale units: 1 scale unit - 500 Mbps x 2 - Error ``` json { \"code\" : \"DeploymentFailed\" , \"message\" : \"At least one resource deployment operation failed. Please list deployment operations for details. Please see https://aka.ms/DeployOperations for usage details.\" , \"details\" : [ { \"code\" : \"CustomAsnNotEnabledForVirtualHub\" , \"message\" : \"Virtual hub is not enabled to allow Router ASN modification. Please set ASN to 65515 or contact Support for enabling virtual hub to allow modification.\" } ] } ``` ASNs (Autonomous System Numbers) \u00b6 Reference: https://github.com/MicrosoftDocs/azure-docs/blob/main/includes/vpn-gateway-faq-bgp-include.md You can use your own public ASNs or private ASNs for both your on-premises networks and Azure virtual networks. You can't use the ranges reserved by Azure or IANA. The following ASNs are reserved by Azure or IANA: * ASNs reserved by Azure: Public ASNs: 8074, 8075, 12076 Private ASNs: 65515, 65517, 65518, 65519, 65520 ASNs reserved by IANA : 23456, 64496-64511, 65535-65551 and 429496729 Gateway Limitation \u00b6 Reference: https://github.com/MicrosoftDocs/azure-docs/blob/main/articles/expressroute/expressroute-howto-coexist-resource-manager.md If you want to use transit routing between ExpressRoute and VPN, the ASN of Azure VPN Gateway must be set to 65515 and Azure Route Server should be used. Azure VPN Gateway supports the BGP routing protocol. For ExpressRoute and Azure VPN to work together, you must keep the Autonomous System Number of your Azure VPN gateway at its default value, 65515. If you previously selected an ASN other than 65515 and you change the setting to 65515, you must reset the VPN gateway for the setting to take effect.","title":"Azure Virtual WAN Components"},{"location":"cloud/azure/azure-wan/#virtual-hub-deployment","text":"IP Addressing https://docs.microsoft.com/en-us/azure/virtual-wan/virtual-wan-faq#what-is-the-recommended-hub-address-space-during-hub-creation The recommended Virtual WAN hub address space is /23. Virtual WAN hub assigns subnets to various gateways (ExpressRoute, Site-to-site VPN, Point-to-site VPN, Azure Firewall, Virtual hub Router). For scenarios where NVAs are deployed inside a virtual hub, a /28 is typically carved out for the NVA instances. However, if the user were to provision multiple NVAs, a /27 subnet may be assigned. Therefore, keeping a future architecture in mind, while Virtual WAN hubs are deployed with a minimum size of /24, the recommended hub address space at creation time for user to input is /23. BGP Configuration Basics Region: South Central US Name: vhub2 Hub private address space: 10.0.0.0/24 Virtual hub capacity: 2 Routing Infrastructure Units, 3 Gbps Router, Supports 2000 VMs Hub routing preference: ExpressRoute Router ASN: 65515 ##<--- This ASN cannot be changed Site to site Site to site (VPN gateway): Enabled AS Number: 64525 ##<--- This ASN can be changed during build. It cannot be modified later. Gateway scale units: 1 scale unit - 500 Mbps x 2 - Error ``` json { \"code\" : \"DeploymentFailed\" , \"message\" : \"At least one resource deployment operation failed. Please list deployment operations for details. Please see https://aka.ms/DeployOperations for usage details.\" , \"details\" : [ { \"code\" : \"CustomAsnNotEnabledForVirtualHub\" , \"message\" : \"Virtual hub is not enabled to allow Router ASN modification. Please set ASN to 65515 or contact Support for enabling virtual hub to allow modification.\" } ] } ```","title":"Virtual Hub Deployment"},{"location":"cloud/azure/azure-wan/#asns-autonomous-system-numbers","text":"Reference: https://github.com/MicrosoftDocs/azure-docs/blob/main/includes/vpn-gateway-faq-bgp-include.md You can use your own public ASNs or private ASNs for both your on-premises networks and Azure virtual networks. You can't use the ranges reserved by Azure or IANA. The following ASNs are reserved by Azure or IANA: * ASNs reserved by Azure: Public ASNs: 8074, 8075, 12076 Private ASNs: 65515, 65517, 65518, 65519, 65520 ASNs reserved by IANA : 23456, 64496-64511, 65535-65551 and 429496729","title":"ASNs (Autonomous System Numbers)"},{"location":"cloud/azure/azure-wan/#gateway-limitation","text":"Reference: https://github.com/MicrosoftDocs/azure-docs/blob/main/articles/expressroute/expressroute-howto-coexist-resource-manager.md If you want to use transit routing between ExpressRoute and VPN, the ASN of Azure VPN Gateway must be set to 65515 and Azure Route Server should be used. Azure VPN Gateway supports the BGP routing protocol. For ExpressRoute and Azure VPN to work together, you must keep the Autonomous System Number of your Azure VPN gateway at its default value, 65515. If you previously selected an ASN other than 65515 and you change the setting to 65515, you must reset the VPN gateway for the setting to take effect.","title":"Gateway Limitation"},{"location":"cloud/azure/intro/","text":"Comming Soon \u00b6","title":"Introduction"},{"location":"cloud/azure/intro/#comming-soon","text":"","title":"Comming Soon"},{"location":"cloud/azure/landing-zones/","text":"Azure Landing Zone Architecture \u00b6 A landing zone is a hhierarchical approach used to group subscriptions to provide scalability, security, governance, networking and identity Landing Zone Description Platform centralized services such as networking, identity, security, and management services. Application environments such as corp, dmz, external applications. Sandbox labs and validation proof of concepts","title":"Landing Zones"},{"location":"cloud/azure/landing-zones/#azure-landing-zone-architecture","text":"A landing zone is a hhierarchical approach used to group subscriptions to provide scalability, security, governance, networking and identity Landing Zone Description Platform centralized services such as networking, identity, security, and management services. Application environments such as corp, dmz, external applications. Sandbox labs and validation proof of concepts","title":"Azure Landing Zone Architecture"},{"location":"cloud/azure/mgmt-groups/","text":"Management groups \u00b6 used to efficiently manage access, policies, and compliance in a multi-subscription environment. simplify management and security of subscriptions, resource groups, and resources. all associated subscriptions inherit governance conditions. A policy applied to management group that restrict resource creation to only authorized regions would be applied to all nested management groups, subscriptions and resources Management Group and Subscription Hierarchy \u00b6 Management groups can be nested to provide governance based on usage, role, or environment. This simplifies security policy management. A subscription cam be moved from one management group to another inheriting any security policy associated with the new management group. Management Groups Facts \u00b6 10,000 Management Groups per directory 6 levels of depth per management group not including the Root or Subscription level Management Groups or Subscription can only belong to a single parents. A management group can have multiple children - sub management groups and multiple subscriptions Root Management Group Facts \u00b6 A directory has a single top-level root management group call Tentant root group Root management group cannot be moved or deleted Landing Zones \u00b6 Reference: Management Groups - Landing Zone Management group Description Intermediate Root Management Group This management group is located directly under the tenant root group. Created with a prefix provided by the organization, which purposely avoids the usage of the root group so that organizations can move existing Azure subscriptions into the hierarchy. It also enables future scenarios. This management group is a parent to all the other management groups created by the Azure landing zone accelerator. Platform This management group contains all the platform child management groups, like management, connectivity, and identity. Management This management group contains a dedicated subscription for management, monitoring, and security. This subscription will host an Azure Log Analytics workspace, including associated solutions, and an Azure Automation account. Connectivity This management group contains a dedicated subscription for connectivity. This subscription will host the Azure networking resources required for the platform, like Azure Virtual WAN, Azure Firewall, and Azure DNS private zones. Identity This management group contains a dedicated subscription for identity. This subscription is a placeholder for Windows Server Active Directory Domain Services (AD DS) virtual machines (VMs) or Azure Active Directory Domain Services. The subscription also enables AuthN or AuthZ for workloads within the landing zones. Specific Azure policies are assigned to harden and manage the resources in the identity subscription. Landing Zones The parent management group for all the landing zone child management groups. It will have workload agnostic Azure policies assigned to ensure workloads are secure and compliant. Online The dedicated management group for online landing zones. This group is for workloads that might require direct internet inbound/outbound connectivity or for workloads that might not require a virtual network. Corp The dedicated management group for corporate landing zones. This group is for workloads that require connectivity or hybrid connectivity with the corporate network via the hub in the connectivity subscription. Sandboxes The dedicated management group for subscriptions that will only be used for testing and exploration by an organization. These subscriptions will be securely disconnected from the corporate and online landing zones. Sandboxes also have a less restrictive set of policies assigned to enable testing, exploration, and configuration of Azure services. Decommissioned The dedicated management group for landing zones that are being canceled. Canceled landing zones will be moved to this management group before deletion by Azure after 30-60 days. Note The above is just a proposal. Some organizations may consider making changes to this hierarchy in order to make relevant to their requirements and environment.","title":"Azure Management Groups"},{"location":"cloud/azure/mgmt-groups/#management-groups","text":"used to efficiently manage access, policies, and compliance in a multi-subscription environment. simplify management and security of subscriptions, resource groups, and resources. all associated subscriptions inherit governance conditions. A policy applied to management group that restrict resource creation to only authorized regions would be applied to all nested management groups, subscriptions and resources","title":"Management groups"},{"location":"cloud/azure/mgmt-groups/#management-group-and-subscription-hierarchy","text":"Management groups can be nested to provide governance based on usage, role, or environment. This simplifies security policy management. A subscription cam be moved from one management group to another inheriting any security policy associated with the new management group.","title":"Management Group and Subscription Hierarchy"},{"location":"cloud/azure/mgmt-groups/#management-groups-facts","text":"10,000 Management Groups per directory 6 levels of depth per management group not including the Root or Subscription level Management Groups or Subscription can only belong to a single parents. A management group can have multiple children - sub management groups and multiple subscriptions","title":"Management Groups Facts"},{"location":"cloud/azure/mgmt-groups/#root-management-group-facts","text":"A directory has a single top-level root management group call Tentant root group Root management group cannot be moved or deleted","title":"Root Management Group Facts"},{"location":"cloud/azure/mgmt-groups/#landing-zones","text":"Reference: Management Groups - Landing Zone Management group Description Intermediate Root Management Group This management group is located directly under the tenant root group. Created with a prefix provided by the organization, which purposely avoids the usage of the root group so that organizations can move existing Azure subscriptions into the hierarchy. It also enables future scenarios. This management group is a parent to all the other management groups created by the Azure landing zone accelerator. Platform This management group contains all the platform child management groups, like management, connectivity, and identity. Management This management group contains a dedicated subscription for management, monitoring, and security. This subscription will host an Azure Log Analytics workspace, including associated solutions, and an Azure Automation account. Connectivity This management group contains a dedicated subscription for connectivity. This subscription will host the Azure networking resources required for the platform, like Azure Virtual WAN, Azure Firewall, and Azure DNS private zones. Identity This management group contains a dedicated subscription for identity. This subscription is a placeholder for Windows Server Active Directory Domain Services (AD DS) virtual machines (VMs) or Azure Active Directory Domain Services. The subscription also enables AuthN or AuthZ for workloads within the landing zones. Specific Azure policies are assigned to harden and manage the resources in the identity subscription. Landing Zones The parent management group for all the landing zone child management groups. It will have workload agnostic Azure policies assigned to ensure workloads are secure and compliant. Online The dedicated management group for online landing zones. This group is for workloads that might require direct internet inbound/outbound connectivity or for workloads that might not require a virtual network. Corp The dedicated management group for corporate landing zones. This group is for workloads that require connectivity or hybrid connectivity with the corporate network via the hub in the connectivity subscription. Sandboxes The dedicated management group for subscriptions that will only be used for testing and exploration by an organization. These subscriptions will be securely disconnected from the corporate and online landing zones. Sandboxes also have a less restrictive set of policies assigned to enable testing, exploration, and configuration of Azure services. Decommissioned The dedicated management group for landing zones that are being canceled. Canceled landing zones will be moved to this management group before deletion by Azure after 30-60 days. Note The above is just a proposal. Some organizations may consider making changes to this hierarchy in order to make relevant to their requirements and environment.","title":"Landing Zones"},{"location":"cloud/azure/networking/","text":"Virtual networks \u00b6 Features \u00b6 private and secure infrastructure VM residing inside a virtual network are able to privately communicate with each other access other azure resources Network Segmentation Virtual Networks are isolated from each other Per Subscription / Region VNETs are region and subscription specific. Components \u00b6 Subnets Subnets devide a virtual network into smaller segments. These segments can have unique security policies from each other. Some services require dedicated subnets such as network gateways, Bastion, and SQL Managed Instances Network Interfaces Network interfaces are use to connect virtuals machines to virtual networks. a virtual machine can have network interfaces in different subnets Network Security Groups (NSGs) Security groups filter inbound/outbound source/destination IP address/Port traffic based on a priority list of rules. they can be associated to either subnets or network interfaces . NSGs are region specific ( just like VMs ) Application Security Groups (ASGs) ASGs are used to group network interfaces based on purpose/usage. an ASG is similar to a tag on the network interface. a Network Security Group Rule can it destination set to an ASG. graph TD subgraph D[Virtual Private Network 10.0.0.0/26] subgraph E[Subnet 10.0.1.0/24] 1[Virtual Machine 1] 2[Application Security Group: ASG1] --> 1 end subgraph F[Subnet 10.0.2.0/24] 3[Virtual Machine 3] 4[Application Security Group: ASG1] --> 3 end 5[Network Security Group: NSG1<br>Direction: Inbound<br>Source: Internet<br>Destination: ASG1<br>Service: HTTPS<br>Action: Allow] end style 4 fill:#F0FFFF style 2 fill:#F0FFFF style D fill:#FFFFFF style 5 fill:#F0FFFF style E fill:#F0F8FF style F fill:#F0F8FF Azure Firewalls \u00b6 Features \u00b6 Azure Firewall as a Service is used to provide monitor and protection based on rules between virtual network egress/ingress traffic provides defense in depth Azure firewall is deployed between VNET 1 and VNET2 and the Internet. incomming traffic from the internet will be inspected by the Azure firewall before allowing or denying access to VNET1/VNET2 graph TD subgraph D[Azure] subgraph A[Virtual Network1] subgraph AA[Subnet] AAA[VM] end end subgraph B[Virtual Network1] subgraph BB[Subnet] BBB[VM] end end subgraph C[Virtual Network1] subgraph CC[Subnet] CCC[Azure Firewall] end end end F[Internet] style D fill:#FFFFFF style A fill:#F0FFFF style B fill:#F0FFFF style C fill:#F0FFFF style AA fill:#FFFFFF style BB fill:#FFFFFF style CC fill:#FFFFFF style AAA fill:#F0FFFF style BBB fill:#F0FFFF style CCC fill:#F0FFFF Endpoint Services \u00b6 A service endpoints provide private connectivity between Virtual Networks and a range of Azure services, including Azure Storage, SQL Databases, and Key Vault. communication travels across Azure backbone graph TD subgraph D[Azure] subgraph A[fa:fa-bucket Azure Storage] end subgraph C[fa:fa-network-wired Virtual Network] subgraph CC[Subnet] CCC[Azure Firewall] end end end F[Internet] style D fill:#FFFFFF style A fill:#F0FFFF style C fill:#F0FFFF style CC fill:#FFFFFF style CCC fill:#F0FFFF Private Links \u00b6 provide private connectivity to Azure PaaS services. accessible on connected virtual networks and globally peered networks. DNS Integration supported Private Endpoint \u00b6 a managed network interface used to provide connectivity to a private link Service Endpoints Private Endpoints Provides connectivity from a subnet to an entire Azure service cannot be used by on-premises networks uses routes to direct traffic Provides connectivity to single instance of an entire Azure service supports connected routes ( transitive routing ) integrates with DNS data transfer fees(inbound/outbound) Azure Bastion \u00b6 provides a managed secure remote management portal supports RDP and SSH supports Network Security Groups (NSGs) Virtual Network Peering \u00b6 Virtual Network Peerings provide connectivity between virtual networks within regions, across regions and subscriptions Peering connections must be created in both directions Non-transitive CIDR must not overlap Default Routes - The default route list has grown and is Azure managed. Azure is using the \"more specific\" approach - if there is no more specific drop all traffic. Address Prefix Next Hop Description Virtual network address range Virtual Network routes traffic within the virtual network 0.0.0.0/0 Internet Default route to the internet 10.0.0.0/8 172.16.0.0/12 192.168.0.0/16 None Drops private Ip address ranges that are not part of the virtual network 100.64.0.0/10 None Drops shared address space traffic Custom Routes are user-defined or learned via BGP Route Tables \u00b6 There are two types of routes: - System routes are dynamically created. - Dynamically Created - These routes cannot be deleted or modify, but some can be overwritten but custom or BGP learned routes such as 0.0.0.0/0 - Default routes - subnet routes, vnet prefix routes, public internet, reserved addresses blackhole (Next Hop - none ) - Services Routes - vnet peering, virtual network gateway, service endpoints User-defined routes are self managed custom routes that can be used to overwrite some of the system routes. Gateways \u00b6 Virtual Network Gateway provides either VPN or ExpressRoute connectivity to Azure. requires a special gateway subnet with a minimun of a /27 CIDR block Similar to an AWS VGW - its VPC specific VPN Gateway provides external connectivity - on-premises networks, cloud providers, or remote devices supports site to site, multiple sites, and point to site connections route-based VPN and policy-based connections ExpressRoute direct, private connection between on-premises network and Azure connectivity to Azure VNets using private peering or Azure services using microsoft services connection types: IPVPN - integrates with MPLS (VPLS) Point-to-Point Ethernet (Ethernet Private Links - EPLs) CloudExchange to connect from co-locations VPN Gateway Express Route - connects on-premises networks and other clouds - connects only to virtual networks - connects via the internet - connects on-premises networks - connects to virtual networks and microsoft services - connects via dedicated, private connections Virtual WAN \u00b6 Virtual WANs are used to manage communication between multiple virtual networks, on-premises networks, and remote sites. It also improves performance by using azure backbone. While the concept of Virtual WAN is global, the actual Virtual WAN resource is Resource Manager-based and deployed regionally. If the virtual WAN region itself were to have an issue, all hubs in that virtual WAN will continue to function as is, but the user won't be able to create new hubs until the virtual WAN region is available. Virtual WAN is assigned a region. However, Virtual WAN resources such as Hubs and Gateways can be in any region. managed hub-and-spoke provides connecitivity between virtual networks, on-premises networks and remote workers one hub per region regional hubs are dynamically connected to each other similar to an AWS Transit Gateway Load balancing \u00b6 Load Balancer Application Gateway - Layer 4 / TCP/IP Applications - load balancing based on source/destination - low latency - Layer 7 - HTTP/HTTPs based application - load balancing based on source/destination/hostname/path - Web Application Firewall is supported IP Addressing \u00b6 Static Public IP Security Secure by default model and be closed to inbound traffic when used as a frontend. Allow traffic with network security group (NSG) is required (for example, on the NIC of a virtual machine with a Standard SKU Public IP attached). Dynamic IP Security Open by default. Network security groups are recommended but optional for restricting inbound or outbound traffic. Subnet Naming Restrictions \u00b6 Dedicated Subnet Name: 'RouteServerSubnet' to associate a virtual network with a Router Server, it must contain a subnet with the name RouteServerSubnet with prefix of at least /27 . Force Tunneling \u00b6 Enabling forced tunneling through a VPN connection PowerShell is the only method available that can complete the process of setting a default site for tunneling.PowerShell command: Set-AzVirtualNetworkGatewayDefaultSite -GatewayDefaultSite $LocalGateway -VirtualNetworkGateway $VirtualGateway","title":"Azure Networking"},{"location":"cloud/azure/networking/#virtual-networks","text":"","title":"Virtual networks"},{"location":"cloud/azure/networking/#features","text":"private and secure infrastructure VM residing inside a virtual network are able to privately communicate with each other access other azure resources Network Segmentation Virtual Networks are isolated from each other Per Subscription / Region VNETs are region and subscription specific.","title":"Features"},{"location":"cloud/azure/networking/#components","text":"Subnets Subnets devide a virtual network into smaller segments. These segments can have unique security policies from each other. Some services require dedicated subnets such as network gateways, Bastion, and SQL Managed Instances Network Interfaces Network interfaces are use to connect virtuals machines to virtual networks. a virtual machine can have network interfaces in different subnets Network Security Groups (NSGs) Security groups filter inbound/outbound source/destination IP address/Port traffic based on a priority list of rules. they can be associated to either subnets or network interfaces . NSGs are region specific ( just like VMs ) Application Security Groups (ASGs) ASGs are used to group network interfaces based on purpose/usage. an ASG is similar to a tag on the network interface. a Network Security Group Rule can it destination set to an ASG. graph TD subgraph D[Virtual Private Network 10.0.0.0/26] subgraph E[Subnet 10.0.1.0/24] 1[Virtual Machine 1] 2[Application Security Group: ASG1] --> 1 end subgraph F[Subnet 10.0.2.0/24] 3[Virtual Machine 3] 4[Application Security Group: ASG1] --> 3 end 5[Network Security Group: NSG1<br>Direction: Inbound<br>Source: Internet<br>Destination: ASG1<br>Service: HTTPS<br>Action: Allow] end style 4 fill:#F0FFFF style 2 fill:#F0FFFF style D fill:#FFFFFF style 5 fill:#F0FFFF style E fill:#F0F8FF style F fill:#F0F8FF","title":"Components"},{"location":"cloud/azure/networking/#azure-firewalls","text":"","title":"Azure Firewalls"},{"location":"cloud/azure/networking/#features_1","text":"Azure Firewall as a Service is used to provide monitor and protection based on rules between virtual network egress/ingress traffic provides defense in depth Azure firewall is deployed between VNET 1 and VNET2 and the Internet. incomming traffic from the internet will be inspected by the Azure firewall before allowing or denying access to VNET1/VNET2 graph TD subgraph D[Azure] subgraph A[Virtual Network1] subgraph AA[Subnet] AAA[VM] end end subgraph B[Virtual Network1] subgraph BB[Subnet] BBB[VM] end end subgraph C[Virtual Network1] subgraph CC[Subnet] CCC[Azure Firewall] end end end F[Internet] style D fill:#FFFFFF style A fill:#F0FFFF style B fill:#F0FFFF style C fill:#F0FFFF style AA fill:#FFFFFF style BB fill:#FFFFFF style CC fill:#FFFFFF style AAA fill:#F0FFFF style BBB fill:#F0FFFF style CCC fill:#F0FFFF","title":"Features"},{"location":"cloud/azure/networking/#endpoint-services","text":"A service endpoints provide private connectivity between Virtual Networks and a range of Azure services, including Azure Storage, SQL Databases, and Key Vault. communication travels across Azure backbone graph TD subgraph D[Azure] subgraph A[fa:fa-bucket Azure Storage] end subgraph C[fa:fa-network-wired Virtual Network] subgraph CC[Subnet] CCC[Azure Firewall] end end end F[Internet] style D fill:#FFFFFF style A fill:#F0FFFF style C fill:#F0FFFF style CC fill:#FFFFFF style CCC fill:#F0FFFF","title":"Endpoint Services"},{"location":"cloud/azure/networking/#private-links","text":"provide private connectivity to Azure PaaS services. accessible on connected virtual networks and globally peered networks. DNS Integration supported","title":"Private Links"},{"location":"cloud/azure/networking/#private-endpoint","text":"a managed network interface used to provide connectivity to a private link Service Endpoints Private Endpoints Provides connectivity from a subnet to an entire Azure service cannot be used by on-premises networks uses routes to direct traffic Provides connectivity to single instance of an entire Azure service supports connected routes ( transitive routing ) integrates with DNS data transfer fees(inbound/outbound)","title":"Private Endpoint"},{"location":"cloud/azure/networking/#azure-bastion","text":"provides a managed secure remote management portal supports RDP and SSH supports Network Security Groups (NSGs)","title":"Azure Bastion"},{"location":"cloud/azure/networking/#virtual-network-peering","text":"Virtual Network Peerings provide connectivity between virtual networks within regions, across regions and subscriptions Peering connections must be created in both directions Non-transitive CIDR must not overlap Default Routes - The default route list has grown and is Azure managed. Azure is using the \"more specific\" approach - if there is no more specific drop all traffic. Address Prefix Next Hop Description Virtual network address range Virtual Network routes traffic within the virtual network 0.0.0.0/0 Internet Default route to the internet 10.0.0.0/8 172.16.0.0/12 192.168.0.0/16 None Drops private Ip address ranges that are not part of the virtual network 100.64.0.0/10 None Drops shared address space traffic Custom Routes are user-defined or learned via BGP","title":"Virtual Network Peering"},{"location":"cloud/azure/networking/#route-tables","text":"There are two types of routes: - System routes are dynamically created. - Dynamically Created - These routes cannot be deleted or modify, but some can be overwritten but custom or BGP learned routes such as 0.0.0.0/0 - Default routes - subnet routes, vnet prefix routes, public internet, reserved addresses blackhole (Next Hop - none ) - Services Routes - vnet peering, virtual network gateway, service endpoints User-defined routes are self managed custom routes that can be used to overwrite some of the system routes.","title":"Route Tables"},{"location":"cloud/azure/networking/#gateways","text":"Virtual Network Gateway provides either VPN or ExpressRoute connectivity to Azure. requires a special gateway subnet with a minimun of a /27 CIDR block Similar to an AWS VGW - its VPC specific VPN Gateway provides external connectivity - on-premises networks, cloud providers, or remote devices supports site to site, multiple sites, and point to site connections route-based VPN and policy-based connections ExpressRoute direct, private connection between on-premises network and Azure connectivity to Azure VNets using private peering or Azure services using microsoft services connection types: IPVPN - integrates with MPLS (VPLS) Point-to-Point Ethernet (Ethernet Private Links - EPLs) CloudExchange to connect from co-locations VPN Gateway Express Route - connects on-premises networks and other clouds - connects only to virtual networks - connects via the internet - connects on-premises networks - connects to virtual networks and microsoft services - connects via dedicated, private connections","title":"Gateways"},{"location":"cloud/azure/networking/#virtual-wan","text":"Virtual WANs are used to manage communication between multiple virtual networks, on-premises networks, and remote sites. It also improves performance by using azure backbone. While the concept of Virtual WAN is global, the actual Virtual WAN resource is Resource Manager-based and deployed regionally. If the virtual WAN region itself were to have an issue, all hubs in that virtual WAN will continue to function as is, but the user won't be able to create new hubs until the virtual WAN region is available. Virtual WAN is assigned a region. However, Virtual WAN resources such as Hubs and Gateways can be in any region. managed hub-and-spoke provides connecitivity between virtual networks, on-premises networks and remote workers one hub per region regional hubs are dynamically connected to each other similar to an AWS Transit Gateway","title":"Virtual WAN"},{"location":"cloud/azure/networking/#load-balancing","text":"Load Balancer Application Gateway - Layer 4 / TCP/IP Applications - load balancing based on source/destination - low latency - Layer 7 - HTTP/HTTPs based application - load balancing based on source/destination/hostname/path - Web Application Firewall is supported","title":"Load balancing"},{"location":"cloud/azure/networking/#ip-addressing","text":"Static Public IP Security Secure by default model and be closed to inbound traffic when used as a frontend. Allow traffic with network security group (NSG) is required (for example, on the NIC of a virtual machine with a Standard SKU Public IP attached). Dynamic IP Security Open by default. Network security groups are recommended but optional for restricting inbound or outbound traffic.","title":"IP Addressing"},{"location":"cloud/azure/networking/#subnet-naming-restrictions","text":"Dedicated Subnet Name: 'RouteServerSubnet' to associate a virtual network with a Router Server, it must contain a subnet with the name RouteServerSubnet with prefix of at least /27 .","title":"Subnet Naming Restrictions"},{"location":"cloud/azure/networking/#force-tunneling","text":"Enabling forced tunneling through a VPN connection PowerShell is the only method available that can complete the process of setting a default site for tunneling.PowerShell command: Set-AzVirtualNetworkGatewayDefaultSite -GatewayDefaultSite $LocalGateway -VirtualNetworkGateway $VirtualGateway","title":"Force Tunneling"},{"location":"cloud/azure/tshoo/","text":"Creating Virtual Hub \u00b6 Virtual hub is not enabled to allow Router ASN modification. However, it's not grayed out so you can change with without having the option enabled. See error below when changing it. { \"code\" : \"DeploymentFailed\" , \"message\" : \"At least one resource deployment operation failed. Please list deployment operations for details. Please see https://aka.ms/DeployOperations for usage details.\" , \"details\" : [ { \"code\" : \"CustomAsnNotEnabledForVirtualHub\" , \"message\" : \"Virtual hub is not enabled to allow Router ASN modification. Please set ASN to 65515 or contact Support for enabling virtual hub to allow modification.\" } ] } VPN (Site to site) Gateway: This gateway is being updated. It may take upto 30 minutes for the update . vhub routing \u00b6 Before vhub Firewall \u00b6 VPN Gateway Default Route Table - Effective Routes Route table","title":"Azure Troubleshooting Notes"},{"location":"cloud/azure/tshoo/#creating-virtual-hub","text":"Virtual hub is not enabled to allow Router ASN modification. However, it's not grayed out so you can change with without having the option enabled. See error below when changing it. { \"code\" : \"DeploymentFailed\" , \"message\" : \"At least one resource deployment operation failed. Please list deployment operations for details. Please see https://aka.ms/DeployOperations for usage details.\" , \"details\" : [ { \"code\" : \"CustomAsnNotEnabledForVirtualHub\" , \"message\" : \"Virtual hub is not enabled to allow Router ASN modification. Please set ASN to 65515 or contact Support for enabling virtual hub to allow modification.\" } ] } VPN (Site to site) Gateway: This gateway is being updated. It may take upto 30 minutes for the update .","title":"Creating Virtual Hub"},{"location":"cloud/azure/tshoo/#vhub-routing","text":"","title":"vhub routing"},{"location":"cloud/azure/tshoo/#before-vhub-firewall","text":"VPN Gateway Default Route Table - Effective Routes Route table","title":"Before vhub Firewall"},{"location":"cloud/gcp/gcloud/","text":"Review current configuration \u00b6 Current configuration $ gcloud config configurations list NAME IS_ACTIVE ACCOUNT PROJECT COMPUTE_DEFAULT_ZONE COMPUTE_DEFAULT_REGION default True kg48037@mykronos.com shd-cus-host01-7f07 update account $ gcloud config set account kg48037@mykronos.com Updated property [ core/account ] . update project $ gcloud config set project rdy-dev-cld-usa-dev01 Updated property [ core/project ] . New configuration shell $ gcloud config configurations list NAME IS_ACTIVE ACCOUNT PROJECT COMPUTE_DEFAULT_ZONE COMPUTE_DEFAULT_REGION default True kg48037@mykronos.com rdy-dev-cld-usa-dev01","title":"Gcloud Command Overview"},{"location":"cloud/gcp/gcloud/#review-current-configuration","text":"Current configuration $ gcloud config configurations list NAME IS_ACTIVE ACCOUNT PROJECT COMPUTE_DEFAULT_ZONE COMPUTE_DEFAULT_REGION default True kg48037@mykronos.com shd-cus-host01-7f07 update account $ gcloud config set account kg48037@mykronos.com Updated property [ core/account ] . update project $ gcloud config set project rdy-dev-cld-usa-dev01 Updated property [ core/project ] . New configuration shell $ gcloud config configurations list NAME IS_ACTIVE ACCOUNT PROJECT COMPUTE_DEFAULT_ZONE COMPUTE_DEFAULT_REGION default True kg48037@mykronos.com rdy-dev-cld-usa-dev01","title":"Review current configuration"},{"location":"cloud/gcp/intro/","text":"Comming Soon \u00b6","title":"Introduction"},{"location":"cloud/gcp/intro/#comming-soon","text":"","title":"Comming Soon"},{"location":"contact-us/intro/","text":"Gtz4All \u00b6 Gtz4all is designed to provide step by step procedures in multiple areas of an enterprise environment with a focus on automation and simplicity.","title":"About Me"},{"location":"contact-us/intro/#gtz4all","text":"Gtz4all is designed to provide step by step procedures in multiple areas of an enterprise environment with a focus on automation and simplicity.","title":"Gtz4All"},{"location":"devops/intro/","text":"Scripting and Automation. \u00b6 Terraform Documentation terraform is currently the leading devops provider. Ansible Documentation ansible is currently the leading devops provider. Docker Documentation docker is currently the leading devops provider. Kubernetes Documentation kubernetes is currently the leading devops provider. Web Services Documentation python is currently the leading devops provider.","title":"Introduction"},{"location":"devops/intro/#scripting-and-automation","text":"Terraform Documentation terraform is currently the leading devops provider. Ansible Documentation ansible is currently the leading devops provider. Docker Documentation docker is currently the leading devops provider. Kubernetes Documentation kubernetes is currently the leading devops provider. Web Services Documentation python is currently the leading devops provider.","title":"Scripting and Automation."},{"location":"devops/ansible/intro/","text":"Ansible Documentation ansible is currently the leading devops provider.","title":"Introduction"},{"location":"devops/databases/intro/","text":"Databases \u00b6","title":"Introduction"},{"location":"devops/databases/intro/#databases","text":"","title":"Databases"},{"location":"devops/databases/mongodb/","text":"Over the past years, I've been searching for an easy to use database that can interact with other tools like ansible and python. Since I didnt have one at the moment, I started using CVS files with the plan to migrate to a database in the future. MongoDB is an open-source, document-oriented, NoSQL database program which uses JSON-like document schemas, making it easy for other tools to work with it. MongoDB provides tools that can import/export CSV files. MongoDB being an non-rational database can be used to store different type of data such as key-value pairs, JSON data, CLI configuration and even images https://www.mongodb.com Document model Database Document-Oriented NoSQL DB stores and retrieves data as a Key-value pair, but the value part is stored as a document in JSON or XML formats. Some MongoDB Advatanges Store unstructured, semi-structured, or structured data. Uses easy to update,flexible schemas and fields. Open Source Mongo DB Structure Type description usage Databases hold one or more collections of documents no create option instead you just try to use it and it gets created. Collection stores documents, analogous to rational database tables same as database, applies to MongoDB CLI and Python. Mongo-Express Mongo-Express is an open-source Web-based MongoDB admin interface written with Node.js, Express and Bootstrap3. It provides an easy to read, visual represtation of our databases https://github.com/mongo-express/mongo-express MongoDB Test Drive \u00b6 Using docker to test drive both mongodb and mongo-express Mongo Dockerfile This dockerfile creates both a mongodb and mongo-express instance version : '3.1' services : mongodb : container_name : mongodb image : mongo restart : always environment : ## MongoDB Credentials MONGO_INITDB_ROOT_USERNAME : ${DBUSER} MONGO_INITDB_ROOT_PASSWORD : ${DBPASS} mongo-express : container_name : mongogui image : mongo-express restart : always ports : - 8081:8081 environment : ## MongoDB Container Name ME_CONFIG_MONGODB_SERVER : mongodb ## MongoDB Credentials ME_CONFIG_MONGODB_ADMINUSERNAME : ${DBUSER} ME_CONFIG_MONGODB_ADMINPASSWORD : ${DBPASS} ## MongoDB Web UI Credentials ME_CONFIG_BASICAUTH_USERNAME : ${DBUSER} ME_CONFIG_BASICAUTH_PASSWORD : ${DBPASS} Run Containers DBUSER = admin DBPASS = admin docker-compose up -d MongoDB CLI \u00b6 Install sudo apt update && sudo apt install mongodb-clients -y Export Credentials export DBUSER = admin DBPASS = admin echo $DBUSER echo $DBPASS Mongo Container IP export MONGODB_IP = $( docker inspect mongodb --format '{range.NetworkSettings.Networks }{.IPAddress}{ end }' ) echo $MONGODB_IP Connect to MongoDB mongo mongodb:// $MONGODB_IP :27017/ -u $DBUSER -p $DBPASS --authenticationDatabase \"admin\" MongoDB CLI connection output :~$ mongo mongodb:// $MONGODB_IP :27017/ -u $DBUSER -p $DBPASS --authenticationDatabase \"admin\" MongoDB shell version v3.6.3 connecting to: mongodb://172.18.0.2:27017/ MongoDB server version: 4 .4.6 WARNING: shell and server versions do not match Server has startup warnings: { \"t\" : { \" $date \" : \"2021-06-07T18:10:19.421+00:00\" } , \"s\" : \"I\" , \"c\" : \"STORAGE\" , \"id\" :22297, \"ctx\" : \"initandlisten\" , \"msg\" : \"Using the XFS filesystem is strongly recommended with the WiredTiger storage engine. See http://dochub.mongodb.org/core/prodnotes-filesystem\" , \"tags\" : [ \"startupWarnings\" ]} { \"t\" : { \" $date \" : \"2021-06-07T18:10:20.787+00:00\" } , \"s\" : \"W\" , \"c\" : \"CONTROL\" , \"id\" :22167, \"ctx\" : \"initandlisten\" , \"msg\" : \"You are running on a NUMA machine. We suggest launching mongod like this to avoid performance problems: numactl --interleave=all mongod [other options]\" , \"tags\" : [ \"startupWarnings\" ]} > List all databases > db.adminCommand ( { listDatabases: 1 } ) { \"databases\" : [ { \"name\" : \"admin\" , \"sizeOnDisk\" : 102400 , \"empty\" : false } , { \"name\" : \"config\" , \"sizeOnDisk\" : 12288 , \"empty\" : false } , { \"name\" : \"local\" , \"sizeOnDisk\" : 73728 , \"empty\" : false } ] , \"totalSize\" : 188416 , \"ok\" : 1 } > Create new database use DataCenterDB create a collection and insert items ## use - creates database use DataCenterDB ## db.[collection-name].insert - creates collection if it does not already exist db.DataCenter1.insert ([ { \"name\" : \"COREROUTER1\" , \"tag\" : \"CORE\" , \"asn\" : \"64521\" , \"lo0\" : \"10.168.1.1\" } , { \"name\" : \"COREROUTER1\" , \"tag\" : \"CORE\" , \"asn\" : \"64522\" , \"lo0\" : \"10.168.1.2\" } , { \"name\" : \"DISTROUTER1\" , \"tag\" : \"DIST\" , \"asn\" : \"64523\" , \"lo0\" : \"10.168.1.3\" } , { \"name\" : \"DISTROUTER1\" , \"tag\" : \"DIST\" , \"asn\" : \"64521\" , \"lo0\" : \"10.168.1.2\" } ]) Insert output BulkWriteResult ({ \"writeErrors\" : [ ] , \"writeConcernErrors\" : [ ] , \"nInserted\" : 4 , \"nUpserted\" : 0 , \"nMatched\" : 0 , \"nModified\" : 0 , \"nRemoved\" : 0 , \"upserted\" : [ ] }) Query database find() : query data from MongoDB collection. pretty() : display the results in a JSON formatted way. ## db.[collection-name].find() use DataCenterDB > db.DataCenter1.find () { \"_id\" : ObjectId ( \"60be987af6652ebfdef9b958\" ) , \"name\" : \"COREROUTER1\" , \"tag\" : \"CORE\" , \"asn\" : \"64521\" , \"lo0\" : \"10.168.1.1\" } { \"_id\" : ObjectId ( \"60be987af6652ebfdef9b959\" ) , \"name\" : \"COREROUTER1\" , \"tag\" : \"CORE\" , \"asn\" : \"64522\" , \"lo0\" : \"10.168.1.2\" } { \"_id\" : ObjectId ( \"60be987af6652ebfdef9b95a\" ) , \"name\" : \"DISTROUTER1\" , \"tag\" : \"DIST\" , \"asn\" : \"64523\" , \"lo0\" : \"10.168.1.3\" } { \"_id\" : ObjectId ( \"60be987af6652ebfdef9b95b\" ) , \"name\" : \"DISTROUTER1\" , \"tag\" : \"DIST\" , \"asn\" : \"64521\" , \"lo0\" : \"10.168.1.2\" } > db.DataCenter1.find () .pretty () { \"_id\" : ObjectId ( \"60be987af6652ebfdef9b958\" ) , \"name\" : \"COREROUTER1\" , \"tag\" : \"CORE\" , \"asn\" : \"64521\" , \"lo0\" : \"10.168.1.1\" } { \"_id\" : ObjectId ( \"60be987af6652ebfdef9b959\" ) , \"name\" : \"COREROUTER1\" , \"tag\" : \"CORE\" , \"asn\" : \"64522\" , \"lo0\" : \"10.168.1.2\" } { \"_id\" : ObjectId ( \"60be987af6652ebfdef9b95a\" ) , \"name\" : \"DISTROUTER1\" , \"tag\" : \"DIST\" , \"asn\" : \"64523\" , \"lo0\" : \"10.168.1.3\" } { \"_id\" : ObjectId ( \"60be987af6652ebfdef9b95b\" ) , \"name\" : \"DISTROUTER1\" , \"tag\" : \"DIST\" , \"asn\" : \"64521\" , \"lo0\" : \"10.168.1.2\" } ## Find an specific item > db.DataCenter1.find ({ \"tag\" : \"DIST\" }) .pretty () { \"_id\" : ObjectId ( \"60be987af6652ebfdef9b95a\" ) , \"name\" : \"DISTROUTER1\" , \"tag\" : \"DIST\" , \"asn\" : \"64523\" , \"lo0\" : \"10.168.1.3\" } { \"_id\" : ObjectId ( \"60be987af6652ebfdef9b95b\" ) , \"name\" : \"DISTROUTER1\" , \"tag\" : \"DIST\" , \"asn\" : \"64521\" , \"lo0\" : \"10.168.1.2\" } > MongoDB Python 3 Interactive Mode \u00b6 Install pip3 install pymongo Interactive Mode import os import pymongo dbpass = os . environ . get ( 'DBPASS' ) dbuser = os . environ . get ( 'DBUSER' ) dbip = os . environ . get ( 'MONGODB_IP' ) dbclient = pymongo . MongoClient ( f \"mongodb:// { dbip } :27017/\" , username = dbuser , password = dbpass ) list database option 1 for db in dbclient . list_databases (): print ( db ) * list database option 2 print ( list ( dbclient . list_databases ())) create database \"DataCenterDB\" DCdb = dbclient [ \"DataCenterDB\" ] create \"DataCenter1\" collection inside \"DataCenterDB\" DC1 = DCdb [ \"DataCenter1\" ] ## Add items to Collection updatedb = DC1 . insert ([ { \"name\" : \"COREROUTER1\" , \"tag\" : \"CORE\" , \"asn\" : \"64511\" , \"lo0\" : \"10.168.1.1\" }, { \"name\" : \"COREROUTER1\" , \"tag\" : \"CORE\" , \"asn\" : \"64512\" , \"lo0\" : \"10.168.1.2\" }, { \"name\" : \"DISTROUTER1\" , \"tag\" : \"DIST\" , \"asn\" : \"64513\" , \"lo0\" : \"10.168.1.3\" }, { \"name\" : \"DISTROUTER1\" , \"tag\" : \"DIST\" , \"asn\" : \"64511\" , \"lo0\" : \"10.168.1.2\" } ]) create \"DataCenter1\" collection inside \"DataCenterDB\" DC2 = DCdb [ \"DataCenter2\" ] ## Add items to Collection updatedb = DC2 . insert ([ { \"name\" : \"COREROUTER2\" , \"tag\" : \"CORE\" , \"asn\" : \"64521\" , \"lo0\" : \"10.168.2.1\" }, { \"name\" : \"COREROUTER2\" , \"tag\" : \"CORE\" , \"asn\" : \"64522\" , \"lo0\" : \"10.168.2.2\" }, { \"name\" : \"DISTROUTER2\" , \"tag\" : \"DIST\" , \"asn\" : \"64523\" , \"lo0\" : \"10.168.2.3\" }, { \"name\" : \"DISTROUTER2\" , \"tag\" : \"DIST\" , \"asn\" : \"64521\" , \"lo0\" : \"10.168.2.2\" } ]) list collections print ( DCdb . list_collection_names ()) ## list output [ 'DataCenter2' , 'DataCenter1' ] query all items in collection print ( list ( DC1 . find ())) ## output [ { \"_id\" : \"ObjectId(\"\"60beaac074d25b62b989fda4\"\")\" , \"name\" : \"COREROUTER1\" , \"tag\" : \"CORE\" , \"asn\" : \"64511\" , \"lo0\" : \"10.168.1.1\" }, { \"_id\" : \"ObjectId(\"\"60beaac074d25b62b989fda5\"\")\" , \"name\" : \"COREROUTER1\" , \"tag\" : \"CORE\" , \"asn\" : \"64512\" , \"lo0\" : \"10.168.1.2\" }, { \"_id\" : \"ObjectId(\"\"60beaac074d25b62b989fda6\"\")\" , \"name\" : \"DISTROUTER1\" , \"tag\" : \"DIST\" , \"asn\" : \"64513\" , \"lo0\" : \"10.168.1.3\" }, { \"_id\" : \"ObjectId(\"\"60beaac074d25b62b989fda7\"\")\" , \"name\" : \"DISTROUTER1\" , \"tag\" : \"DIST\" , \"asn\" : \"64511\" , \"lo0\" : \"10.168.1.2\" } ] query an item in collection print ( list ( DC1 . find ({ \"tag\" : \"DIST\" }))) ## output [ { \"_id\" : \"ObjectId(\"\"60beaac074d25b62b989fda6\"\")\" , \"name\" : \"DISTROUTER1\" , \"tag\" : \"DIST\" , \"asn\" : \"64513\" , \"lo0\" : \"10.168.1.3\" }, { \"_id\" : \"ObjectId(\"\"60beaac074d25b62b989fda7\"\")\" , \"name\" : \"DISTROUTER1\" , \"tag\" : \"DIST\" , \"asn\" : \"64511\" , \"lo0\" : \"10.168.1.2\" } ] Conclusion This is the end of the mongoDB demo","title":"MongoDB"},{"location":"devops/databases/mongodb/#mongodb-test-drive","text":"Using docker to test drive both mongodb and mongo-express Mongo Dockerfile This dockerfile creates both a mongodb and mongo-express instance version : '3.1' services : mongodb : container_name : mongodb image : mongo restart : always environment : ## MongoDB Credentials MONGO_INITDB_ROOT_USERNAME : ${DBUSER} MONGO_INITDB_ROOT_PASSWORD : ${DBPASS} mongo-express : container_name : mongogui image : mongo-express restart : always ports : - 8081:8081 environment : ## MongoDB Container Name ME_CONFIG_MONGODB_SERVER : mongodb ## MongoDB Credentials ME_CONFIG_MONGODB_ADMINUSERNAME : ${DBUSER} ME_CONFIG_MONGODB_ADMINPASSWORD : ${DBPASS} ## MongoDB Web UI Credentials ME_CONFIG_BASICAUTH_USERNAME : ${DBUSER} ME_CONFIG_BASICAUTH_PASSWORD : ${DBPASS} Run Containers DBUSER = admin DBPASS = admin docker-compose up -d","title":"MongoDB Test Drive"},{"location":"devops/databases/mongodb/#mongodb-cli","text":"Install sudo apt update && sudo apt install mongodb-clients -y Export Credentials export DBUSER = admin DBPASS = admin echo $DBUSER echo $DBPASS Mongo Container IP export MONGODB_IP = $( docker inspect mongodb --format '{range.NetworkSettings.Networks }{.IPAddress}{ end }' ) echo $MONGODB_IP Connect to MongoDB mongo mongodb:// $MONGODB_IP :27017/ -u $DBUSER -p $DBPASS --authenticationDatabase \"admin\" MongoDB CLI connection output :~$ mongo mongodb:// $MONGODB_IP :27017/ -u $DBUSER -p $DBPASS --authenticationDatabase \"admin\" MongoDB shell version v3.6.3 connecting to: mongodb://172.18.0.2:27017/ MongoDB server version: 4 .4.6 WARNING: shell and server versions do not match Server has startup warnings: { \"t\" : { \" $date \" : \"2021-06-07T18:10:19.421+00:00\" } , \"s\" : \"I\" , \"c\" : \"STORAGE\" , \"id\" :22297, \"ctx\" : \"initandlisten\" , \"msg\" : \"Using the XFS filesystem is strongly recommended with the WiredTiger storage engine. See http://dochub.mongodb.org/core/prodnotes-filesystem\" , \"tags\" : [ \"startupWarnings\" ]} { \"t\" : { \" $date \" : \"2021-06-07T18:10:20.787+00:00\" } , \"s\" : \"W\" , \"c\" : \"CONTROL\" , \"id\" :22167, \"ctx\" : \"initandlisten\" , \"msg\" : \"You are running on a NUMA machine. We suggest launching mongod like this to avoid performance problems: numactl --interleave=all mongod [other options]\" , \"tags\" : [ \"startupWarnings\" ]} > List all databases > db.adminCommand ( { listDatabases: 1 } ) { \"databases\" : [ { \"name\" : \"admin\" , \"sizeOnDisk\" : 102400 , \"empty\" : false } , { \"name\" : \"config\" , \"sizeOnDisk\" : 12288 , \"empty\" : false } , { \"name\" : \"local\" , \"sizeOnDisk\" : 73728 , \"empty\" : false } ] , \"totalSize\" : 188416 , \"ok\" : 1 } > Create new database use DataCenterDB create a collection and insert items ## use - creates database use DataCenterDB ## db.[collection-name].insert - creates collection if it does not already exist db.DataCenter1.insert ([ { \"name\" : \"COREROUTER1\" , \"tag\" : \"CORE\" , \"asn\" : \"64521\" , \"lo0\" : \"10.168.1.1\" } , { \"name\" : \"COREROUTER1\" , \"tag\" : \"CORE\" , \"asn\" : \"64522\" , \"lo0\" : \"10.168.1.2\" } , { \"name\" : \"DISTROUTER1\" , \"tag\" : \"DIST\" , \"asn\" : \"64523\" , \"lo0\" : \"10.168.1.3\" } , { \"name\" : \"DISTROUTER1\" , \"tag\" : \"DIST\" , \"asn\" : \"64521\" , \"lo0\" : \"10.168.1.2\" } ]) Insert output BulkWriteResult ({ \"writeErrors\" : [ ] , \"writeConcernErrors\" : [ ] , \"nInserted\" : 4 , \"nUpserted\" : 0 , \"nMatched\" : 0 , \"nModified\" : 0 , \"nRemoved\" : 0 , \"upserted\" : [ ] }) Query database find() : query data from MongoDB collection. pretty() : display the results in a JSON formatted way. ## db.[collection-name].find() use DataCenterDB > db.DataCenter1.find () { \"_id\" : ObjectId ( \"60be987af6652ebfdef9b958\" ) , \"name\" : \"COREROUTER1\" , \"tag\" : \"CORE\" , \"asn\" : \"64521\" , \"lo0\" : \"10.168.1.1\" } { \"_id\" : ObjectId ( \"60be987af6652ebfdef9b959\" ) , \"name\" : \"COREROUTER1\" , \"tag\" : \"CORE\" , \"asn\" : \"64522\" , \"lo0\" : \"10.168.1.2\" } { \"_id\" : ObjectId ( \"60be987af6652ebfdef9b95a\" ) , \"name\" : \"DISTROUTER1\" , \"tag\" : \"DIST\" , \"asn\" : \"64523\" , \"lo0\" : \"10.168.1.3\" } { \"_id\" : ObjectId ( \"60be987af6652ebfdef9b95b\" ) , \"name\" : \"DISTROUTER1\" , \"tag\" : \"DIST\" , \"asn\" : \"64521\" , \"lo0\" : \"10.168.1.2\" } > db.DataCenter1.find () .pretty () { \"_id\" : ObjectId ( \"60be987af6652ebfdef9b958\" ) , \"name\" : \"COREROUTER1\" , \"tag\" : \"CORE\" , \"asn\" : \"64521\" , \"lo0\" : \"10.168.1.1\" } { \"_id\" : ObjectId ( \"60be987af6652ebfdef9b959\" ) , \"name\" : \"COREROUTER1\" , \"tag\" : \"CORE\" , \"asn\" : \"64522\" , \"lo0\" : \"10.168.1.2\" } { \"_id\" : ObjectId ( \"60be987af6652ebfdef9b95a\" ) , \"name\" : \"DISTROUTER1\" , \"tag\" : \"DIST\" , \"asn\" : \"64523\" , \"lo0\" : \"10.168.1.3\" } { \"_id\" : ObjectId ( \"60be987af6652ebfdef9b95b\" ) , \"name\" : \"DISTROUTER1\" , \"tag\" : \"DIST\" , \"asn\" : \"64521\" , \"lo0\" : \"10.168.1.2\" } ## Find an specific item > db.DataCenter1.find ({ \"tag\" : \"DIST\" }) .pretty () { \"_id\" : ObjectId ( \"60be987af6652ebfdef9b95a\" ) , \"name\" : \"DISTROUTER1\" , \"tag\" : \"DIST\" , \"asn\" : \"64523\" , \"lo0\" : \"10.168.1.3\" } { \"_id\" : ObjectId ( \"60be987af6652ebfdef9b95b\" ) , \"name\" : \"DISTROUTER1\" , \"tag\" : \"DIST\" , \"asn\" : \"64521\" , \"lo0\" : \"10.168.1.2\" } >","title":"MongoDB CLI"},{"location":"devops/databases/mongodb/#mongodb-python-3-interactive-mode","text":"Install pip3 install pymongo Interactive Mode import os import pymongo dbpass = os . environ . get ( 'DBPASS' ) dbuser = os . environ . get ( 'DBUSER' ) dbip = os . environ . get ( 'MONGODB_IP' ) dbclient = pymongo . MongoClient ( f \"mongodb:// { dbip } :27017/\" , username = dbuser , password = dbpass ) list database option 1 for db in dbclient . list_databases (): print ( db ) * list database option 2 print ( list ( dbclient . list_databases ())) create database \"DataCenterDB\" DCdb = dbclient [ \"DataCenterDB\" ] create \"DataCenter1\" collection inside \"DataCenterDB\" DC1 = DCdb [ \"DataCenter1\" ] ## Add items to Collection updatedb = DC1 . insert ([ { \"name\" : \"COREROUTER1\" , \"tag\" : \"CORE\" , \"asn\" : \"64511\" , \"lo0\" : \"10.168.1.1\" }, { \"name\" : \"COREROUTER1\" , \"tag\" : \"CORE\" , \"asn\" : \"64512\" , \"lo0\" : \"10.168.1.2\" }, { \"name\" : \"DISTROUTER1\" , \"tag\" : \"DIST\" , \"asn\" : \"64513\" , \"lo0\" : \"10.168.1.3\" }, { \"name\" : \"DISTROUTER1\" , \"tag\" : \"DIST\" , \"asn\" : \"64511\" , \"lo0\" : \"10.168.1.2\" } ]) create \"DataCenter1\" collection inside \"DataCenterDB\" DC2 = DCdb [ \"DataCenter2\" ] ## Add items to Collection updatedb = DC2 . insert ([ { \"name\" : \"COREROUTER2\" , \"tag\" : \"CORE\" , \"asn\" : \"64521\" , \"lo0\" : \"10.168.2.1\" }, { \"name\" : \"COREROUTER2\" , \"tag\" : \"CORE\" , \"asn\" : \"64522\" , \"lo0\" : \"10.168.2.2\" }, { \"name\" : \"DISTROUTER2\" , \"tag\" : \"DIST\" , \"asn\" : \"64523\" , \"lo0\" : \"10.168.2.3\" }, { \"name\" : \"DISTROUTER2\" , \"tag\" : \"DIST\" , \"asn\" : \"64521\" , \"lo0\" : \"10.168.2.2\" } ]) list collections print ( DCdb . list_collection_names ()) ## list output [ 'DataCenter2' , 'DataCenter1' ] query all items in collection print ( list ( DC1 . find ())) ## output [ { \"_id\" : \"ObjectId(\"\"60beaac074d25b62b989fda4\"\")\" , \"name\" : \"COREROUTER1\" , \"tag\" : \"CORE\" , \"asn\" : \"64511\" , \"lo0\" : \"10.168.1.1\" }, { \"_id\" : \"ObjectId(\"\"60beaac074d25b62b989fda5\"\")\" , \"name\" : \"COREROUTER1\" , \"tag\" : \"CORE\" , \"asn\" : \"64512\" , \"lo0\" : \"10.168.1.2\" }, { \"_id\" : \"ObjectId(\"\"60beaac074d25b62b989fda6\"\")\" , \"name\" : \"DISTROUTER1\" , \"tag\" : \"DIST\" , \"asn\" : \"64513\" , \"lo0\" : \"10.168.1.3\" }, { \"_id\" : \"ObjectId(\"\"60beaac074d25b62b989fda7\"\")\" , \"name\" : \"DISTROUTER1\" , \"tag\" : \"DIST\" , \"asn\" : \"64511\" , \"lo0\" : \"10.168.1.2\" } ] query an item in collection print ( list ( DC1 . find ({ \"tag\" : \"DIST\" }))) ## output [ { \"_id\" : \"ObjectId(\"\"60beaac074d25b62b989fda6\"\")\" , \"name\" : \"DISTROUTER1\" , \"tag\" : \"DIST\" , \"asn\" : \"64513\" , \"lo0\" : \"10.168.1.3\" }, { \"_id\" : \"ObjectId(\"\"60beaac074d25b62b989fda7\"\")\" , \"name\" : \"DISTROUTER1\" , \"tag\" : \"DIST\" , \"asn\" : \"64511\" , \"lo0\" : \"10.168.1.2\" } ] Conclusion This is the end of the mongoDB demo","title":"MongoDB Python 3 Interactive Mode"},{"location":"devops/docker/hugo.docker.container/","text":"Building a Docker Image \u00b6 Hugo Multi-Stage Dockerfile \u00b6 Multi-stage builds can use multiple FROM statements in the Dockerfile. Each FROM instruction can use its own base image. Any artifacts from each FROM can be copied to another stage. The last FROM statement is used as final image. Everything else is destroyed. ############################################ # Multi Stage- Hugo Static Page ############################################ ## Stage 1 - name 'build' FROM nginx:alpine as build ## Setting Arguments ARG HUGO_VERSION = \"0.72.0\" ARG STATIC_PAGE = \"\" ## Running commands inside container during build RUN apk add --update wget RUN wget --quiet \"https://github.com/gohugoio/hugo/releases/download/v ${ HUGO_VERSION } /hugo_ ${ HUGO_VERSION } _Linux-64bit.tar.gz\" && \\ tar xzf hugo_ ${ HUGO_VERSION } _Linux-64bit.tar.gz && \\ rm -r hugo_ ${ HUGO_VERSION } _Linux-64bit.tar.gz && \\ mv hugo /usr/bin ## Copying local Hugo Static Site Directory to Container directory COPY ./ ${ STATIC_PAGE } /site WORKDIR /site ## Building Site RUN hugo ## Stage 2. FROM nginx:alpine ## copy /site/public folder created by the 'hugo' RUN command in Stage 1 to the nginx working directory. COPY --from = build /site/public /usr/share/nginx/html WORKDIR /usr/share/nginx/html * Changin config.toml baseURL value Local Testing : export BASE_URL =/ Gitlab Pages : export BASE_URL = https : // netgtz . gitlab . io / Since the BASE_URL variable includes \"/\" using \"|\" as delimiter instead shell command : sed - i \"s|BASE_URL|$BASE_URL|g\" config . toml sed -i \"s|BASE_URL| $BASE_URL |g\" netgtz.gitlab.io/config.toml * Building a Docker Image based on Dockerfile docker build --build-arg STATIC_PAGE = netgtz.gitlab.io -t hugo . * Running a container using the recently built image exposing container port 80 on local host port 80 docker run --rm -p 80 :80 -d --name hugoapp hugo Running a Docker Image \u00b6 Commonly used Docker commands docker run [OPTIONS] IMAGE [COMMAND] docker run [--rm -p 80:80 -dit --name hugoapp] hugo [/bin/ash] docker run --rm -p 80 :80 -dit --name hugoapp hugo /bin/ash ``` ###### OPTIONS Name | shorthand | Description -- | -- | -- --detach | -d | Run container in background and print container ID --interactive | -i | Keep STDIN open even if not attached --publish | -p | Publish local port to container port - -p 8080 :80 - 8080 = local_port, 80 = container_port --rm | --rm | Automatically remove the container when it exits --tty | -t | Allocate a pseudo-TTY --name | --name | Assign a name to the container * To attach to an already running container: ``` sh ## ubuntu - /bin/bash, Alpine - /bin/ash docker exec -it hugoapp /bin/ash Run the container in deattached (-d), interactive (-i) , terminal(-t) node \u00b6 this will allow the ducker to continue running /bin/bash docker run -it -d IMAGE_NAME bin/bash Create Multiple Directories ( -p) \u00b6 sh mkdir -p sbox/docker","title":"Building a Hugo Docker Containers"},{"location":"devops/docker/hugo.docker.container/#building-a-docker-image","text":"","title":"Building a Docker Image"},{"location":"devops/docker/hugo.docker.container/#hugo-multi-stage-dockerfile","text":"Multi-stage builds can use multiple FROM statements in the Dockerfile. Each FROM instruction can use its own base image. Any artifacts from each FROM can be copied to another stage. The last FROM statement is used as final image. Everything else is destroyed. ############################################ # Multi Stage- Hugo Static Page ############################################ ## Stage 1 - name 'build' FROM nginx:alpine as build ## Setting Arguments ARG HUGO_VERSION = \"0.72.0\" ARG STATIC_PAGE = \"\" ## Running commands inside container during build RUN apk add --update wget RUN wget --quiet \"https://github.com/gohugoio/hugo/releases/download/v ${ HUGO_VERSION } /hugo_ ${ HUGO_VERSION } _Linux-64bit.tar.gz\" && \\ tar xzf hugo_ ${ HUGO_VERSION } _Linux-64bit.tar.gz && \\ rm -r hugo_ ${ HUGO_VERSION } _Linux-64bit.tar.gz && \\ mv hugo /usr/bin ## Copying local Hugo Static Site Directory to Container directory COPY ./ ${ STATIC_PAGE } /site WORKDIR /site ## Building Site RUN hugo ## Stage 2. FROM nginx:alpine ## copy /site/public folder created by the 'hugo' RUN command in Stage 1 to the nginx working directory. COPY --from = build /site/public /usr/share/nginx/html WORKDIR /usr/share/nginx/html * Changin config.toml baseURL value Local Testing : export BASE_URL =/ Gitlab Pages : export BASE_URL = https : // netgtz . gitlab . io / Since the BASE_URL variable includes \"/\" using \"|\" as delimiter instead shell command : sed - i \"s|BASE_URL|$BASE_URL|g\" config . toml sed -i \"s|BASE_URL| $BASE_URL |g\" netgtz.gitlab.io/config.toml * Building a Docker Image based on Dockerfile docker build --build-arg STATIC_PAGE = netgtz.gitlab.io -t hugo . * Running a container using the recently built image exposing container port 80 on local host port 80 docker run --rm -p 80 :80 -d --name hugoapp hugo","title":"Hugo Multi-Stage Dockerfile"},{"location":"devops/docker/hugo.docker.container/#running-a-docker-image","text":"Commonly used Docker commands docker run [OPTIONS] IMAGE [COMMAND] docker run [--rm -p 80:80 -dit --name hugoapp] hugo [/bin/ash] docker run --rm -p 80 :80 -dit --name hugoapp hugo /bin/ash ``` ###### OPTIONS Name | shorthand | Description -- | -- | -- --detach | -d | Run container in background and print container ID --interactive | -i | Keep STDIN open even if not attached --publish | -p | Publish local port to container port - -p 8080 :80 - 8080 = local_port, 80 = container_port --rm | --rm | Automatically remove the container when it exits --tty | -t | Allocate a pseudo-TTY --name | --name | Assign a name to the container * To attach to an already running container: ``` sh ## ubuntu - /bin/bash, Alpine - /bin/ash docker exec -it hugoapp /bin/ash","title":"Running a Docker Image"},{"location":"devops/docker/hugo.docker.container/#run-the-container-in-deattached-d-interactive-i-terminal-t-node","text":"this will allow the ducker to continue running /bin/bash docker run -it -d IMAGE_NAME bin/bash","title":"Run the container in deattached (-d), interactive (-i) , terminal(-t) node"},{"location":"devops/docker/hugo.docker.container/#create-multiple-directories-p","text":"sh mkdir -p sbox/docker","title":"Create Multiple Directories ( -p)"},{"location":"devops/docker/hugo.docker.troubleshooting/","text":"Building The Hugo Docker Image \u00b6 Hugo Single-Stage Dockerfile \u00b6 We are just removing the second stage in order to take a closer look FROM nginx:alpine as build RUN apk add --update \\ wget ARG HUGO_VERSION = \"0.72.0\" ARG STATIC_PAGE = \"\" RUN wget --quiet \"https://github.com/gohugoio/hugo/releases/download/v ${ HUGO_VERSION } /hugo_ ${ HUGO_VERSION } _Linux-64bit.tar.gz\" && \\ tar xzf hugo_ ${ HUGO_VERSION } _Linux-64bit.tar.gz && \\ rm -r hugo_ ${ HUGO_VERSION } _Linux-64bit.tar.gz && \\ mv hugo /usr/bin COPY ./ ${ STATIC_PAGE } /site WORKDIR /site RUN hugo Build image and override multiple arguments (ARG) \u00b6 docker build --build-arg STATIC_PAGE = \"netgtz.gitlab.io\" --build-arg HUGO_VERSION = \"0.83.1\" -t hugo . Output \u00b6 :~/lab$ docker build --build-arg STATIC_PAGE = \"netgtz.gitlab.io\" --build-arg HUGO_VERSION = \"0.83.1\" -t hugo . Sending build context to Docker daemon 8 .536MB Step 1 /8 : FROM nginx:alpine as build alpine: Pulling from library/nginx 540db60ca938: Pull complete 0ae30075c5da: Pull complete 9da81141e74e: Pull complete b2e41dd2ded0: Pull complete 7f40e809fb2d: Pull complete 758848c48411: Pull complete Digest: sha256:0f8595aa040ec107821e0409a1dd3f7a5e989501d5c8d5b5ca1f955f33ac81a0 Status: Downloaded newer image for nginx:alpine ---> a6eb2a334a9f Step 2 /8 : RUN apk add --update wget ---> Running in 96e7303147c7 fetch https://dl-cdn.alpinelinux.org/alpine/v3.13/main/x86_64/APKINDEX.tar.gz fetch https://dl-cdn.alpinelinux.org/alpine/v3.13/community/x86_64/APKINDEX.tar.gz ( 1 /3 ) Installing libunistring ( 0 .9.10-r0 ) ( 2 /3 ) Installing libidn2 ( 2 .3.0-r0 ) ( 3 /3 ) Installing wget ( 1 .21.1-r1 ) Executing busybox-1.32.1-r6.trigger OK: 27 MiB in 45 packages Removing intermediate container 96e7303147c7 ---> 8c9b6b201585 Step 3 /8 : ARG HUGO_VERSION = \"0.72.0\" ---> Running in a4c26ace44df Removing intermediate container a4c26ace44df ---> 3ca9889ae589 Step 4 /8 : ARG STATIC_PAGE = \"\" ---> Running in b062a028b5b4 Removing intermediate container b062a028b5b4 ---> 44b5e545b113 Step 5 /8 : RUN wget --quiet \"https://github.com/gohugoio/hugo/releases/download/v ${ HUGO_VERSION } /hugo_ ${ HUGO_VERSION } _Linux-64bit.tar.gz\" && tar xzf hugo_ ${ HUGO_VERSION } _Linux-64bit.tar.gz && rm -r hugo_ ${ HUGO_VERSION } _Linux-64bit.tar.gz && mv hugo /usr/bin ---> Running in f172e6a54f49 Removing intermediate container f172e6a54f49 ---> 08ffd9933d8e Step 6 /8 : COPY ./ ${ STATIC_PAGE } /site ---> b99bdbfef0e1 Step 7 /8 : WORKDIR /site ---> Running in 55bbd1daf43d Removing intermediate container 55bbd1daf43d ---> cb83bc651786 Step 8 /8 : RUN hugo ---> Running in 19c79b409639 Start building sites \u2026 | EN -------------------+----- Pages | 32 Paginator pages | 0 Non-page files | 0 Static files | 13 Processed images | 0 Aliases | 0 Sitemaps | 1 Cleaned | 0 Total in 372 ms Removing intermediate container 19c79b409639 ---> 772bcf6d1bf9 Successfully built 772bcf6d1bf9 Successfully tagged hugo:latest Current images \u00b6 ~/lab$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE hugo latest 772bcf6d1bf9 About a minute ago 81 .4MB nginx alpine a6eb2a334a9f 10 days ago 22 .6MB Running a container using the hugo image \u00b6 Using the command '/bin/ash' to keep the container running Exposing container port 80 on local port 80 Passing --rm to delete container after stopping it - no need to do 'docker rm [Container ID]' docker run --rm -p 80 :80 -dit --name hugoapp hugo /bin/ash Attaching to running container docker exec -it hugoapp /bin/ash Rerunning hugo command to generate site and get its Output ~/lab$ docker exec -it hugoapp /bin/ash /site # ls archetypes assets config.toml content i18n layouts public resources static themes /site # hugo Start building sites \u2026 | EN -------------------+----- Pages | 32 Paginator pages | 0 Non-page files | 0 Static files | 13 Processed images | 0 Aliases | 0 Sitemaps | 1 Cleaned | 0 Total in 362 ms Moving hugo public folder content to nginx working directory mv public/* /usr/share/nginx/html/ Running the nginx server /usr/sbin/nginx Nginx Output sh /site # /usr/sbin/nginx 2021/06/04 22:01:54 [notice] 18#18: using the \"epoll\" event method 2021/06/04 22:01:54 [notice] 18#18: nginx/1.21.0 2021/06/04 22:01:54 [notice] 18#18: built by gcc 10.2.1 20201203 (Alpine 10.2.1_pre1) 2021/06/04 22:01:54 [notice] 18#18: OS: Linux 4.15.0-20-generic 2021/06/04 22:01:54 [notice] 18#18: getrlimit(RLIMIT_NOFILE): 1048576:1048576 /site # 2021/06/04 22:01:54 [notice] 19#19: start worker processes 2021/06/04 22:01:54 [notice] 19#19: start worker process 20","title":"Working with Hugo Container"},{"location":"devops/docker/hugo.docker.troubleshooting/#building-the-hugo-docker-image","text":"","title":"Building The Hugo Docker Image"},{"location":"devops/docker/hugo.docker.troubleshooting/#hugo-single-stage-dockerfile","text":"We are just removing the second stage in order to take a closer look FROM nginx:alpine as build RUN apk add --update \\ wget ARG HUGO_VERSION = \"0.72.0\" ARG STATIC_PAGE = \"\" RUN wget --quiet \"https://github.com/gohugoio/hugo/releases/download/v ${ HUGO_VERSION } /hugo_ ${ HUGO_VERSION } _Linux-64bit.tar.gz\" && \\ tar xzf hugo_ ${ HUGO_VERSION } _Linux-64bit.tar.gz && \\ rm -r hugo_ ${ HUGO_VERSION } _Linux-64bit.tar.gz && \\ mv hugo /usr/bin COPY ./ ${ STATIC_PAGE } /site WORKDIR /site RUN hugo","title":"Hugo Single-Stage Dockerfile"},{"location":"devops/docker/hugo.docker.troubleshooting/#build-image-and-override-multiple-arguments-arg","text":"docker build --build-arg STATIC_PAGE = \"netgtz.gitlab.io\" --build-arg HUGO_VERSION = \"0.83.1\" -t hugo .","title":"Build image and override multiple arguments (ARG)"},{"location":"devops/docker/hugo.docker.troubleshooting/#output","text":":~/lab$ docker build --build-arg STATIC_PAGE = \"netgtz.gitlab.io\" --build-arg HUGO_VERSION = \"0.83.1\" -t hugo . Sending build context to Docker daemon 8 .536MB Step 1 /8 : FROM nginx:alpine as build alpine: Pulling from library/nginx 540db60ca938: Pull complete 0ae30075c5da: Pull complete 9da81141e74e: Pull complete b2e41dd2ded0: Pull complete 7f40e809fb2d: Pull complete 758848c48411: Pull complete Digest: sha256:0f8595aa040ec107821e0409a1dd3f7a5e989501d5c8d5b5ca1f955f33ac81a0 Status: Downloaded newer image for nginx:alpine ---> a6eb2a334a9f Step 2 /8 : RUN apk add --update wget ---> Running in 96e7303147c7 fetch https://dl-cdn.alpinelinux.org/alpine/v3.13/main/x86_64/APKINDEX.tar.gz fetch https://dl-cdn.alpinelinux.org/alpine/v3.13/community/x86_64/APKINDEX.tar.gz ( 1 /3 ) Installing libunistring ( 0 .9.10-r0 ) ( 2 /3 ) Installing libidn2 ( 2 .3.0-r0 ) ( 3 /3 ) Installing wget ( 1 .21.1-r1 ) Executing busybox-1.32.1-r6.trigger OK: 27 MiB in 45 packages Removing intermediate container 96e7303147c7 ---> 8c9b6b201585 Step 3 /8 : ARG HUGO_VERSION = \"0.72.0\" ---> Running in a4c26ace44df Removing intermediate container a4c26ace44df ---> 3ca9889ae589 Step 4 /8 : ARG STATIC_PAGE = \"\" ---> Running in b062a028b5b4 Removing intermediate container b062a028b5b4 ---> 44b5e545b113 Step 5 /8 : RUN wget --quiet \"https://github.com/gohugoio/hugo/releases/download/v ${ HUGO_VERSION } /hugo_ ${ HUGO_VERSION } _Linux-64bit.tar.gz\" && tar xzf hugo_ ${ HUGO_VERSION } _Linux-64bit.tar.gz && rm -r hugo_ ${ HUGO_VERSION } _Linux-64bit.tar.gz && mv hugo /usr/bin ---> Running in f172e6a54f49 Removing intermediate container f172e6a54f49 ---> 08ffd9933d8e Step 6 /8 : COPY ./ ${ STATIC_PAGE } /site ---> b99bdbfef0e1 Step 7 /8 : WORKDIR /site ---> Running in 55bbd1daf43d Removing intermediate container 55bbd1daf43d ---> cb83bc651786 Step 8 /8 : RUN hugo ---> Running in 19c79b409639 Start building sites \u2026 | EN -------------------+----- Pages | 32 Paginator pages | 0 Non-page files | 0 Static files | 13 Processed images | 0 Aliases | 0 Sitemaps | 1 Cleaned | 0 Total in 372 ms Removing intermediate container 19c79b409639 ---> 772bcf6d1bf9 Successfully built 772bcf6d1bf9 Successfully tagged hugo:latest","title":"Output"},{"location":"devops/docker/hugo.docker.troubleshooting/#current-images","text":"~/lab$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE hugo latest 772bcf6d1bf9 About a minute ago 81 .4MB nginx alpine a6eb2a334a9f 10 days ago 22 .6MB","title":"Current images"},{"location":"devops/docker/hugo.docker.troubleshooting/#running-a-container-using-the-hugo-image","text":"Using the command '/bin/ash' to keep the container running Exposing container port 80 on local port 80 Passing --rm to delete container after stopping it - no need to do 'docker rm [Container ID]' docker run --rm -p 80 :80 -dit --name hugoapp hugo /bin/ash Attaching to running container docker exec -it hugoapp /bin/ash Rerunning hugo command to generate site and get its Output ~/lab$ docker exec -it hugoapp /bin/ash /site # ls archetypes assets config.toml content i18n layouts public resources static themes /site # hugo Start building sites \u2026 | EN -------------------+----- Pages | 32 Paginator pages | 0 Non-page files | 0 Static files | 13 Processed images | 0 Aliases | 0 Sitemaps | 1 Cleaned | 0 Total in 362 ms Moving hugo public folder content to nginx working directory mv public/* /usr/share/nginx/html/ Running the nginx server /usr/sbin/nginx Nginx Output sh /site # /usr/sbin/nginx 2021/06/04 22:01:54 [notice] 18#18: using the \"epoll\" event method 2021/06/04 22:01:54 [notice] 18#18: nginx/1.21.0 2021/06/04 22:01:54 [notice] 18#18: built by gcc 10.2.1 20201203 (Alpine 10.2.1_pre1) 2021/06/04 22:01:54 [notice] 18#18: OS: Linux 4.15.0-20-generic 2021/06/04 22:01:54 [notice] 18#18: getrlimit(RLIMIT_NOFILE): 1048576:1048576 /site # 2021/06/04 22:01:54 [notice] 19#19: start worker processes 2021/06/04 22:01:54 [notice] 19#19: start worker process 20","title":"Running a container using the hugo image"},{"location":"devops/docker/intro/","text":"Docker Documentation docker is currently the leading devops provider. Hugo Docker Image \u00b6 My netgtz.gitlab.io site has the baseURL as baseURL = \" https://netgtz.gitlab.io/ \". Using this will not render the site correctly... * Requirements ``` config.toml \u00b6 baseURL = \"/\" ```","title":"Introduction"},{"location":"devops/docker/intro/#hugo-docker-image","text":"My netgtz.gitlab.io site has the baseURL as baseURL = \" https://netgtz.gitlab.io/ \". Using this will not render the site correctly... * Requirements ```","title":"Hugo Docker Image"},{"location":"devops/docker/intro/#configtoml","text":"baseURL = \"/\" ```","title":"config.toml"},{"location":"devops/kubernetes/intro/","text":"Kubernetes Documentation kubernetes is currently the leading devops provider. Kubernetes at a Glance \u00b6 What is Kubernetes? Kubernetes is an open source container management and orchestration system which provides built in cloud navite features: * Managing clusters of containers * Providing tools for deploying applications * Auto Scaling applications as and when needed * Managing changes to the existing containerized applications * Helping to optimize the use of underlying hardware beneath your container * Enableing the application component to restart and move across the system as and when needed Kubernetes Architecture and Componets \u00b6 Kubernetes includes multiple components to manage and control the cluster: Backend System Pods root@f8aca9a6c11c:/home/cloud_user# kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE coredns-584795fc57-qd468 1 /1 Running 0 25m coredns-584795fc57-whf8j 1 /1 Running 0 25m etcd-f8aca9a6c11c.mylabserver.com 1 /1 Running 0 25m kube-apiserver-f8aca9a6c11c.mylabserver.com 1 /1 Running 0 25m kube-controller-manager-f8aca9a6c11c.mylabserver.com 1 /1 Running 0 24m kube-flannel-ds-amd64-dk4bm 1 /1 Running 0 10m kube-flannel-ds-amd64-rbz6w 1 /1 Running 0 10m kube-flannel-ds-amd64-xgvnv 1 /1 Running 0 10m kube-proxy-hc6xr 1 /1 Running 0 19m kube-proxy-hlmlk 1 /1 Running 0 25m kube-proxy-wpmhm 1 /1 Running 0 20m kube-scheduler-f8aca9a6c11c.mylabserver.com 1 /1 Running 0 25m Kube Master Control Plane \u00b6 etcd : provides distrubuted synchronized data storage for the cluster state. Information about the cluster such as pods, nodes, etc are stored here. kube-apiserver : Primary interface for the cluster which is a simple REST based web API. kubectl this API to interact with the cluster. kube-controller-manager : bundles several backend components doing all the behind the scenes work of controlling the cluster. kube-scheduler : determines when to run pods and what nodes to run them on based on deployments or other automation Individual Kube Node Components \u00b6 kubelet : agent used to interact between the kubernetes's API and contanier's runtime. Masters control place communicates to node kubelets which instruct docker to create/run the container. Master and worker nodes have this agent installed kubelet runs as a service root@f8aca9a6c11c:/home/cloud_user# sudo systemctl status kubelet \u25cf kubelet.service - kubelet: The Kubernetes Node Agent Loaded: loaded ( /lib/systemd/system/kubelet.service ; enabled ; vendor preset: enabled ) Drop-In: /etc/systemd/system/kubelet.service.d \u2514\u250010-kubeadm.conf Active: active ( running ) since Sun 2021 -07-11 13 :41:35 UTC ; 1h 18min ago Docs: https://kubernetes.io/docs/home/ Main PID: 7174 ( kubelet ) Tasks: 17 ( limit: 2312 ) CGroup: /system.slice/kubelet.service \u2514\u25007174 /usr/bin/kubelet --bootstrap-kubeconfig = /etc/kubernetes/bootstrap-kubelet.conf --kubeconfig = /etc/kubernetes/kubelet.conf --config = /var/lib/kubelet/conf Jul 11 13 :56:51 f8aca9a6c11c.mylabserver.com kubelet [ 7174 ] : E0711 13 :56:51.107949 7174 kubelet.go:2170 ] Container runtime network not ready: NetworkReady = false reason Jul 11 13 :56:55 f8aca9a6c11c.mylabserver.com kubelet [ 7174 ] : W0711 13 :56:55.713797 7174 cni.go:213 ] Unable to update cni config: No networks found in /etc/cni/net.d Jul 11 13 :56:56 f8aca9a6c11c.mylabserver.com kubelet [ 7174 ] : E0711 13 :56:56.109215 7174 kubelet.go:2170 ] Container runtime network not ready: NetworkReady = false reason Jul 11 13 :56:59 f8aca9a6c11c.mylabserver.com kubelet [ 7174 ] : E0711 13 :56:59.901445 7174 reflector.go:126 ] object- \"kube-system\" / \"flannel-token-sxgpr\" : Failed to list *v Jul 11 13 :57:00 f8aca9a6c11c.mylabserver.com kubelet [ 7174 ] : I0711 13 :57:00.049225 7174 reconciler.go:207 ] operationExecutor.VerifyControllerAttachedVolume started for Jul 11 13 :57:00 f8aca9a6c11c.mylabserver.com kubelet [ 7174 ] : I0711 13 :57:00.052181 7174 reconciler.go:207 ] operationExecutor.VerifyControllerAttachedVolume started for Jul 11 13 :57:00 f8aca9a6c11c.mylabserver.com kubelet [ 7174 ] : I0711 13 :57:00.052995 7174 reconciler.go:207 ] operationExecutor.VerifyControllerAttachedVolume started for Jul 11 13 :57:00 f8aca9a6c11c.mylabserver.com kubelet [ 7174 ] : I0711 13 :57:00.058499 7174 reconciler.go:207 ] operationExecutor.VerifyControllerAttachedVolume started for Jul 11 13 :57:00 f8aca9a6c11c.mylabserver.com kubelet [ 7174 ] : W0711 13 :57:00.714022 7174 cni.go:213 ] Unable to update cni config: No networks found in /etc/cni/net.d Jul 11 13 :57:01 f8aca9a6c11c.mylabserver.com kubelet [ 7174 ] : E0711 13 :57:01.110293 7174 kubelet.go:2170 ] Container runtime network not ready: NetworkReady = false reason r * ****kube-proxy***: each nodes ( master and workers ) needs its own, handles virtual network communication between nodes by adding firewall routing rules when pods are trying to communicate across nodes.","title":"Introduction"},{"location":"devops/kubernetes/intro/#kubernetes-at-a-glance","text":"What is Kubernetes? Kubernetes is an open source container management and orchestration system which provides built in cloud navite features: * Managing clusters of containers * Providing tools for deploying applications * Auto Scaling applications as and when needed * Managing changes to the existing containerized applications * Helping to optimize the use of underlying hardware beneath your container * Enableing the application component to restart and move across the system as and when needed","title":"Kubernetes at a Glance"},{"location":"devops/kubernetes/intro/#kubernetes-architecture-and-componets","text":"Kubernetes includes multiple components to manage and control the cluster: Backend System Pods root@f8aca9a6c11c:/home/cloud_user# kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE coredns-584795fc57-qd468 1 /1 Running 0 25m coredns-584795fc57-whf8j 1 /1 Running 0 25m etcd-f8aca9a6c11c.mylabserver.com 1 /1 Running 0 25m kube-apiserver-f8aca9a6c11c.mylabserver.com 1 /1 Running 0 25m kube-controller-manager-f8aca9a6c11c.mylabserver.com 1 /1 Running 0 24m kube-flannel-ds-amd64-dk4bm 1 /1 Running 0 10m kube-flannel-ds-amd64-rbz6w 1 /1 Running 0 10m kube-flannel-ds-amd64-xgvnv 1 /1 Running 0 10m kube-proxy-hc6xr 1 /1 Running 0 19m kube-proxy-hlmlk 1 /1 Running 0 25m kube-proxy-wpmhm 1 /1 Running 0 20m kube-scheduler-f8aca9a6c11c.mylabserver.com 1 /1 Running 0 25m","title":"Kubernetes Architecture and Componets"},{"location":"devops/kubernetes/intro/#kube-master-control-plane","text":"etcd : provides distrubuted synchronized data storage for the cluster state. Information about the cluster such as pods, nodes, etc are stored here. kube-apiserver : Primary interface for the cluster which is a simple REST based web API. kubectl this API to interact with the cluster. kube-controller-manager : bundles several backend components doing all the behind the scenes work of controlling the cluster. kube-scheduler : determines when to run pods and what nodes to run them on based on deployments or other automation","title":"Kube Master Control Plane"},{"location":"devops/kubernetes/intro/#individual-kube-node-components","text":"kubelet : agent used to interact between the kubernetes's API and contanier's runtime. Masters control place communicates to node kubelets which instruct docker to create/run the container. Master and worker nodes have this agent installed kubelet runs as a service root@f8aca9a6c11c:/home/cloud_user# sudo systemctl status kubelet \u25cf kubelet.service - kubelet: The Kubernetes Node Agent Loaded: loaded ( /lib/systemd/system/kubelet.service ; enabled ; vendor preset: enabled ) Drop-In: /etc/systemd/system/kubelet.service.d \u2514\u250010-kubeadm.conf Active: active ( running ) since Sun 2021 -07-11 13 :41:35 UTC ; 1h 18min ago Docs: https://kubernetes.io/docs/home/ Main PID: 7174 ( kubelet ) Tasks: 17 ( limit: 2312 ) CGroup: /system.slice/kubelet.service \u2514\u25007174 /usr/bin/kubelet --bootstrap-kubeconfig = /etc/kubernetes/bootstrap-kubelet.conf --kubeconfig = /etc/kubernetes/kubelet.conf --config = /var/lib/kubelet/conf Jul 11 13 :56:51 f8aca9a6c11c.mylabserver.com kubelet [ 7174 ] : E0711 13 :56:51.107949 7174 kubelet.go:2170 ] Container runtime network not ready: NetworkReady = false reason Jul 11 13 :56:55 f8aca9a6c11c.mylabserver.com kubelet [ 7174 ] : W0711 13 :56:55.713797 7174 cni.go:213 ] Unable to update cni config: No networks found in /etc/cni/net.d Jul 11 13 :56:56 f8aca9a6c11c.mylabserver.com kubelet [ 7174 ] : E0711 13 :56:56.109215 7174 kubelet.go:2170 ] Container runtime network not ready: NetworkReady = false reason Jul 11 13 :56:59 f8aca9a6c11c.mylabserver.com kubelet [ 7174 ] : E0711 13 :56:59.901445 7174 reflector.go:126 ] object- \"kube-system\" / \"flannel-token-sxgpr\" : Failed to list *v Jul 11 13 :57:00 f8aca9a6c11c.mylabserver.com kubelet [ 7174 ] : I0711 13 :57:00.049225 7174 reconciler.go:207 ] operationExecutor.VerifyControllerAttachedVolume started for Jul 11 13 :57:00 f8aca9a6c11c.mylabserver.com kubelet [ 7174 ] : I0711 13 :57:00.052181 7174 reconciler.go:207 ] operationExecutor.VerifyControllerAttachedVolume started for Jul 11 13 :57:00 f8aca9a6c11c.mylabserver.com kubelet [ 7174 ] : I0711 13 :57:00.052995 7174 reconciler.go:207 ] operationExecutor.VerifyControllerAttachedVolume started for Jul 11 13 :57:00 f8aca9a6c11c.mylabserver.com kubelet [ 7174 ] : I0711 13 :57:00.058499 7174 reconciler.go:207 ] operationExecutor.VerifyControllerAttachedVolume started for Jul 11 13 :57:00 f8aca9a6c11c.mylabserver.com kubelet [ 7174 ] : W0711 13 :57:00.714022 7174 cni.go:213 ] Unable to update cni config: No networks found in /etc/cni/net.d Jul 11 13 :57:01 f8aca9a6c11c.mylabserver.com kubelet [ 7174 ] : E0711 13 :57:01.110293 7174 kubelet.go:2170 ] Container runtime network not ready: NetworkReady = false reason r * ****kube-proxy***: each nodes ( master and workers ) needs its own, handles virtual network communication between nodes by adding firewall routing rules when pods are trying to communicate across nodes.","title":"Individual Kube Node Components"},{"location":"devops/kubernetes/kubernetes-cluster/","text":"Creating a Kubernetes Cluster using Kubeadm \u00b6 Kubeadm is a tool to easily bootstrap a kubernetes cluster with the minimun requirements. Info all nodes includes nodes and master nodes. graph TD subgraph \"Kubernetes Cluster\" A{Master Node} -->|Node1|B(Node One) A{Master Node} -->|Node2|C(Node Two) A{Master Node} -->|Node3|D(Node Three) end Requirements \u00b6 Instance - Ubuntu 18.04 Bionic Beaver LTS Docker Kubeadm, Kubelet, and Kubectl Docker install \u00b6 Install Docker on all nodes including master node: curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo add-apt-repository \\ \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $( lsb_release -cs ) \\ stable\" sudo apt-get update sudo apt-get install -y docker-ce = 18 .06.1~ce~3-0~ubuntu sudo apt-mark hold docker-ce if running any other linux flavor use the convenience script provided by docker. ~$ curl -fsSL https://get.docker.com -o get-docker.sh ~$ sudo sh get-docker.sh ``` 3 . Verify that Docker is up and running with: ``` bash ~$ sudo systemctl status docker \u25cf docker.service - Docker Application Container Engine Loaded: loaded ( /lib/systemd/system/docker.service ; enabled ; vendor preset: enabled ) Active: active ( running ) since Fri 2021 -07-16 19 :47:37 UTC ; 3h 49min ago Docs: https://docs.docker.com Main PID: 909 ( dockerd ) Make sure the Docker service status is active (running) ! ~$ sudo docker version Client: Version: 18 .06.1-ce API version: 1 .38 Go version: go1.10.3 Git commit: e68fc7a Built: Tue Aug 21 17 :24:51 2018 OS/Arch: linux/amd64 Experimental: false Server: Engine: Version: 18 .06.1-ce API version: 1 .38 ( minimum version 1 .12 ) Go version: go1.10.3 Git commit: e68fc7a Built: Tue Aug 21 17 :23:15 2018 OS/Arch: linux/amd64 Experimental: false Install Kubeadm, Kubelet, and Kubectl on all nodes. \u00b6 kubeadm: cluster bootstrapper. kubelet: creates pods and containers, run on all members of the cluster. kubectl: communicates with cluster's API. Install the Kubernetes components by running this on all nodes: curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - cat << EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF sudo apt-get update sudo apt-get install -y kubelet = 1 .14.5-00 kubeadm = 1 .14.5-00 kubectl = 1 .14.5-00 sudo apt-mark hold kubelet kubeadm kubectl Bootstrap the cluster on the Kube master node. \u00b6 On the Kube master node, do this: sudo kubeadm init --pod-network-cidr = 10 .10.0.0/16 That command may take a few minutes to complete. output Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME /.kube sudo cp -i /etc/kubernetes/admin.conf $HOME /.kube/config sudo chown $( id -u ) : $( id -g ) $HOME /.kube/config You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ Then you can join any number of worker nodes by running the following on each as root: kubeadm join 172 .31.109.72:6443 --token ku77lq.2aupucmcqy3mmjuz \\ --discovery-token-ca-cert-hash sha256:533e64c88c12b7f40b8bf2283ac15dc1b080003ad92c8f53e1742c86e94d6f4c running kubectl as non-root user: mkdir -p $HOME /.kube sudo cp -i /etc/kubernetes/admin.conf $HOME /.kube/config sudo chown $( id -u ) : $( id -g ) $HOME /.kube/config optionally export KUBECONFIG = /etc/kubernetes/admin.conf Take note that the kubeadm init command printed a long kubeadm join command to the screen. You will need that kubeadm join command in the next step! Run the following commmand on the Kube master node to verify it is up and running: kubectl version This command should return both a Client Version and a Server Version . Join the two Kube worker nodes to the cluster. \u00b6 Copy the kubeadm join command that was printed by the kubeadm init command earlier, with the token and hash. Run this command on both worker nodes, but make sure you add sudo in front of it: sudo kubeadm join $some_ip :6443 --token $some_token --discovery-token-ca-cert-hash $some_hash Now, on the Kube master node, make sure your nodes joined the cluster successfully: kubectl get nodes Verify that all of your nodes are listed. It will look something like this: NAME STATUS ROLES AGE VERSION ip-10-0-1-101 NotReady master 30s v1.12.2 ip-10-0-1-102 NotReady & lt ; none> 8s v1.12.2 ip-10-0-1-103 NotReady & lt ; none> 5s v1.12.2 Note that the nodes are expected to be in the NotReady state for now. Set up cluster networking with flannel. \u00b6 Turn on iptables bridge calls on all nodes: echo \"net.bridge.bridge-nf-call-iptables=1\" | sudo tee -a /etc/sysctl.conf sudo sysctl -p Next, run this only on the Kube master node: kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml Now flannel is installed! Make sure it is working by checking the node status again: kubectl get nodes After a short time, all nodes should be in the Ready state. If they are not all Ready the first time you run kubectl get nodes , wait a few moments and try again. It should look something like this: bash NAME STATUS ROLES AGE VERSION ip-10-0-1-101 Ready master 85s v1.12.2 ip-10-0-1-102 Ready &lt;none> 63s v1.12.2 ip-10-0-1-103 Ready &lt;none> 60s v1.12.2","title":"Kubernetes Cluster"},{"location":"devops/kubernetes/kubernetes-cluster/#creating-a-kubernetes-cluster-using-kubeadm","text":"Kubeadm is a tool to easily bootstrap a kubernetes cluster with the minimun requirements. Info all nodes includes nodes and master nodes. graph TD subgraph \"Kubernetes Cluster\" A{Master Node} -->|Node1|B(Node One) A{Master Node} -->|Node2|C(Node Two) A{Master Node} -->|Node3|D(Node Three) end","title":"Creating a Kubernetes Cluster using Kubeadm"},{"location":"devops/kubernetes/kubernetes-cluster/#requirements","text":"Instance - Ubuntu 18.04 Bionic Beaver LTS Docker Kubeadm, Kubelet, and Kubectl","title":"Requirements"},{"location":"devops/kubernetes/kubernetes-cluster/#docker-install","text":"Install Docker on all nodes including master node: curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo add-apt-repository \\ \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $( lsb_release -cs ) \\ stable\" sudo apt-get update sudo apt-get install -y docker-ce = 18 .06.1~ce~3-0~ubuntu sudo apt-mark hold docker-ce if running any other linux flavor use the convenience script provided by docker. ~$ curl -fsSL https://get.docker.com -o get-docker.sh ~$ sudo sh get-docker.sh ``` 3 . Verify that Docker is up and running with: ``` bash ~$ sudo systemctl status docker \u25cf docker.service - Docker Application Container Engine Loaded: loaded ( /lib/systemd/system/docker.service ; enabled ; vendor preset: enabled ) Active: active ( running ) since Fri 2021 -07-16 19 :47:37 UTC ; 3h 49min ago Docs: https://docs.docker.com Main PID: 909 ( dockerd ) Make sure the Docker service status is active (running) ! ~$ sudo docker version Client: Version: 18 .06.1-ce API version: 1 .38 Go version: go1.10.3 Git commit: e68fc7a Built: Tue Aug 21 17 :24:51 2018 OS/Arch: linux/amd64 Experimental: false Server: Engine: Version: 18 .06.1-ce API version: 1 .38 ( minimum version 1 .12 ) Go version: go1.10.3 Git commit: e68fc7a Built: Tue Aug 21 17 :23:15 2018 OS/Arch: linux/amd64 Experimental: false","title":"Docker install"},{"location":"devops/kubernetes/kubernetes-cluster/#install-kubeadm-kubelet-and-kubectl-on-all-nodes","text":"kubeadm: cluster bootstrapper. kubelet: creates pods and containers, run on all members of the cluster. kubectl: communicates with cluster's API. Install the Kubernetes components by running this on all nodes: curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - cat << EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF sudo apt-get update sudo apt-get install -y kubelet = 1 .14.5-00 kubeadm = 1 .14.5-00 kubectl = 1 .14.5-00 sudo apt-mark hold kubelet kubeadm kubectl","title":"Install Kubeadm, Kubelet, and Kubectl on all nodes."},{"location":"devops/kubernetes/kubernetes-cluster/#bootstrap-the-cluster-on-the-kube-master-node","text":"On the Kube master node, do this: sudo kubeadm init --pod-network-cidr = 10 .10.0.0/16 That command may take a few minutes to complete. output Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME /.kube sudo cp -i /etc/kubernetes/admin.conf $HOME /.kube/config sudo chown $( id -u ) : $( id -g ) $HOME /.kube/config You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ Then you can join any number of worker nodes by running the following on each as root: kubeadm join 172 .31.109.72:6443 --token ku77lq.2aupucmcqy3mmjuz \\ --discovery-token-ca-cert-hash sha256:533e64c88c12b7f40b8bf2283ac15dc1b080003ad92c8f53e1742c86e94d6f4c running kubectl as non-root user: mkdir -p $HOME /.kube sudo cp -i /etc/kubernetes/admin.conf $HOME /.kube/config sudo chown $( id -u ) : $( id -g ) $HOME /.kube/config optionally export KUBECONFIG = /etc/kubernetes/admin.conf Take note that the kubeadm init command printed a long kubeadm join command to the screen. You will need that kubeadm join command in the next step! Run the following commmand on the Kube master node to verify it is up and running: kubectl version This command should return both a Client Version and a Server Version .","title":"Bootstrap the cluster on the Kube master node."},{"location":"devops/kubernetes/kubernetes-cluster/#join-the-two-kube-worker-nodes-to-the-cluster","text":"Copy the kubeadm join command that was printed by the kubeadm init command earlier, with the token and hash. Run this command on both worker nodes, but make sure you add sudo in front of it: sudo kubeadm join $some_ip :6443 --token $some_token --discovery-token-ca-cert-hash $some_hash Now, on the Kube master node, make sure your nodes joined the cluster successfully: kubectl get nodes Verify that all of your nodes are listed. It will look something like this: NAME STATUS ROLES AGE VERSION ip-10-0-1-101 NotReady master 30s v1.12.2 ip-10-0-1-102 NotReady & lt ; none> 8s v1.12.2 ip-10-0-1-103 NotReady & lt ; none> 5s v1.12.2 Note that the nodes are expected to be in the NotReady state for now.","title":"Join the two Kube worker nodes to the cluster."},{"location":"devops/kubernetes/kubernetes-cluster/#set-up-cluster-networking-with-flannel","text":"Turn on iptables bridge calls on all nodes: echo \"net.bridge.bridge-nf-call-iptables=1\" | sudo tee -a /etc/sysctl.conf sudo sysctl -p Next, run this only on the Kube master node: kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml Now flannel is installed! Make sure it is working by checking the node status again: kubectl get nodes After a short time, all nodes should be in the Ready state. If they are not all Ready the first time you run kubectl get nodes , wait a few moments and try again. It should look something like this: bash NAME STATUS ROLES AGE VERSION ip-10-0-1-101 Ready master 85s v1.12.2 ip-10-0-1-102 Ready &lt;none> 63s v1.12.2 ip-10-0-1-103 Ready &lt;none> 60s v1.12.2","title":"Set up cluster networking with flannel."},{"location":"devops/kubernetes/kubernetes-deployments/","text":"Kubernetes Deployments \u00b6 Deployments provide a way to spin up and automate multiple pods by specifying a desired state to be maintained by the cluster. * Scaling the deployment will either create or delete pods in order to meet the number of replicas requested. The deployment can dynamically adjust itself without impacting appliaction. * Rolling Updates : the deployment can be adjusted to a new image version. it will gradually spin up new containers with the new vesrion to replace existing containers. * Self-Healing : if any of pods in the deployment is destroyed, the deployment will create new ones in order to meet desired number of replicas. deployment \u00b6 apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx-app spec: replicas: 2 selector: matchLabels: app: nginx-app template: metadata: labels: app: nginx-app spec: containers: - name: nginx image: nginx:1.20.1 ports: - containerPort: 80 use kubectl create -f <deployment-file.yml> to build deployment. # kubectl create -f ./kube-deployment.yml deployment.apps/nginx-deployment created # kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE nginx-deployment 2 /2 2 2 9s # kubectl get pods NAME READY STATUS RESTARTS AGE nginx-deployment-6fb8795c7d-dq9zp 1 /1 Running 0 22s nginx-deployment-6fb8795c7d-r8bwk 1 /1 Running 0 22s the kubectl describe deployment <deployment-name> provides detailed information about the deployment. the event section provides any deployment changes such scaling, self-healing, and rolling-updates # kubectl describe deployment nginx-deployment Name: nginx-deployment Namespace: default CreationTimestamp: Thu, 15 Jul 2021 23 :57:20 +0000 Labels: app = nginx-app Annotations: deployment.kubernetes.io/revision: 1 Selector: app = nginx-app Replicas: 2 desired | 2 updated | 2 total | 2 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25 % max unavailable, 25 % max surge Pod Template: Labels: app = nginx-app Containers: nginx: Image: nginx:1.20.1 Port: 80 /TCP Host Port: 0 /TCP Environment: <none> Mounts: <none> Volumes: <none> Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailable OldReplicaSets: <none> NewReplicaSet: nginx-deployment-7d569cb5f5 ( 2 /2 replicas created ) Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScalingReplicaSet 16s deployment-controller Scaled up replica set nginx-deployment-7d569cb5f5 to 2 ``` #### Deployment Scaling the deployment will either create or delete pods in order to meet the number of replicas requested. The deployment can dynamically adjust itself without impacting appliaction. ###### Scale up * Edit the ``` deployment.yml ``` file spec: replicas from ``` 2 ``` to ``` 6 ``` . Optionally, we can edit the deployment running-config using ``` kubectl edit deployment.v1.apps/nginx-deployment ``` ``` yml ##omitted sections spec: replicas: 6 * apply changes # kubectl apply -f kube-deployment.yml Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply deployment.apps/nginx-deployment configured * Optionally, we can edit the deployment running-config using kubectl edit deployment.v1.apps/nginx-deployment use i to edit and [ESC] followed by :wq! to apply changes we can do it with a single command kubectl scale deployments/nginx-deployment --replicas=6 # kubectl edit deployment.v1.apps/nginx-deployment deployment.apps/nginx-deployment edited * Validating # kubectl get pods NAME READY STATUS RESTARTS AGE nginx-deployment-7d569cb5f5-5kc9r 0 /1 ContainerCreating 0 2s nginx-deployment-7d569cb5f5-cqdqg 0 /1 ContainerCreating 0 2s nginx-deployment-7d569cb5f5-h7x79 1 /1 Running 0 26m nginx-deployment-7d569cb5f5-l2pbh 0 /1 ContainerCreating 0 2s nginx-deployment-7d569cb5f5-r2ln4 1 /1 Running 0 26m nginx-deployment-7d569cb5f5-wtbsw 0 /1 ContainerCreating 0 2s # kubectl get pods NAME READY STATUS RESTARTS AGE nginx-deployment-7d569cb5f5-5kc9r 1 /1 Running 0 4s nginx-deployment-7d569cb5f5-cqdqg 1 /1 Running 0 4s nginx-deployment-7d569cb5f5-h7x79 1 /1 Running 0 26m nginx-deployment-7d569cb5f5-l2pbh 1 /1 Running 0 4s nginx-deployment-7d569cb5f5-r2ln4 1 /1 Running 0 26m nginx-deployment-7d569cb5f5-wtbsw 1 /1 Running 0 4s Scale Down \u00b6 update kubectl edit deployment.v1.apps/nginx-deployment replicas from 6 to 2 # kubectl get pods NAME READY STATUS RESTARTS AGE nginx-deployment-7d569cb5f5-5kc9r 0/1 Terminating 0 2m15s nginx-deployment-7d569cb5f5-cqdqg 1/1 Running 0 2m15s nginx-deployment-7d569cb5f5-h7x79 1/1 Running 0 28m nginx-deployment-7d569cb5f5-l2pbh 1/1 Running 0 2m15s nginx-deployment-7d569cb5f5-r2ln4 1/1 Running 0 28m nginx-deployment-7d569cb5f5-wtbsw 0/1 Terminating 0 2m15s # kubectl get pods NAME READY STATUS RESTARTS AGE nginx-deployment-7d569cb5f5-cqdqg 1/1 Running 0 2m20s nginx-deployment-7d569cb5f5-h7x79 1/1 Running 0 28m nginx-deployment-7d569cb5f5-l2pbh 1/1 Running 0 2m20s nginx-deployment-7d569cb5f5-r2ln4 1/1 Running 0 28m Scaling Loggs \u00b6 # kubectl describe deployment nginx-deployment ### ommitted output ### Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScalingReplicaSet 30m deployment-controller Scaled up replica set nginx-deployment-7d569cb5f5 to 2 Normal ScalingReplicaSet 7m12s deployment-controller Scaled down replica set nginx-deployment-7d569cb5f5 to 2 Normal ScalingReplicaSet 3m25s ( x3 over 16m ) deployment-controller Scaled up replica set nginx-deployment-7d569cb5f5 to 6 Normal ScalingReplicaSet 72s ( x2 over 11m ) deployment-controller Scaled down replica set nginx-deployment-7d569cb5f5 to 4 Deployment Self-Healing \u00b6 if any of pods in the deployment is destroyed, the deployment will create new ones in order to meet desired number of replicas # kubectl get pods NAME READY STATUS RESTARTS AGE nginx-deployment-7d569cb5f5-cqdqg 1 /1 Running 0 8m nginx-deployment-7d569cb5f5-l2pbh 1 /1 Running 0 8m nginx-deployment-7d569cb5f5-nbb4w 1 /1 Running 0 4s nginx-deployment-7d569cb5f5-r2ln4 1 /1 Running 0 34m # kubectl delete pod nginx-deployment-7d569cb5f5-r2ln4 pod \"nginx-deployment-7d569cb5f5-r2ln4\" deleted # kubectl get pods NAME READY STATUS RESTARTS AGE nginx-deployment-7d569cb5f5-cqdqg 1 /1 Running 0 8m38s nginx-deployment-7d569cb5f5-l2pbh 1 /1 Running 0 8m38s nginx-deployment-7d569cb5f5-nbb4w 1 /1 Running 0 42s nginx-deployment-7d569cb5f5-tjsnw 1 /1 Running 0 3s the cluster build a new container right after I requested the deletion immediately. I couldnt catch the cluster creating the new container but we can tell by looking at the AGE value. nginx-deployment-7d569cb5f5-tjsnw was created when I deleted nginx-deployment-7d569cb5f5-r2ln4 . Deployment Rolling Updates \u00b6 the deployment can be adjusted to a new image version. it will gradually spin up new containers with the new vesrion to replace existing containers. Upgrading from image: nginx:1.20.1 to nginx=nginx:1.21.1 . The cluster will create new containers with the new image once those containers are running it will terminate the containers running the old image: # kubectl get pods NAME READY STATUS RESTARTS AGE nginx-deployment-5cfcccdb74-ck2nj 1 /1 Running 0 7s nginx-deployment-5cfcccdb74-lvgq2 0 /1 ContainerCreating 0 2s nginx-deployment-5cfcccdb74-v2k4g 0 /1 ContainerCreating 0 1s nginx-deployment-5cfcccdb74-zn666 1 /1 Running 0 7s nginx-deployment-7d569cb5f5-cqdqg 1 /1 Terminating 0 17m nginx-deployment-7d569cb5f5-l2pbh 1 /1 Running 0 17m nginx-deployment-7d569cb5f5-nbb4w 0 /1 Terminating 0 9m29s Conclusion \u00b6 Edittig the running-config vs editing the actual file and apply the changes works the same way.","title":"Kubernetes Deployments"},{"location":"devops/kubernetes/kubernetes-deployments/#kubernetes-deployments","text":"Deployments provide a way to spin up and automate multiple pods by specifying a desired state to be maintained by the cluster. * Scaling the deployment will either create or delete pods in order to meet the number of replicas requested. The deployment can dynamically adjust itself without impacting appliaction. * Rolling Updates : the deployment can be adjusted to a new image version. it will gradually spin up new containers with the new vesrion to replace existing containers. * Self-Healing : if any of pods in the deployment is destroyed, the deployment will create new ones in order to meet desired number of replicas.","title":"Kubernetes Deployments"},{"location":"devops/kubernetes/kubernetes-deployments/#deployment","text":"apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx-app spec: replicas: 2 selector: matchLabels: app: nginx-app template: metadata: labels: app: nginx-app spec: containers: - name: nginx image: nginx:1.20.1 ports: - containerPort: 80 use kubectl create -f <deployment-file.yml> to build deployment. # kubectl create -f ./kube-deployment.yml deployment.apps/nginx-deployment created # kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE nginx-deployment 2 /2 2 2 9s # kubectl get pods NAME READY STATUS RESTARTS AGE nginx-deployment-6fb8795c7d-dq9zp 1 /1 Running 0 22s nginx-deployment-6fb8795c7d-r8bwk 1 /1 Running 0 22s the kubectl describe deployment <deployment-name> provides detailed information about the deployment. the event section provides any deployment changes such scaling, self-healing, and rolling-updates # kubectl describe deployment nginx-deployment Name: nginx-deployment Namespace: default CreationTimestamp: Thu, 15 Jul 2021 23 :57:20 +0000 Labels: app = nginx-app Annotations: deployment.kubernetes.io/revision: 1 Selector: app = nginx-app Replicas: 2 desired | 2 updated | 2 total | 2 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25 % max unavailable, 25 % max surge Pod Template: Labels: app = nginx-app Containers: nginx: Image: nginx:1.20.1 Port: 80 /TCP Host Port: 0 /TCP Environment: <none> Mounts: <none> Volumes: <none> Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailable OldReplicaSets: <none> NewReplicaSet: nginx-deployment-7d569cb5f5 ( 2 /2 replicas created ) Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScalingReplicaSet 16s deployment-controller Scaled up replica set nginx-deployment-7d569cb5f5 to 2 ``` #### Deployment Scaling the deployment will either create or delete pods in order to meet the number of replicas requested. The deployment can dynamically adjust itself without impacting appliaction. ###### Scale up * Edit the ``` deployment.yml ``` file spec: replicas from ``` 2 ``` to ``` 6 ``` . Optionally, we can edit the deployment running-config using ``` kubectl edit deployment.v1.apps/nginx-deployment ``` ``` yml ##omitted sections spec: replicas: 6 * apply changes # kubectl apply -f kube-deployment.yml Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply deployment.apps/nginx-deployment configured * Optionally, we can edit the deployment running-config using kubectl edit deployment.v1.apps/nginx-deployment use i to edit and [ESC] followed by :wq! to apply changes we can do it with a single command kubectl scale deployments/nginx-deployment --replicas=6 # kubectl edit deployment.v1.apps/nginx-deployment deployment.apps/nginx-deployment edited * Validating # kubectl get pods NAME READY STATUS RESTARTS AGE nginx-deployment-7d569cb5f5-5kc9r 0 /1 ContainerCreating 0 2s nginx-deployment-7d569cb5f5-cqdqg 0 /1 ContainerCreating 0 2s nginx-deployment-7d569cb5f5-h7x79 1 /1 Running 0 26m nginx-deployment-7d569cb5f5-l2pbh 0 /1 ContainerCreating 0 2s nginx-deployment-7d569cb5f5-r2ln4 1 /1 Running 0 26m nginx-deployment-7d569cb5f5-wtbsw 0 /1 ContainerCreating 0 2s # kubectl get pods NAME READY STATUS RESTARTS AGE nginx-deployment-7d569cb5f5-5kc9r 1 /1 Running 0 4s nginx-deployment-7d569cb5f5-cqdqg 1 /1 Running 0 4s nginx-deployment-7d569cb5f5-h7x79 1 /1 Running 0 26m nginx-deployment-7d569cb5f5-l2pbh 1 /1 Running 0 4s nginx-deployment-7d569cb5f5-r2ln4 1 /1 Running 0 26m nginx-deployment-7d569cb5f5-wtbsw 1 /1 Running 0 4s","title":"deployment"},{"location":"devops/kubernetes/kubernetes-deployments/#scale-down","text":"update kubectl edit deployment.v1.apps/nginx-deployment replicas from 6 to 2 # kubectl get pods NAME READY STATUS RESTARTS AGE nginx-deployment-7d569cb5f5-5kc9r 0/1 Terminating 0 2m15s nginx-deployment-7d569cb5f5-cqdqg 1/1 Running 0 2m15s nginx-deployment-7d569cb5f5-h7x79 1/1 Running 0 28m nginx-deployment-7d569cb5f5-l2pbh 1/1 Running 0 2m15s nginx-deployment-7d569cb5f5-r2ln4 1/1 Running 0 28m nginx-deployment-7d569cb5f5-wtbsw 0/1 Terminating 0 2m15s # kubectl get pods NAME READY STATUS RESTARTS AGE nginx-deployment-7d569cb5f5-cqdqg 1/1 Running 0 2m20s nginx-deployment-7d569cb5f5-h7x79 1/1 Running 0 28m nginx-deployment-7d569cb5f5-l2pbh 1/1 Running 0 2m20s nginx-deployment-7d569cb5f5-r2ln4 1/1 Running 0 28m","title":"Scale Down"},{"location":"devops/kubernetes/kubernetes-deployments/#scaling-loggs","text":"# kubectl describe deployment nginx-deployment ### ommitted output ### Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScalingReplicaSet 30m deployment-controller Scaled up replica set nginx-deployment-7d569cb5f5 to 2 Normal ScalingReplicaSet 7m12s deployment-controller Scaled down replica set nginx-deployment-7d569cb5f5 to 2 Normal ScalingReplicaSet 3m25s ( x3 over 16m ) deployment-controller Scaled up replica set nginx-deployment-7d569cb5f5 to 6 Normal ScalingReplicaSet 72s ( x2 over 11m ) deployment-controller Scaled down replica set nginx-deployment-7d569cb5f5 to 4","title":"Scaling Loggs"},{"location":"devops/kubernetes/kubernetes-deployments/#deployment-self-healing","text":"if any of pods in the deployment is destroyed, the deployment will create new ones in order to meet desired number of replicas # kubectl get pods NAME READY STATUS RESTARTS AGE nginx-deployment-7d569cb5f5-cqdqg 1 /1 Running 0 8m nginx-deployment-7d569cb5f5-l2pbh 1 /1 Running 0 8m nginx-deployment-7d569cb5f5-nbb4w 1 /1 Running 0 4s nginx-deployment-7d569cb5f5-r2ln4 1 /1 Running 0 34m # kubectl delete pod nginx-deployment-7d569cb5f5-r2ln4 pod \"nginx-deployment-7d569cb5f5-r2ln4\" deleted # kubectl get pods NAME READY STATUS RESTARTS AGE nginx-deployment-7d569cb5f5-cqdqg 1 /1 Running 0 8m38s nginx-deployment-7d569cb5f5-l2pbh 1 /1 Running 0 8m38s nginx-deployment-7d569cb5f5-nbb4w 1 /1 Running 0 42s nginx-deployment-7d569cb5f5-tjsnw 1 /1 Running 0 3s the cluster build a new container right after I requested the deletion immediately. I couldnt catch the cluster creating the new container but we can tell by looking at the AGE value. nginx-deployment-7d569cb5f5-tjsnw was created when I deleted nginx-deployment-7d569cb5f5-r2ln4 .","title":"Deployment Self-Healing"},{"location":"devops/kubernetes/kubernetes-deployments/#deployment-rolling-updates","text":"the deployment can be adjusted to a new image version. it will gradually spin up new containers with the new vesrion to replace existing containers. Upgrading from image: nginx:1.20.1 to nginx=nginx:1.21.1 . The cluster will create new containers with the new image once those containers are running it will terminate the containers running the old image: # kubectl get pods NAME READY STATUS RESTARTS AGE nginx-deployment-5cfcccdb74-ck2nj 1 /1 Running 0 7s nginx-deployment-5cfcccdb74-lvgq2 0 /1 ContainerCreating 0 2s nginx-deployment-5cfcccdb74-v2k4g 0 /1 ContainerCreating 0 1s nginx-deployment-5cfcccdb74-zn666 1 /1 Running 0 7s nginx-deployment-7d569cb5f5-cqdqg 1 /1 Terminating 0 17m nginx-deployment-7d569cb5f5-l2pbh 1 /1 Running 0 17m nginx-deployment-7d569cb5f5-nbb4w 0 /1 Terminating 0 9m29s","title":"Deployment Rolling Updates"},{"location":"devops/kubernetes/kubernetes-deployments/#conclusion","text":"Edittig the running-config vs editing the actual file and apply the changes works the same way.","title":"Conclusion"},{"location":"devops/kubernetes/kubernetes_services/","text":"Kubernetes Services \u00b6 Kubernetes Services provides an abstraction layer to expose pods created by the deployment. Duo to the pod dynamic nature where pods are dynamically destroyed and created by scaling up/down or rolling updates. Services load balance client connections making sure traffic is only routed to active/running pods that are part of the the service graph LR subgraph \"Kubernetes Cluster\" Kubernetes_Service -->B{nginx-deployment} B -->|Pod1|E[nginx-deployment-5cfcccdb74-ck2nj ] B -->|Pod2|E.A[nginx-deployment-5cfcccdb74-lvgq2] B -->|Pod3|E.B[nginx-deployment-5cfcccdb74-v2k4g] B -->|Pod4|E.C[nginx-deployment-5cfcccdb74-zn666] end kind: Service apiVersion: v1 metadata: name: nginx-service spec: selector: app: nginx-app ports: - protocol: TCP port: 80 targetPort: 80 nodePort: 30080 type: NodePort validate service is running $# kubectl create -f kube-service.yml service/nginx-service created $# kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .96.0.1 <none> 443 /TCP 5d8h nginx-service NodePort 10 .100.81.26 <none> 80 :32180/TCP 5s $# curl localhost:32180 <!DOCTYPE html> <html> <head> <title>Welcome to nginx!</title> <style> body { width: 35em ; margin: 0 auto ; font-family: Tahoma, Verdana, Arial, sans-serif ; } </style> </head> <body> <h1>Welcome to nginx!</h1> <p>If you see this page, the nginx web server is successfully installed and working. Further configuration is required.</p> <p>For online documentation and support please refer to <a href = \"http://nginx.org/\" >nginx.org</a>.<br/> Commercial support is available at <a href = \"http://nginx.com/\" >nginx.com</a>.</p> <p><em>Thank you for using nginx.</em></p> </body> </html> Validating \u00b6 kubectl describe svc can provide an Endpoint list of all running pods # kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .96.0.1 <none> 443 /TCP 5d8h nginx-service NodePort 10 .100.81.26 <none> 80 :32180/TCP 19m root@f8aca9a6c11c:/home/cloud_user# kubectl describe svc nginx-service Name: nginx-service Namespace: default Labels: <none> Annotations: <none> Selector: app = nginx-app Type: NodePort IP: 10 .100.81.26 Port: <unset> 80 /TCP TargetPort: 80 /TCP NodePort: <unset> 32180 /TCP Endpoints: 10 .244.1.20:80,10.244.1.22:80,10.244.2.14:80 + 1 more.. Session Affinity: None External Traffic Policy: Cluster Events: <none> In order to validate only running pods are in the Endpoint list, scale down the deployment: * current state # kubectl get pods NAME READY STATUS RESTARTS AGE nginx-deployment-5cfcccdb74-ck2nj 1 /1 Running 2 21h nginx-deployment-5cfcccdb74-lvgq2 1 /1 Running 2 21h nginx-deployment-5cfcccdb74-v2k4g 1 /1 Running 2 21h nginx-deployment-5cfcccdb74-zn666 1 /1 Running 2 21h use kubectl scale deployments/nginx-deployment --replicas=2 to scale down from 4 to 2 ```bash kubectl scale deployments/nginx-deployment --replicas=2 \u00b6 deployment.extensions/nginx-deployment scaled kubectl get pods \u00b6 NAME READY STATUS RESTARTS AGE nginx-deployment-5cfcccdb74-ck2nj 1/1 Running 2 21h nginx-deployment-5cfcccdb74-lvgq2 1/1 Terminating 2 21h nginx-deployment-5cfcccdb74-v2k4g 1/1 Running 2 21h nginx-deployment-5cfcccdb74-zn666 0/1 Terminating 2 21h kubectl describe svc nginx-service \u00b6 Name: nginx-service Namespace: default Labels: Annotations: Selector: app=nginx-app Type: NodePort IP: 10.100.81.26 Port: 80/TCP TargetPort: 80/TCP NodePort: 32180/TCP Endpoints: 10.244.2.14:80,10.244.2.15:80 Session Affinity: None External Traffic Policy: Cluster Events: ```","title":"Kubernetes Services"},{"location":"devops/kubernetes/kubernetes_services/#kubernetes-services","text":"Kubernetes Services provides an abstraction layer to expose pods created by the deployment. Duo to the pod dynamic nature where pods are dynamically destroyed and created by scaling up/down or rolling updates. Services load balance client connections making sure traffic is only routed to active/running pods that are part of the the service graph LR subgraph \"Kubernetes Cluster\" Kubernetes_Service -->B{nginx-deployment} B -->|Pod1|E[nginx-deployment-5cfcccdb74-ck2nj ] B -->|Pod2|E.A[nginx-deployment-5cfcccdb74-lvgq2] B -->|Pod3|E.B[nginx-deployment-5cfcccdb74-v2k4g] B -->|Pod4|E.C[nginx-deployment-5cfcccdb74-zn666] end kind: Service apiVersion: v1 metadata: name: nginx-service spec: selector: app: nginx-app ports: - protocol: TCP port: 80 targetPort: 80 nodePort: 30080 type: NodePort validate service is running $# kubectl create -f kube-service.yml service/nginx-service created $# kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .96.0.1 <none> 443 /TCP 5d8h nginx-service NodePort 10 .100.81.26 <none> 80 :32180/TCP 5s $# curl localhost:32180 <!DOCTYPE html> <html> <head> <title>Welcome to nginx!</title> <style> body { width: 35em ; margin: 0 auto ; font-family: Tahoma, Verdana, Arial, sans-serif ; } </style> </head> <body> <h1>Welcome to nginx!</h1> <p>If you see this page, the nginx web server is successfully installed and working. Further configuration is required.</p> <p>For online documentation and support please refer to <a href = \"http://nginx.org/\" >nginx.org</a>.<br/> Commercial support is available at <a href = \"http://nginx.com/\" >nginx.com</a>.</p> <p><em>Thank you for using nginx.</em></p> </body> </html>","title":"Kubernetes Services"},{"location":"devops/kubernetes/kubernetes_services/#validating","text":"kubectl describe svc can provide an Endpoint list of all running pods # kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .96.0.1 <none> 443 /TCP 5d8h nginx-service NodePort 10 .100.81.26 <none> 80 :32180/TCP 19m root@f8aca9a6c11c:/home/cloud_user# kubectl describe svc nginx-service Name: nginx-service Namespace: default Labels: <none> Annotations: <none> Selector: app = nginx-app Type: NodePort IP: 10 .100.81.26 Port: <unset> 80 /TCP TargetPort: 80 /TCP NodePort: <unset> 32180 /TCP Endpoints: 10 .244.1.20:80,10.244.1.22:80,10.244.2.14:80 + 1 more.. Session Affinity: None External Traffic Policy: Cluster Events: <none> In order to validate only running pods are in the Endpoint list, scale down the deployment: * current state # kubectl get pods NAME READY STATUS RESTARTS AGE nginx-deployment-5cfcccdb74-ck2nj 1 /1 Running 2 21h nginx-deployment-5cfcccdb74-lvgq2 1 /1 Running 2 21h nginx-deployment-5cfcccdb74-v2k4g 1 /1 Running 2 21h nginx-deployment-5cfcccdb74-zn666 1 /1 Running 2 21h use kubectl scale deployments/nginx-deployment --replicas=2 to scale down from 4 to 2 ```bash","title":"Validating"},{"location":"devops/kubernetes/kubernetes_services/#kubectl-scale-deploymentsnginx-deployment-replicas2","text":"deployment.extensions/nginx-deployment scaled","title":"kubectl scale deployments/nginx-deployment --replicas=2"},{"location":"devops/kubernetes/kubernetes_services/#kubectl-get-pods","text":"NAME READY STATUS RESTARTS AGE nginx-deployment-5cfcccdb74-ck2nj 1/1 Running 2 21h nginx-deployment-5cfcccdb74-lvgq2 1/1 Terminating 2 21h nginx-deployment-5cfcccdb74-v2k4g 1/1 Running 2 21h nginx-deployment-5cfcccdb74-zn666 0/1 Terminating 2 21h","title":"kubectl get pods"},{"location":"devops/kubernetes/kubernetes_services/#kubectl-describe-svc-nginx-service","text":"Name: nginx-service Namespace: default Labels: Annotations: Selector: app=nginx-app Type: NodePort IP: 10.100.81.26 Port: 80/TCP TargetPort: 80/TCP NodePort: 32180/TCP Endpoints: 10.244.2.14:80,10.244.2.15:80 Session Affinity: None External Traffic Policy: Cluster Events: ```","title":"kubectl describe svc nginx-service"},{"location":"devops/terraform/azure-auth/","text":"Terraform Authentication \u00b6 there are multiple ways to provide terraform authentication. the simplest one is using a Microsoft account using az login . However this is good for running terraform locally. To run terraform in a CI/CD Pipeline, you need to use a service principal. Requirements \u00b6 Install Terraform Install Azure CLI Azure Service Principal \u00b6 Reference: Terraform AzureRM Authentication An Azure service principal is a user identity used by user-created apps, services, and automation tools to access specific Azure resources. It is used to grant the minimum permissions level needed to perform specific tasks. It can contain either a password or certificate used for authentication. Go to Azure App Register > New Registration Name : terraform Supported Account Types : Accounts in this organizational directory only (Default Directory only - Single tenant) Redirect URI : web Leave Redirect URI web Value Blank Proceed with [ Register ] The newly created App Registration terraform should be visible on-screen At the top of this page, note the \"Application (client) ID\" and the \"Directory (tenant) ID\". Generate a Client Secret Add Client Secrete !!! note the ** Value ** Id . This is the required value for the environment variable : ` `` ARM_CLIENT_SECRET `` ` . this value is only displayed once . Granting the Application access to manage resources in a Subscription navigate to the Subscriptions blade within the Azure Portal select the Subscription you wish to use click Access Control (IAM), [+] Add > Add role assignment. Specify a Role which grants the appropriate permissions needed for the Service Principal(for example, Network Contributor will grant Read/Write on all network resources in the Subscription). Role : A role definition is a collection of permissions. You can use the built-in roles or you can create your own custom roles. [ Network Contributor ] Members : Selected role : Network Contributor Assign access to : User, group, or service principal Members : [+]Select member (search for the newly created service principal) [Review + Assign] Export Terraform Variables export ARM_CLIENT_ID = \"00000000-0000-0000-0000-000000000000\" export ARM_CLIENT_SECRET = \"00000000-0000-0000-0000-000000000000\" export ARM_SUBSCRIPTION_ID = \"00000000-0000-0000-0000-000000000000\" export ARM_TENANT_ID = \"00000000-0000-0000-0000-000000000000\" Validate credentials using AZ CLI az login --service-principal -u $ARM_CLIENT_ID -p $ARM_CLIENT_SECRET --tenant $ARM_TENANT_ID [ { \"cloudName\" : \"AzureCloud\" , \"id\" : \"97bd4d2e-0b8b-4cab-88d6-445e32c5f2b8\" , \"isDefault\" : true, \"name\" : \"sandbox-services\" , \"state\" : \"Enabled\" , \"tenantId\" : \"90b97141-b1c4-453e-8f76-10db96d5d950\" , \"user\" : { \"name\" : \"9bc261bc-84bf-4c41-af58-c4da7acabd86\" , \"type\" : \"servicePrincipal\" } } ] Terrafom Example \u00b6 Terraform main.tf content ## terraform provider terraform { required_version = \">= 0.12.29, < 2.0\" required_providers { azurerm = { source = \"hashicorp/azurerm\" version = \"~> 3.7\" } } } ## Azure Provider configuration provider \"azurerm\" { features {} skip_provider_registration = true } ## Variables locals { resource_group_name = \"sandbox-rg1\" location = \"East US\" name = \"lab1\" address_space = \"10.100.0.0/16\" } ## Virtual Network resource \"azurerm_virtual_network\" \"sandbox\" { name = \"vnet${local.name}\" location = local.location resource_group_name = local.resource_group_name address_space = [local.address_space] subnet { name = \"vnet${local.name}-snet1\" address_prefix = cidrsubnets(local.address_space, 8,8)[0] } subnet { name = \"vnet${local.name}-snet2\" address_prefix = cidrsubnets(local.address_space, 8,8)[1] } tags = { environment = \"sandbox\" } } terraform init terraform plan terraform apply terraform destroy","title":"Terraform Azure Authentication"},{"location":"devops/terraform/azure-auth/#terraform-authentication","text":"there are multiple ways to provide terraform authentication. the simplest one is using a Microsoft account using az login . However this is good for running terraform locally. To run terraform in a CI/CD Pipeline, you need to use a service principal.","title":"Terraform Authentication"},{"location":"devops/terraform/azure-auth/#requirements","text":"Install Terraform Install Azure CLI","title":"Requirements"},{"location":"devops/terraform/azure-auth/#azure-service-principal","text":"Reference: Terraform AzureRM Authentication An Azure service principal is a user identity used by user-created apps, services, and automation tools to access specific Azure resources. It is used to grant the minimum permissions level needed to perform specific tasks. It can contain either a password or certificate used for authentication. Go to Azure App Register > New Registration Name : terraform Supported Account Types : Accounts in this organizational directory only (Default Directory only - Single tenant) Redirect URI : web Leave Redirect URI web Value Blank Proceed with [ Register ] The newly created App Registration terraform should be visible on-screen At the top of this page, note the \"Application (client) ID\" and the \"Directory (tenant) ID\". Generate a Client Secret Add Client Secrete !!! note the ** Value ** Id . This is the required value for the environment variable : ` `` ARM_CLIENT_SECRET `` ` . this value is only displayed once . Granting the Application access to manage resources in a Subscription navigate to the Subscriptions blade within the Azure Portal select the Subscription you wish to use click Access Control (IAM), [+] Add > Add role assignment. Specify a Role which grants the appropriate permissions needed for the Service Principal(for example, Network Contributor will grant Read/Write on all network resources in the Subscription). Role : A role definition is a collection of permissions. You can use the built-in roles or you can create your own custom roles. [ Network Contributor ] Members : Selected role : Network Contributor Assign access to : User, group, or service principal Members : [+]Select member (search for the newly created service principal) [Review + Assign] Export Terraform Variables export ARM_CLIENT_ID = \"00000000-0000-0000-0000-000000000000\" export ARM_CLIENT_SECRET = \"00000000-0000-0000-0000-000000000000\" export ARM_SUBSCRIPTION_ID = \"00000000-0000-0000-0000-000000000000\" export ARM_TENANT_ID = \"00000000-0000-0000-0000-000000000000\" Validate credentials using AZ CLI az login --service-principal -u $ARM_CLIENT_ID -p $ARM_CLIENT_SECRET --tenant $ARM_TENANT_ID [ { \"cloudName\" : \"AzureCloud\" , \"id\" : \"97bd4d2e-0b8b-4cab-88d6-445e32c5f2b8\" , \"isDefault\" : true, \"name\" : \"sandbox-services\" , \"state\" : \"Enabled\" , \"tenantId\" : \"90b97141-b1c4-453e-8f76-10db96d5d950\" , \"user\" : { \"name\" : \"9bc261bc-84bf-4c41-af58-c4da7acabd86\" , \"type\" : \"servicePrincipal\" } } ]","title":"Azure Service Principal"},{"location":"devops/terraform/azure-auth/#terrafom-example","text":"Terraform main.tf content ## terraform provider terraform { required_version = \">= 0.12.29, < 2.0\" required_providers { azurerm = { source = \"hashicorp/azurerm\" version = \"~> 3.7\" } } } ## Azure Provider configuration provider \"azurerm\" { features {} skip_provider_registration = true } ## Variables locals { resource_group_name = \"sandbox-rg1\" location = \"East US\" name = \"lab1\" address_space = \"10.100.0.0/16\" } ## Virtual Network resource \"azurerm_virtual_network\" \"sandbox\" { name = \"vnet${local.name}\" location = local.location resource_group_name = local.resource_group_name address_space = [local.address_space] subnet { name = \"vnet${local.name}-snet1\" address_prefix = cidrsubnets(local.address_space, 8,8)[0] } subnet { name = \"vnet${local.name}-snet2\" address_prefix = cidrsubnets(local.address_space, 8,8)[1] } tags = { environment = \"sandbox\" } } terraform init terraform plan terraform apply terraform destroy","title":"Terrafom Example"},{"location":"devops/terraform/google-secret-manager/","text":"Secret Manager can be used to store API keys, passwords, and any other sensitive data. Requirements \u00b6 GCP Project Access Terraform Create Secret \u00b6 Create secrets to store self-manage certificate public and private keys. self-managed-cert.crt -----BEGIN CERTIFICATE----- MYCERTVALUES -----END CERTIFICATE----- self-managed-cert.key -----BEGIN RSA PRIVATE KEY----- MYCERTKEY -----END RSA PRIVATE KEY----- create secret ## Creates secrets resource \"google_secret_manager_secret\" \"cert\" { project = \"rdy-dev-cld-usa-dev01\" secret_id = \"self-managed-cert\" replication { automatic = true } } resource \"google_secret_manager_secret\" \"key\" { project = \"rdy-dev-cld-usa-dev01\" secret_id = \"self-managed-cert-key\" replication { automatic = true } } ## adds content to secrets resource \"google_secret_manager_secret_version\" \"cert\" { secret = google_secret_manager_secret.cert.id secret_data = file(\"self-managed-cert.cert\") } resource \"google_secret_manager_secret_version\" \"key\" { secret = google_secret_manager_secret.key.id secret_data = file(\"self-managed-cert.key\") } read secret data ## gets secret_data data \"google_secret_manager_secret_version\" \"cert\" { project = \"rdy-dev-cld-usa-dev01\" secret = google_secret_manager_secret.cert.id } data \"google_secret_manager_secret_version\" \"key\" { project = \"rdy-dev-cld-usa-dev01\" secret = google_secret_manager_secret.key.id } consuming secret data using terraform outputs output \"cert_secret_value\" { value = data.google_secret_manager_secret_version.cert.secret_data sensitive = true } output \"key_secret_value\" { value = data.google_secret_manager_secret_version.key.secret_data sensitive = true } terraform apply does not output secret_data by default. use terraform ouput -json to print secret values $ terraform apply < omitted data> Apply complete! Resources: 0 added, 0 changed, 0 destroyed. Outputs: cert_secret_value = <sensitive> key_secret_value = <sensitive> hlc $ terraform output -json { \"cert_secret_value\": { \"sensitive\": true, \"type\": \"string\", \"value\": \" -----BEGIN CERTIFICATE-----\\n MYCERTVALUES\\n -----END CERTIFICATE-----\" }, \"key_secret_value\": { \"sensitive\": true, \"type\": \"string\", \"value\": \" -----BEGIN RSA PRIVATE KEY-----\\n MYCERTKEY\\n -----END RSA PRIVATE KEY-----\" } }","title":" Google Secret Manager"},{"location":"devops/terraform/google-secret-manager/#requirements","text":"GCP Project Access Terraform","title":"Requirements"},{"location":"devops/terraform/google-secret-manager/#create-secret","text":"Create secrets to store self-manage certificate public and private keys. self-managed-cert.crt -----BEGIN CERTIFICATE----- MYCERTVALUES -----END CERTIFICATE----- self-managed-cert.key -----BEGIN RSA PRIVATE KEY----- MYCERTKEY -----END RSA PRIVATE KEY----- create secret ## Creates secrets resource \"google_secret_manager_secret\" \"cert\" { project = \"rdy-dev-cld-usa-dev01\" secret_id = \"self-managed-cert\" replication { automatic = true } } resource \"google_secret_manager_secret\" \"key\" { project = \"rdy-dev-cld-usa-dev01\" secret_id = \"self-managed-cert-key\" replication { automatic = true } } ## adds content to secrets resource \"google_secret_manager_secret_version\" \"cert\" { secret = google_secret_manager_secret.cert.id secret_data = file(\"self-managed-cert.cert\") } resource \"google_secret_manager_secret_version\" \"key\" { secret = google_secret_manager_secret.key.id secret_data = file(\"self-managed-cert.key\") } read secret data ## gets secret_data data \"google_secret_manager_secret_version\" \"cert\" { project = \"rdy-dev-cld-usa-dev01\" secret = google_secret_manager_secret.cert.id } data \"google_secret_manager_secret_version\" \"key\" { project = \"rdy-dev-cld-usa-dev01\" secret = google_secret_manager_secret.key.id } consuming secret data using terraform outputs output \"cert_secret_value\" { value = data.google_secret_manager_secret_version.cert.secret_data sensitive = true } output \"key_secret_value\" { value = data.google_secret_manager_secret_version.key.secret_data sensitive = true } terraform apply does not output secret_data by default. use terraform ouput -json to print secret values $ terraform apply < omitted data> Apply complete! Resources: 0 added, 0 changed, 0 destroyed. Outputs: cert_secret_value = <sensitive> key_secret_value = <sensitive> hlc $ terraform output -json { \"cert_secret_value\": { \"sensitive\": true, \"type\": \"string\", \"value\": \" -----BEGIN CERTIFICATE-----\\n MYCERTVALUES\\n -----END CERTIFICATE-----\" }, \"key_secret_value\": { \"sensitive\": true, \"type\": \"string\", \"value\": \" -----BEGIN RSA PRIVATE KEY-----\\n MYCERTKEY\\n -----END RSA PRIVATE KEY-----\" } }","title":"Create Secret"},{"location":"devops/terraform/intro/","text":"Terraform Documentation Infrastructure as code ( IaC) solves several problems with manual and static infrastructure management. IaC provides the ability to build and destroy an environment multiple times if required. IaC can be used to create environment lifecycle where changes can transition from development, testing to production. Terraform is an IaC (Infrastructure as Code) tool used to build and modify infrastructure safely and efficiently. It keeps track of its environment state so rerunning the tool against a code with no changes will not generate any changes. Terraform Flow \u00b6 Known Issues \u00b6 Empty Lists - https://github.com/hashicorp/terraform/issues/23562","title":"Introduction"},{"location":"devops/terraform/intro/#terraform-flow","text":"","title":"Terraform Flow"},{"location":"devops/terraform/intro/#known-issues","text":"Empty Lists - https://github.com/hashicorp/terraform/issues/23562","title":"Known Issues"},{"location":"devops/terraform/terraform-refactoring/","text":"Note Always create a local backup of your state file before making any changes to it. If using a Cloud Bucket as your backend, enable versioning. Terraform State/Workspace Renaming \u00b6 list current terraform workspaces terraform init terraform workspace list select terraform workspace terraform workspace select <workspace-name> create a local terraform state backup hlc terraform state pull > <new-workspace-name>.tfstate create new workspace and copy local .tfstate terraform workspace new -state=<new-workspace-name>.tfstate <new-workspace-name> validate terraform state in new workspace terraform plan delete old terraform workspace Note default workspace cannot be deleted. terraform workspace delete -force <old-workspace-name> Terraform State/Workspace Refactoring \u00b6 This can be used to move state between buckets and workspaces. Note: This moves all resources within a state file. list current terraform workspaces terraform init terraform workspace list switch to current terraform workspace terraform workspace select <current-workspace-name> validate current terraform state terraform plan create a local terraform state backup terraform state pull > <new-workspace-name>.tfstate 5. comment out/edit the backend block in backend.tf if exists. This will force a terraform init which will switch to a new remote or local backend # backend \"gcs\" { # bucket = \"***\" # prefix = \"terraform/state/***/***\" # } switch to new backend - this will copy any current workspaces from all terraform init -migrate-state Optional Steps \u00b6 This used to copy local state file into a newly created workspace. create new workspace and copy .tfstate terraform workspace new -state=<new-workspace-name>.tfstate <new-workspace-name> validate terraform state terraform plan validate terraform workspace terraform workspace list Terraform Resource State Migration \u00b6 This can be used to move specific resources from one state file to another. create a local terraform state backup terraform state pull > <workspace-name>.tfstate list all terraform resources in state file terraform state list output gtz4all@admin:~/terraform$ terraform state list module.new-vpcs[\"gtz4all-core-vnet01\"].module.subnet.google_compute_subnetwork.subnetwork[\"us-east1/gtz4all-use1-prd-snet01\"] module.new-vpcs[\"gtz4all-core-net01\"].module.subnet.google_compute_subnetwork.subnetwork[\"us-east1/gtz4all-use1-npr-snet01\"] module.new-vpcs[\"gtz4all-core-net01\"].module.subnet.google_compute_subnetwork.subnetwork[\"us-east1/gtz4all-use1-dev-snet01\"] module.new-vpcs[\"gtz4all-core-net01\"].module.subnet.google_compute_subnetwork.subnetwork[\"us-east1/gtz4all-use1-shd-snet01\"] move a resource from current workspace state file into a local state file terraform state mv -state-out=<new-state-name>.tfstate \\ 'module.xxx[\"xxx\"]' \\ #<--- resource name from state list - wrap resource inside single quotes 'module.xxx[\"xxx\"]' #<--- new resource name - use same name as state list - wrap resource inside single quotes Note since resource value is using double quotes, single quotes are being used outside resource. gtz4all@admin:~/terraform$ terraform state mv -state-out=prd.tfstate \\ > 'module.new-vpcs[\"gtz4all-core-vnet01\"].module.subnet.google_compute_subnetwork.subnetwork[\"us-east1/gtz4all-use1-prd-snet01\"]' \\ > 'module.new-vpcs[\"gtz4all-core-vnet01\"].module.subnet.google_compute_subnetwork.subnetwork[\"us-east1/gtz4all-use1-prd-snet01\"]' edit variables file and remove any resources that have been move out of state file. validate changes by running terraform plan/apply - expected msg: No changes. Your infrastructure matches the configuration. migrate moved resources to another workspace create variables files with the resources that were moved create new workspace and copy local .tfstate terraform workspace new -state=<new-state-name>.tfstate <new-workspace-name> validate by running terraform plan/apply - expected msg: No changes. Your infrastructure matches the configuration.","title":"Terraform State Refactoring"},{"location":"devops/terraform/terraform-refactoring/#terraform-stateworkspace-renaming","text":"list current terraform workspaces terraform init terraform workspace list select terraform workspace terraform workspace select <workspace-name> create a local terraform state backup hlc terraform state pull > <new-workspace-name>.tfstate create new workspace and copy local .tfstate terraform workspace new -state=<new-workspace-name>.tfstate <new-workspace-name> validate terraform state in new workspace terraform plan delete old terraform workspace Note default workspace cannot be deleted. terraform workspace delete -force <old-workspace-name>","title":"Terraform State/Workspace Renaming"},{"location":"devops/terraform/terraform-refactoring/#terraform-stateworkspace-refactoring","text":"This can be used to move state between buckets and workspaces. Note: This moves all resources within a state file. list current terraform workspaces terraform init terraform workspace list switch to current terraform workspace terraform workspace select <current-workspace-name> validate current terraform state terraform plan create a local terraform state backup terraform state pull > <new-workspace-name>.tfstate 5. comment out/edit the backend block in backend.tf if exists. This will force a terraform init which will switch to a new remote or local backend # backend \"gcs\" { # bucket = \"***\" # prefix = \"terraform/state/***/***\" # } switch to new backend - this will copy any current workspaces from all terraform init -migrate-state","title":"Terraform State/Workspace Refactoring"},{"location":"devops/terraform/terraform-refactoring/#optional-steps","text":"This used to copy local state file into a newly created workspace. create new workspace and copy .tfstate terraform workspace new -state=<new-workspace-name>.tfstate <new-workspace-name> validate terraform state terraform plan validate terraform workspace terraform workspace list","title":"Optional Steps"},{"location":"devops/terraform/terraform-refactoring/#terraform-resource-state-migration","text":"This can be used to move specific resources from one state file to another. create a local terraform state backup terraform state pull > <workspace-name>.tfstate list all terraform resources in state file terraform state list output gtz4all@admin:~/terraform$ terraform state list module.new-vpcs[\"gtz4all-core-vnet01\"].module.subnet.google_compute_subnetwork.subnetwork[\"us-east1/gtz4all-use1-prd-snet01\"] module.new-vpcs[\"gtz4all-core-net01\"].module.subnet.google_compute_subnetwork.subnetwork[\"us-east1/gtz4all-use1-npr-snet01\"] module.new-vpcs[\"gtz4all-core-net01\"].module.subnet.google_compute_subnetwork.subnetwork[\"us-east1/gtz4all-use1-dev-snet01\"] module.new-vpcs[\"gtz4all-core-net01\"].module.subnet.google_compute_subnetwork.subnetwork[\"us-east1/gtz4all-use1-shd-snet01\"] move a resource from current workspace state file into a local state file terraform state mv -state-out=<new-state-name>.tfstate \\ 'module.xxx[\"xxx\"]' \\ #<--- resource name from state list - wrap resource inside single quotes 'module.xxx[\"xxx\"]' #<--- new resource name - use same name as state list - wrap resource inside single quotes Note since resource value is using double quotes, single quotes are being used outside resource. gtz4all@admin:~/terraform$ terraform state mv -state-out=prd.tfstate \\ > 'module.new-vpcs[\"gtz4all-core-vnet01\"].module.subnet.google_compute_subnetwork.subnetwork[\"us-east1/gtz4all-use1-prd-snet01\"]' \\ > 'module.new-vpcs[\"gtz4all-core-vnet01\"].module.subnet.google_compute_subnetwork.subnetwork[\"us-east1/gtz4all-use1-prd-snet01\"]' edit variables file and remove any resources that have been move out of state file. validate changes by running terraform plan/apply - expected msg: No changes. Your infrastructure matches the configuration. migrate moved resources to another workspace create variables files with the resources that were moved create new workspace and copy local .tfstate terraform workspace new -state=<new-state-name>.tfstate <new-workspace-name> validate by running terraform plan/apply - expected msg: No changes. Your infrastructure matches the configuration.","title":"Terraform Resource State Migration"},{"location":"devops/webservices/environmentvars/","text":"Enabling CGI on Apache2 \u00b6 Install Apache2 \u00b6 sudo apt update -y sudo apt install apache2 -y Enable CGI \u00b6 sudo ln -s /etc/apache2/mods-available/cgi.load /etc/apache2/mods-enabled/ sudo systemctl restart apache2 HTTP Envelop Scripts \u00b6 - cd /usr/lib/cgi-bin/ - nano http-env Perl Python #!/usr/bin/perl print \"Content-type: text/html \\n\\n \" ; print \"<pre> \\n \" ; foreach $ key ( sort keys ( % ENV )) { print \"$key = $ENV{$key}<p>\" ; } print \"</pre> \\n \" ; #!/usr/bin/python3 import os def main (): print ( \"Content-type:text/html \\r\\n\\r\\n \" ) for var in sorted ( os . environ . keys ()): print ( ' %s = %s </br/>' % ( var , os . environ [ var ])) return 0 if __name__ == '__main__' : main () Make http-env exectuable chmod +x /usr/lib/cgi-bin/http-env Bash Script \u00b6 set executable permissions sudo chmod +x http-env.sh ./http-env.sh http-env.sh \u00b6 ```python echo \" ### Apache2 - Install Apache ### \" sudo apt update -y sudo apt install apache2 -y echo \" ### Apache2 - Enable CGI ### \" sudo ln -s /etc/apache2/mods-available/cgi.load /etc/apache2/mods-enabled/ sudo systemctl restart apache2 echo \" ### Apache2 - Create HTTP-ENV Script ### \" cat < /usr/lib/cgi-bin/http-env !/usr/bin/python3 \u00b6 import os def main(): print(\"Content-type:text/html\\r\\n\\r\\n\") for var in sorted ( os . environ . keys ()) : print ( '%s = %s</br/>' % ( var , os . environ [ var ] )) return 0 if name == ' main ': main() EOT echo \" ### Apache2 - Set Permissions on HTTP-ENV Script ### \" sudo chmod 755 /usr/lib/cgi-bin/http-env echo \" ### Apache2 - Verification ### \" verification= \\((curl http://127.0.0.1/cgi-bin/http-env) echo \"\\) verification\" echo","title":"Apache Environment Variables"},{"location":"devops/webservices/environmentvars/#enabling-cgi-on-apache2","text":"","title":"Enabling CGI on Apache2"},{"location":"devops/webservices/environmentvars/#install-apache2","text":"sudo apt update -y sudo apt install apache2 -y","title":"Install Apache2"},{"location":"devops/webservices/environmentvars/#enable-cgi","text":"sudo ln -s /etc/apache2/mods-available/cgi.load /etc/apache2/mods-enabled/ sudo systemctl restart apache2","title":"Enable CGI"},{"location":"devops/webservices/environmentvars/#http-envelop-scripts","text":"- cd /usr/lib/cgi-bin/ - nano http-env Perl Python #!/usr/bin/perl print \"Content-type: text/html \\n\\n \" ; print \"<pre> \\n \" ; foreach $ key ( sort keys ( % ENV )) { print \"$key = $ENV{$key}<p>\" ; } print \"</pre> \\n \" ; #!/usr/bin/python3 import os def main (): print ( \"Content-type:text/html \\r\\n\\r\\n \" ) for var in sorted ( os . environ . keys ()): print ( ' %s = %s </br/>' % ( var , os . environ [ var ])) return 0 if __name__ == '__main__' : main () Make http-env exectuable chmod +x /usr/lib/cgi-bin/http-env","title":"HTTP Envelop Scripts"},{"location":"devops/webservices/environmentvars/#bash-script","text":"set executable permissions sudo chmod +x http-env.sh ./http-env.sh","title":"Bash Script"},{"location":"devops/webservices/environmentvars/#http-envsh","text":"```python echo \" ### Apache2 - Install Apache ### \" sudo apt update -y sudo apt install apache2 -y echo \" ### Apache2 - Enable CGI ### \" sudo ln -s /etc/apache2/mods-available/cgi.load /etc/apache2/mods-enabled/ sudo systemctl restart apache2 echo \" ### Apache2 - Create HTTP-ENV Script ### \" cat < /usr/lib/cgi-bin/http-env","title":"http-env.sh"},{"location":"devops/webservices/environmentvars/#usrbinpython3","text":"import os def main(): print(\"Content-type:text/html\\r\\n\\r\\n\") for var in sorted ( os . environ . keys ()) : print ( '%s = %s</br/>' % ( var , os . environ [ var ] )) return 0 if name == ' main ': main() EOT echo \" ### Apache2 - Set Permissions on HTTP-ENV Script ### \" sudo chmod 755 /usr/lib/cgi-bin/http-env echo \" ### Apache2 - Verification ### \" verification= \\((curl http://127.0.0.1/cgi-bin/http-env) echo \"\\) verification\" echo","title":"!/usr/bin/python3"},{"location":"devops/webservices/http-server-with-dynamic-content/","text":"Apache HTTPD CGI \u00b6 Apache is an open-source HTTP server (httpd) that has been around since 1995. The HTTPD CGI (Common Gateway Interface) module provides a method to display dynamic content on web sites using scripts written in any language such as python, perl or bash. https://httpd.apache.org/ Python CGI module The CGI Module can be use to evaluate and process user input submitted through an HTML or javascript function() using the FieldStorage class. The FieldStorage supports indexed Python dictionary, standard dictionary method keys() and the built-in function len(). Form fields containing empty strings are ignored and do not appear in the dictionary; to keep such values, provide a true value for the optional keep_blank_values keyword parameter when creating the FieldStorage instance. # create key/value pairs form fields passed by javascript/html form = cgi . FieldStorage () form_vars = {} for key in form . keys (): form_vars [ key ] = form [ key ] . value Building an Apache-CGI Docker Image using a Dockerfile \u00b6 Docker is virtualization tool used to create lightweight, portable images and containers. Docker images are self-contained, meaning they dont have any dependencies on the uderlaying OS. As long as you have docker installed, you'll be able to build a container with any docker image. Instructions \u00b6 There are many prebuild httpd-cgi docker images out there(Docker Hub). However I took a few for a test drive and noticed they didnt have what I needed. In order to address some of these limitations, I had two options: Build an image on top of those prebuilt images or build one from scratch. I decided to build one from scratch in order to fully customize it to my needs. Some of the packages are for future use such as requests and boto3. Alpine Base image \u00b6 Alpine Linux is a 3MB lightweight Linux Image with a large selection of packages it is package manager apk. https://alpinelinux.org/about/ Software requirements \u00b6 These software requirements are specific to my webform needs. I use Python3 CGI to collect submitted form key,values, Jinja2 to generate dynamic HTML pages, request to make Rest API calls, and boto3 to directly make use of AWS resources. Nano is a plain and simple text editor. Software Description apache2 HTTP Server python3 CGI Script prefered programing language pip3 Used to install other python modules jinja2 Used to HTML render templates request used to make HTTP GET/POST calls to Rest APIs boto3 Used to interact with AWS Resources nano Simple text editor Enable CGI by Modifying HTTPD Config - httpd.config \u00b6 Since I dont spend too much time doing web development work. I try to avoid making extensive changes to the default httpd.config file in order to avoid unexpected behavior. ## omitted some not so relevant items ##** Enabled CGI Module **## LoadModule cgi_module modules/mod_cgi.so ##** send logs to stdout - docker logs **## CustomLog /proc/self/fd/1 format ##** Avoids attempting to invoke directoy as script **## Alias \"/\" \"/var/www/localhost/cgi-bin/\" ##** only runs cgi,pl,py as scripts **## AddHandler cgi-script cgi pl py sh Options ExecCGI Dockerfile \u00b6 A Dockerfile is a text document that contains all the commands to assemble an image. The docker build command executes the command-line instructions in the Dockerfile one-by-one, committing the result of each instruction to a new image(if necessary) before finally outputting the new image ID. If necessary, the image can by modified by editing the Dockerfile and rerunning the docker build command using the same tag provided during the initial built. ###--------------------START-------------------------------------# ################################################################ # httpd:Alpine # Features: CGI, Python3 - Jinja2, Requests, Boto3 ################################################################ # Base Image - Official Alpine FROM alpine:latest LABEL vendor = \"Gtz4All\" LABEL maintainer = \"netgtz\" # Upgrade existing packages in the base image RUN apk --no-cache upgrade # Install apache from packages with out caching install files RUN apk add --update --no-cache nano apache2 python3 && ln -sf python3 /usr/bin/python RUN python3 -m ensurepip RUN pip3 install --no-cache --upgrade pip setuptools jinja2 requests boto3 # backup httpd.conf RUN mv /etc/apache2/httpd.conf /etc/apache2/httpd.conf.bk # Copy custom files COPY httpd.conf /etc/apache2/httpd.conf COPY cgi-bin/ /var/www/localhost/cgi-bin # Open port for httpd access EXPOSE 80 # Run httpd FOREGROUND in the background(-D option) CMD [ \"-D\" , \"FOREGROUND\" ] # Start httpd ENTRYPOINT [ \"/usr/sbin/httpd\" ] ###--------------------END-------------------------------------# Building the docker image \u00b6 Image Tags - you can create multiple tags for the same docker image, they will show up in 'docker images'. Since they are just that, tags, they are not consuming any additional disk space. Options Description -t image tag name -f custom Dockerfile name . Default Dockerfile name docker build -t custom-httpd-cgi . Docker Image Built Summary \u00b6 The Custom Dockerfile pulled the latest alpine image and installed apache2, python3, pip3, jinja2, requests, boto3 and nano. It renamed the original httpd.config file and copied the custom version. Final image size - 150MB . Running The Docker Container \u00b6 These are some of the commonly options used when working with containers. Options|Description --|-- -d|deattach -i|interactive -t|terminal -p|portmapping [local-port]:[container-port] --name|custom name --rm|Clean up (--rm) removes container after stopping it run container in deattach mode \u00b6 This command runs the container in deattached mode, mapping local port 8080 to container port 80 docker run -d --name webserver -p 8080 :80 custom-httpd-cgi * Verify image is running $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 4ca72abac592 custom-httpd-cgi \"/usr/sbin/httpd -D \u2026\" About a minute ago Up About a minute 0 .0.0.0:8080->80/tcp, :::8080->80/tcp webserver * Verify webserver is running Output came from /cgi-bin/index.html $ curl localhost:8080 <!DOCTYPE html> <html> <head> <title>Gtz4All</title> </head> <body> <h1><center>Welcome to Gtz4All Demo Page</center></h1> <p><center>To learn more, please visit <a href = \"https://netgtz.gitlab.io/\" >Gtz4All Blog</center></a></p> </body> </html> Environment Variables $ curl localhost:8080/show-env.py CONTEXT_DOCUMENT_ROOT = /var/www/localhost/cgi-bin/</br/> CONTEXT_PREFIX = /</br/> DOCUMENT_ROOT = /var/www/localhost/htdocs</br/> GATEWAY_INTERFACE = CGI/1.1</br/> HTTP_ACCEPT = */*</br/> HTTP_HOST = localhost:8080</br/> HTTP_USER_AGENT = curl/7.58.0</br/> PATH = /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin</br/> QUERY_STRING = </br/> REMOTE_ADDR = 172 .17.0.1</br/> REMOTE_PORT = 49684 </br/> REQUEST_METHOD = GET</br/> REQUEST_SCHEME = http</br/> REQUEST_URI = /show-env.py</br/> SCRIPT_FILENAME = /var/www/localhost/cgi-bin/show-env.py</br/> SCRIPT_NAME = /show-env.py</br/> SERVER_ADDR = 172 .17.0.2</br/> SERVER_ADMIN = you@example.com</br/> SERVER_NAME = localhost</br/> SERVER_PORT = 8080 </br/> SERVER_PROTOCOL = HTTP/1.1</br/> SERVER_SIGNATURE = <address>Apache/2.4.48 ( Unix ) Server at localhost Port 8080 </address> </br/> SERVER_SOFTWARE = Apache/2.4.48 ( Unix ) </br/> Attach to Container terminal Every image has its own shell terminal - ubuntu = /bin/bash, Alpine = /bin/sh docker exec -it webserver /bin/sh Mounting webservices folder \u00b6 This folder contains a sample webform used as a base to provision/build/deploy resources. It makes use of Javascript,Jinja templates, and CGI. Mounting folder keeps all files changes local [local-folder]:[container-folder] [container-name] map local folder to container's cgi-bin folder \u00b6 docker run -d -p 8080:80 -v $(pwd)/webservices:/var/www/localhost/cgi-bin/webservices --name webserver custom-httpd-cgi registry.gitlab.com \u00b6 Pushing image to registry.gitlab.com \u00b6 Login to Docker registry docker login registry.gitlab.com --username netgtz Current Image - Rename to Standard docker rename custom-httpd-cgi registry.gitlab.com/netgtz/httpd-cgi/custom-httpd-cgi Build Docker Image docker build -t registry.gitlab.com/netgtz/httpd-cgi/custom-httpd-cgi . Push Docker Image docker push registry.gitlab.com/netgtz/httpd-cgi/custom-httpd-cgi Pull image from registry.gitlab.com \u00b6 Login to Docker registry docker login registry.gitlab.com --username netgtz Pull Image docker pull registry.gitlab.com/netgtz/httpd-cgi/custom-httpd-cgi","title":"HTTP Server with Dynamic Content"},{"location":"devops/webservices/http-server-with-dynamic-content/#apache-httpd-cgi","text":"Apache is an open-source HTTP server (httpd) that has been around since 1995. The HTTPD CGI (Common Gateway Interface) module provides a method to display dynamic content on web sites using scripts written in any language such as python, perl or bash. https://httpd.apache.org/ Python CGI module The CGI Module can be use to evaluate and process user input submitted through an HTML or javascript function() using the FieldStorage class. The FieldStorage supports indexed Python dictionary, standard dictionary method keys() and the built-in function len(). Form fields containing empty strings are ignored and do not appear in the dictionary; to keep such values, provide a true value for the optional keep_blank_values keyword parameter when creating the FieldStorage instance. # create key/value pairs form fields passed by javascript/html form = cgi . FieldStorage () form_vars = {} for key in form . keys (): form_vars [ key ] = form [ key ] . value","title":"Apache HTTPD CGI"},{"location":"devops/webservices/http-server-with-dynamic-content/#building-an-apache-cgi-docker-image-using-a-dockerfile","text":"Docker is virtualization tool used to create lightweight, portable images and containers. Docker images are self-contained, meaning they dont have any dependencies on the uderlaying OS. As long as you have docker installed, you'll be able to build a container with any docker image.","title":"Building an Apache-CGI Docker Image using a Dockerfile"},{"location":"devops/webservices/http-server-with-dynamic-content/#instructions","text":"There are many prebuild httpd-cgi docker images out there(Docker Hub). However I took a few for a test drive and noticed they didnt have what I needed. In order to address some of these limitations, I had two options: Build an image on top of those prebuilt images or build one from scratch. I decided to build one from scratch in order to fully customize it to my needs. Some of the packages are for future use such as requests and boto3.","title":"Instructions"},{"location":"devops/webservices/http-server-with-dynamic-content/#alpine-base-image","text":"Alpine Linux is a 3MB lightweight Linux Image with a large selection of packages it is package manager apk. https://alpinelinux.org/about/","title":"Alpine Base image"},{"location":"devops/webservices/http-server-with-dynamic-content/#software-requirements","text":"These software requirements are specific to my webform needs. I use Python3 CGI to collect submitted form key,values, Jinja2 to generate dynamic HTML pages, request to make Rest API calls, and boto3 to directly make use of AWS resources. Nano is a plain and simple text editor. Software Description apache2 HTTP Server python3 CGI Script prefered programing language pip3 Used to install other python modules jinja2 Used to HTML render templates request used to make HTTP GET/POST calls to Rest APIs boto3 Used to interact with AWS Resources nano Simple text editor","title":"Software requirements"},{"location":"devops/webservices/http-server-with-dynamic-content/#enable-cgi-by-modifying-httpd-config-httpdconfig","text":"Since I dont spend too much time doing web development work. I try to avoid making extensive changes to the default httpd.config file in order to avoid unexpected behavior. ## omitted some not so relevant items ##** Enabled CGI Module **## LoadModule cgi_module modules/mod_cgi.so ##** send logs to stdout - docker logs **## CustomLog /proc/self/fd/1 format ##** Avoids attempting to invoke directoy as script **## Alias \"/\" \"/var/www/localhost/cgi-bin/\" ##** only runs cgi,pl,py as scripts **## AddHandler cgi-script cgi pl py sh Options ExecCGI","title":"Enable CGI by Modifying HTTPD Config - httpd.config"},{"location":"devops/webservices/http-server-with-dynamic-content/#dockerfile","text":"A Dockerfile is a text document that contains all the commands to assemble an image. The docker build command executes the command-line instructions in the Dockerfile one-by-one, committing the result of each instruction to a new image(if necessary) before finally outputting the new image ID. If necessary, the image can by modified by editing the Dockerfile and rerunning the docker build command using the same tag provided during the initial built. ###--------------------START-------------------------------------# ################################################################ # httpd:Alpine # Features: CGI, Python3 - Jinja2, Requests, Boto3 ################################################################ # Base Image - Official Alpine FROM alpine:latest LABEL vendor = \"Gtz4All\" LABEL maintainer = \"netgtz\" # Upgrade existing packages in the base image RUN apk --no-cache upgrade # Install apache from packages with out caching install files RUN apk add --update --no-cache nano apache2 python3 && ln -sf python3 /usr/bin/python RUN python3 -m ensurepip RUN pip3 install --no-cache --upgrade pip setuptools jinja2 requests boto3 # backup httpd.conf RUN mv /etc/apache2/httpd.conf /etc/apache2/httpd.conf.bk # Copy custom files COPY httpd.conf /etc/apache2/httpd.conf COPY cgi-bin/ /var/www/localhost/cgi-bin # Open port for httpd access EXPOSE 80 # Run httpd FOREGROUND in the background(-D option) CMD [ \"-D\" , \"FOREGROUND\" ] # Start httpd ENTRYPOINT [ \"/usr/sbin/httpd\" ] ###--------------------END-------------------------------------#","title":"Dockerfile"},{"location":"devops/webservices/http-server-with-dynamic-content/#building-the-docker-image","text":"Image Tags - you can create multiple tags for the same docker image, they will show up in 'docker images'. Since they are just that, tags, they are not consuming any additional disk space. Options Description -t image tag name -f custom Dockerfile name . Default Dockerfile name docker build -t custom-httpd-cgi .","title":"Building the docker image"},{"location":"devops/webservices/http-server-with-dynamic-content/#docker-image-built-summary","text":"The Custom Dockerfile pulled the latest alpine image and installed apache2, python3, pip3, jinja2, requests, boto3 and nano. It renamed the original httpd.config file and copied the custom version. Final image size - 150MB .","title":"Docker Image Built Summary"},{"location":"devops/webservices/http-server-with-dynamic-content/#running-the-docker-container","text":"These are some of the commonly options used when working with containers. Options|Description --|-- -d|deattach -i|interactive -t|terminal -p|portmapping [local-port]:[container-port] --name|custom name --rm|Clean up (--rm) removes container after stopping it","title":"Running The Docker Container"},{"location":"devops/webservices/http-server-with-dynamic-content/#run-container-in-deattach-mode","text":"This command runs the container in deattached mode, mapping local port 8080 to container port 80 docker run -d --name webserver -p 8080 :80 custom-httpd-cgi * Verify image is running $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 4ca72abac592 custom-httpd-cgi \"/usr/sbin/httpd -D \u2026\" About a minute ago Up About a minute 0 .0.0.0:8080->80/tcp, :::8080->80/tcp webserver * Verify webserver is running Output came from /cgi-bin/index.html $ curl localhost:8080 <!DOCTYPE html> <html> <head> <title>Gtz4All</title> </head> <body> <h1><center>Welcome to Gtz4All Demo Page</center></h1> <p><center>To learn more, please visit <a href = \"https://netgtz.gitlab.io/\" >Gtz4All Blog</center></a></p> </body> </html> Environment Variables $ curl localhost:8080/show-env.py CONTEXT_DOCUMENT_ROOT = /var/www/localhost/cgi-bin/</br/> CONTEXT_PREFIX = /</br/> DOCUMENT_ROOT = /var/www/localhost/htdocs</br/> GATEWAY_INTERFACE = CGI/1.1</br/> HTTP_ACCEPT = */*</br/> HTTP_HOST = localhost:8080</br/> HTTP_USER_AGENT = curl/7.58.0</br/> PATH = /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin</br/> QUERY_STRING = </br/> REMOTE_ADDR = 172 .17.0.1</br/> REMOTE_PORT = 49684 </br/> REQUEST_METHOD = GET</br/> REQUEST_SCHEME = http</br/> REQUEST_URI = /show-env.py</br/> SCRIPT_FILENAME = /var/www/localhost/cgi-bin/show-env.py</br/> SCRIPT_NAME = /show-env.py</br/> SERVER_ADDR = 172 .17.0.2</br/> SERVER_ADMIN = you@example.com</br/> SERVER_NAME = localhost</br/> SERVER_PORT = 8080 </br/> SERVER_PROTOCOL = HTTP/1.1</br/> SERVER_SIGNATURE = <address>Apache/2.4.48 ( Unix ) Server at localhost Port 8080 </address> </br/> SERVER_SOFTWARE = Apache/2.4.48 ( Unix ) </br/> Attach to Container terminal Every image has its own shell terminal - ubuntu = /bin/bash, Alpine = /bin/sh docker exec -it webserver /bin/sh","title":"run container in deattach mode"},{"location":"devops/webservices/http-server-with-dynamic-content/#mounting-webservices-folder","text":"This folder contains a sample webform used as a base to provision/build/deploy resources. It makes use of Javascript,Jinja templates, and CGI. Mounting folder keeps all files changes local [local-folder]:[container-folder] [container-name]","title":"Mounting webservices folder"},{"location":"devops/webservices/http-server-with-dynamic-content/#map-local-folder-to-containers-cgi-bin-folder","text":"docker run -d -p 8080:80 -v $(pwd)/webservices:/var/www/localhost/cgi-bin/webservices --name webserver custom-httpd-cgi","title":"map local folder to container's cgi-bin folder"},{"location":"devops/webservices/http-server-with-dynamic-content/#registrygitlabcom","text":"","title":"registry.gitlab.com"},{"location":"devops/webservices/http-server-with-dynamic-content/#pushing-image-to-registrygitlabcom","text":"Login to Docker registry docker login registry.gitlab.com --username netgtz Current Image - Rename to Standard docker rename custom-httpd-cgi registry.gitlab.com/netgtz/httpd-cgi/custom-httpd-cgi Build Docker Image docker build -t registry.gitlab.com/netgtz/httpd-cgi/custom-httpd-cgi . Push Docker Image docker push registry.gitlab.com/netgtz/httpd-cgi/custom-httpd-cgi","title":"Pushing image to registry.gitlab.com"},{"location":"devops/webservices/http-server-with-dynamic-content/#pull-image-from-registrygitlabcom","text":"Login to Docker registry docker login registry.gitlab.com --username netgtz Pull Image docker pull registry.gitlab.com/netgtz/httpd-cgi/custom-httpd-cgi","title":"Pull image from registry.gitlab.com"},{"location":"devops/webservices/intro/","text":"Web Services Documentation python is currently the leading devops provider.","title":"Introduction"},{"location":"networking/intro/","text":"cisco Documentation arista Documentation Coming Soon! \u00b6","title":"Introduction"},{"location":"networking/intro/#coming-soon","text":"","title":"Coming Soon! "},{"location":"networking/arista/intro/","text":"Coming Soon! \u00b6","title":"Introduction"},{"location":"networking/arista/intro/#coming-soon","text":"","title":"Coming Soon!"},{"location":"networking/cisco/intro/","text":"Coming Soon! \u00b6","title":"Introduction"},{"location":"networking/cisco/intro/#coming-soon","text":"","title":"Coming Soon!"},{"location":"tips-tricks/intro/","text":"Reference \u00b6 The purpose of this section is to provide code snippets, tips and solution to simple issues. Git Commands git add -A .; git commit -m \"new content\"; git push -u origin master Side by Side Code Block The usual Markdown Cheatsheet does not cover some of the more advanced Markdown tricks, but here is one. You can combine verbatim HTML with your Markdown. This is particularly useful for tables. Notice that with empty separating lines we can use Markdown inside HTML: Json 1 Json 2 { \"id\" : 5 , \"username\" : \"mary\" , \"email\" : \"mary@example.com\" , \"order_id\" : \"f7177da\" } { \"id\" : 5 , \"username\" : \"mary\" , \"email\" : \"mary@example.com\" , \"order_id\" : \"f7177da\" } Mermaid Sample Code Vertical Horizonal graph TD; subgraph \"Device Connectivity\" 1(DISTRT1)---|Gi1/12|3(ACCSW1); 2(DISTRT2)---|Gi1/12|3(ACCSW1); 1(DISTRT1)---|Gi1/13|4(ACCSW2); 2(DISTRT2)---|Gi1/13|4(ACCSW2); 1(DISTRT1)---|Gi1/14|5(ACCSW3); 2(DISTRT2)---|Gi1/14|5(ACCSW3); 1(DISTRT1)---|Gi1/15|6(ACCSW4); 2(DISTRT2)---|Gi1/15|6(ACCSW4); 1(DISTRT1)---|Gi1/16|7(ACCSW5); 2(DISTRT2)---|Gi1/16|7(ACCSW5); 1(DISTRT1)---|Gi1/17|8(ACCSW6); 2(DISTRT2)---|Gi1/17|8(ACCSW6); end graph LR subgraph \"Requester Pre Provision Trigger URL\" Requester-URL \u2192|Provision|B{CI/CD Complete Provisioning} B \u2192|Switch Variables|E[Pull Switch Vars from Archive] B \u2192|Validate Switch|E.A[Pull switch variables] B \u2192|Validate Switch Code|E.B[Build Code Standard Variables] B \u2192|Email Notification|E.C[Generate Requester Token/Email] E.C \u2192|Requester Email|1[Email w/Token, Switch OSPF/Initial Cfg] end Manage Docker containers/images List All Container IDs \u00b6 ## list all containes docker ps -a ## list all containers IDs ( -q = IDs) docker ps -aq Stop All Running Containers \u00b6 ## list all running containers docker ps ### stop all containers docker stop $(docker ps -aq) Remove All Containers \u00b6 ## Remove all stopped containers docker rm $(docker ps -aq) Remove All Images \u00b6 docker rmi $(docker images -q) Connect to Pod Shell Testing \u00b6 cloud_user# kubectl exec --stdin --tty nginx-deployment-5cfcccdb74-ck2nj -- /bin/bash root@nginx-deployment-5cfcccdb74-ck2nj:/# hugo server -D --bind=192.168.1.55 --baseURL= http://192.168.1.55 grep -RiIl '25000' | xargs sed -i 's/25000/1000/g' grep -rnw 'CUST01/' -e '25000' echo theme = \\\"ananke\\\" >> config.toml","title":"Tips and Tricks"},{"location":"tips-tricks/intro/#reference","text":"The purpose of this section is to provide code snippets, tips and solution to simple issues. Git Commands git add -A .; git commit -m \"new content\"; git push -u origin master Side by Side Code Block The usual Markdown Cheatsheet does not cover some of the more advanced Markdown tricks, but here is one. You can combine verbatim HTML with your Markdown. This is particularly useful for tables. Notice that with empty separating lines we can use Markdown inside HTML: Json 1 Json 2 { \"id\" : 5 , \"username\" : \"mary\" , \"email\" : \"mary@example.com\" , \"order_id\" : \"f7177da\" } { \"id\" : 5 , \"username\" : \"mary\" , \"email\" : \"mary@example.com\" , \"order_id\" : \"f7177da\" } Mermaid Sample Code Vertical Horizonal graph TD; subgraph \"Device Connectivity\" 1(DISTRT1)---|Gi1/12|3(ACCSW1); 2(DISTRT2)---|Gi1/12|3(ACCSW1); 1(DISTRT1)---|Gi1/13|4(ACCSW2); 2(DISTRT2)---|Gi1/13|4(ACCSW2); 1(DISTRT1)---|Gi1/14|5(ACCSW3); 2(DISTRT2)---|Gi1/14|5(ACCSW3); 1(DISTRT1)---|Gi1/15|6(ACCSW4); 2(DISTRT2)---|Gi1/15|6(ACCSW4); 1(DISTRT1)---|Gi1/16|7(ACCSW5); 2(DISTRT2)---|Gi1/16|7(ACCSW5); 1(DISTRT1)---|Gi1/17|8(ACCSW6); 2(DISTRT2)---|Gi1/17|8(ACCSW6); end graph LR subgraph \"Requester Pre Provision Trigger URL\" Requester-URL \u2192|Provision|B{CI/CD Complete Provisioning} B \u2192|Switch Variables|E[Pull Switch Vars from Archive] B \u2192|Validate Switch|E.A[Pull switch variables] B \u2192|Validate Switch Code|E.B[Build Code Standard Variables] B \u2192|Email Notification|E.C[Generate Requester Token/Email] E.C \u2192|Requester Email|1[Email w/Token, Switch OSPF/Initial Cfg] end Manage Docker containers/images","title":"Reference"},{"location":"tips-tricks/intro/#list-all-container-ids","text":"## list all containes docker ps -a ## list all containers IDs ( -q = IDs) docker ps -aq","title":"List All Container IDs"},{"location":"tips-tricks/intro/#stop-all-running-containers","text":"## list all running containers docker ps ### stop all containers docker stop $(docker ps -aq)","title":"Stop All Running Containers"},{"location":"tips-tricks/intro/#remove-all-containers","text":"## Remove all stopped containers docker rm $(docker ps -aq)","title":"Remove All Containers"},{"location":"tips-tricks/intro/#remove-all-images","text":"docker rmi $(docker images -q) Connect to Pod Shell","title":"Remove All Images"},{"location":"tips-tricks/intro/#testing","text":"cloud_user# kubectl exec --stdin --tty nginx-deployment-5cfcccdb74-ck2nj -- /bin/bash root@nginx-deployment-5cfcccdb74-ck2nj:/# hugo server -D --bind=192.168.1.55 --baseURL= http://192.168.1.55 grep -RiIl '25000' | xargs sed -i 's/25000/1000/g' grep -rnw 'CUST01/' -e '25000' echo theme = \\\"ananke\\\" >> config.toml","title":"Testing"}]}